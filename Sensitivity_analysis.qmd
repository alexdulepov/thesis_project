---
title: "Sensitivity analysis"
author: "Aleksandr Dulepov"
format: html
editor: visual
---

```{r}
library(mgcv)
library(mice)
library(broom)
library(miceadds)
library(stargazer) #tables for regression output
library(purrr) # Flexible functional programming
library(stargazer)
library(table1)
library(htmlTable)
library(tidyverse)
library(olsrr) #best subset selection
library(statip) # for mfv1 = mode function
library(sjPlot)
```

```{r}
#| eval: false
#| include: false
######extract summary in tibbles to backtransform the coefficients from log to original scale (broom package) for concurvity
glance(lm_pooled) # model performance
tidy(lm_pooled) # coefficients
augment(lm_pooled) #residuals,fitted values and other statistics
#################################################################
```

# Load mids objects

```{r}
file_path <- "//kaappi.oulu.fi/nfbc$/projects/P0511/Alex/2_pmm_proj_multiimp.rds"
file_path_2 <- "//kaappi.oulu.fi/nfbc$/projects/P0511/Alex/3_pmm_proj_multiimp_cimt.rds"
# Load
pmm_proj_multiimp = readRDS(file_path)
pmm_proj_multiimp_cimt = readRDS(file_path_2)
c66_project_df <- readRDS("//kaappi.oulu.fi/nfbc$/projects/P0511/Alex/1_c66_project_df.rds")
```

```{r}
# A mode function for factor columns
# mfv1 is from statip, or you could define your own:
# get_mode <- function(x) { ... }

bp_df_plot <- pmm_proj_multiimp %>%
  mice::complete("long") %>%
  filter(.imp != 0) %>%   # exclude the original data
  group_by(.id) %>%
  summarise(
    # 1) Retain the project_id for each group 
    #    (assuming one project_id per .id group)
    project_ID = first(project_ID),
    
    # 2) Summarize numeric columns with the mean
    across(
      .cols = where(is.numeric),
      .fns = ~ mean(.x, na.rm = TRUE)
    ),
    
    # 3) Summarize factor columns with the single most frequent value
    across(
      .cols = where(is.factor),
      .fns = ~ mfv1(.x)
    )
  )%>%
  ungroup()

```

# Complete case analysis

We compare estimates after imputing with estimates coming from complete case dataset. We will leave only those variables that are utilized in the fully adjusted model.

```{r}
c66_project_df_no_na = c66_project_df %>%
  select(y1_app_temp_avg_46,
         c66_46_q1_ht , c66_46_c_bmi_cat , 
                    c66_46_ht_med , c66_46_edu ,  
                    c66_46_smok , c66_46_alc ,
                   c66_46_phys , c66_46_occup , 
                     , sex , 
                    c66_46_diab , c66_46_hpcl , 
                    c66_46_season, c66_46_urban_rural, day_mean, c66_46_avrsybp, y1_app_temp_sd_46, c66_46_avrdibp) %>%
  drop_na()

summary(c66_project_df_no_na)
```

### 2.1 Systolic blood pressure

#### 2.1.1 Apparent average temperature

##### 2.1.1.1 Unadjusted model (univariate)

```{r}
# Apply the linear function to each imputed dataset
lm_model_complete <- lm(c66_46_avrsybp ~ y1_app_temp_avg_46, data = c66_project_df_no_na)

summary(lm_model_complete)
confint(lm_model_complete)
glance(lm_model_complete)
```

Pooled b1 for app.temp is -1.12 (p\<0.001) \[-1.4 : -0.84\]. SE=0.13. Radj = 0.01, BIC = 37753.

##### 2.1.1.2 Minimally adjusted model

```{r}
# Apply the linear function to each imputed dataset
lm_model_complete <- lm(c66_46_avrsybp ~ y1_app_temp_avg_46 + c66_46_edu +
                c66_46_occup + sex + 
                c66_46_urban_rural + c66_46_season, data = c66_project_df_no_na)

summary(lm_model_complete)
confint(lm_model_complete)
glance(lm_model_complete)
```

Pooled b1 for app.temp is -0.88 (p\<0.001) \[-1.15 : -0.62\]. SE=0.14. Radj = 0.14, BIC = 37200. **Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
car::Anova(lm_model_complete, type = 3)
```

R2=0.19. All categorical variables are stat. significant except occup.

##### 2.1.1.3 Optimized model

```{r}
# Apply the linear function to each imputed dataset
lm_model_complete <- lm(c66_46_avrsybp ~ y1_app_temp_avg_46  +  
                       c66_46_smok + 
                       c66_46_c_bmi_cat + c66_46_edu +
                       c66_46_ht_med + c66_46_q1_ht +
                       c66_46_urban_rural + day_mean +
                       sex + c66_46_alc, data = c66_project_df_no_na)

summary(lm_model_complete)
confint(lm_model_complete)
glance(lm_model_complete)
```

Pooled b1 for app.temp is -0.85 (p\<0.001) \[-1.13 : -0.58\]. SE=0.14. Radj = 0.26, BIC = 36595. **Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
car::Anova(lm_model_complete, type = 3)
```

R2=0.19. All categorical variables are stat. significant.

##### 2.1.1.4 Fully adjusted

```{r}
# Apply the linear function to each imputed dataset
lm_model_complete <- lm(c66_46_avrsybp ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = c66_project_df_no_na)

summary(lm_model_complete)
confint(lm_model_complete)
glance(lm_model_complete)
```

Pooled b1 for app.temp is -0.87 (p\<0.001) \[-1.15 : -0.59\]. SE=0.14. Radj = 0.26, BIC = 36657. **Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
car::Anova(lm_model_complete, type = 3)
```

R2=0.19. All categorical variables are stat. significant except phys, occup, diab, hpcl, day_mean.

#### 2.1.2 Apparent standard deviation of average temperature

##### 2.1.2.1 Unadjusted model (univariate)

```{r}
# Apply the linear function to each imputed dataset
lm_model_complete <- lm(c66_46_avrsybp ~ y1_app_temp_sd_46, data = c66_project_df_no_na)

summary(lm_model_complete)
confint(lm_model_complete)
glance(lm_model_complete)
```

Pooled b1 for app.temp is -0.07 (p=0.79) \[-0.62 : 0.47\]. SE=0.28. Radj = -0.0002, BIC = 37814.

##### 2.1.2.2 Minimally adjusted model

```{r}
# Apply the linear function to each imputed dataset
lm_model_complete <- lm(c66_46_avrsybp ~ y1_app_temp_sd_46 + c66_46_edu +
                c66_46_occup + sex + 
                c66_46_urban_rural + c66_46_season, data = c66_project_df_no_na)

summary(lm_model_complete)
confint(lm_model_complete)
glance(lm_model_complete)
```

Pooled b1 for app.temp is -0.25 (p=0.35) \[-1.15 : -0.62\]. SE=0.26. Radj = 0.13, BIC = 37241. **Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
car::Anova(lm_model_complete, type = 3)
```

R2=0.19. All categorical variables are stat. significant except occup and app_sd.

##### 2.1.2.3 Optimized model

```{r}
# Apply the linear function to each imputed dataset
lm_model_complete <- lm(c66_46_avrsybp ~ y1_app_temp_sd_46    +  
                       c66_46_c_bmi_cat +
                       c66_46_edu + c66_46_ht_med +
                       c66_46_q1_ht + 
                       c66_46_smok + c66_46_urban_rural +
                       sex + day_mean + c66_46_alc, data = c66_project_df_no_na)

summary(lm_model_complete)
confint(lm_model_complete)
glance(lm_model_complete)
```

Pooled b1 for app.temp is 0.07 (p=0.78) \[-0.41 : 0.54\]. SE=0.24. Radj = 0.25, BIC = 36623. **Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
car::Anova(lm_model_complete, type = 3)
```

R2=0.19. All categorical variables are stat. significant except app_sd.

##### 2.1.2.4 Fully adjusted(-)

```{r}
# Apply the linear function to each imputed dataset
lm_model_complete <- lm(c66_46_avrsybp ~ y1_app_temp_sd_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = c66_project_df_no_na)

summary(lm_model_complete)
confint(lm_model_complete)
glance(lm_model_complete)
```

Pooled b1 for app.temp is 0.72(p=0.04) \[0.017, 1.42\]. SE=0.36. Radj = 0.25, BIC = 36690. **Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
car::Anova(lm_model_complete, type = 3)
```

R2=0.19. All categorical variables are stat. significant except phys, occup, ap_age, diab, hpcl, day_mean.

#### 2.1.3 Average temperature

##### 2.1.3.1 Unadjusted model (univariate)

```{r}
# Apply the linear function to each imputed dataset
lm_model_complete <- lm(c66_46_avrsybp ~ y1_avg_temp, data = c66_project_df_no_na)

summary(lm_model_complete)
confint(lm_model_complete)
glance(lm_model_complete)
```

Pooled b1 for app.temp is -1.12 (p\<0.001) \[-1.4 : -0.84\]. SE=0.13. Radj = 0.01, BIC = 37753.

##### 2.1.3.2 Minimally adjusted model

```{r}
# Apply the linear function to each imputed dataset
lm_model_complete <- lm(c66_46_avrsybp ~ y1_avg_temp + c66_46_edu +
                c66_46_occup + sex + 
                c66_46_ht_med, data = c66_project_df_no_na)

summary(lm_model_complete)
confint(lm_model_complete)
glance(lm_model_complete)
```

Pooled b1 for app.temp is -0.88 (p\<0.001) \[-1.15 : -0.62\]. SE=0.14. Radj = 0.14, BIC = 37200. **Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
car::Anova(lm_model_complete, type = 3)
```

R2=0.19. All categorical variables are stat. significant except occup.

##### 2.1.3.3 Optimized model

```{r}
# Apply the linear function to each imputed dataset
lm_model_complete <- lm(c66_46_avrsybp ~ y1_avg_temp +  
                       c66_46_smok +  +
                       c66_46_c_bmi_cat + c66_46_edu +
                       c66_46_ht_med + c66_46_q1_ht +
                       c66_46_urban_rural + day_mean +
                       sex + c66_46_alc, data = c66_project_df_no_na)

summary(lm_model_complete)
confint(lm_model_complete)
glance(lm_model_complete)
```

Pooled b1 for app.temp is -0.85 (p\<0.001) \[-1.13 : -0.58\]. SE=0.14. Radj = 0.26, BIC = 36595. **Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
car::Anova(lm_model_complete, type = 3)
```

R2=0.19. All categorical variables are stat. significant.

##### 2.1.3.4 Fully adjusted

```{r}
# Apply the linear function to each imputed dataset
lm_model_complete <- lm(c66_46_avrsybp ~ y1_avg_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = c66_project_df_no_na)

summary(lm_model_complete)
confint(lm_model_complete)
glance(lm_model_complete)
```

Pooled b1 for app.temp is -0.87 (p\<0.001) \[-1.15 : -0.59\]. SE=0.14. Radj = 0.26, BIC = 36657. **Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
car::Anova(lm_model_complete, type = 3)
```

R2=0.19. All categorical variables are stat. significant except phys, occup, diab, hpcl, day_mean.

#### 2.1.4 Standard deviation of average temperature

##### 2.1.4.1 Unadjusted model (univariate)

```{r}
# Apply the linear function to each imputed dataset
lm_model_complete <- lm(c66_46_avrsybp ~ y1_sd_temp, data = c66_project_df_no_na)

summary(lm_model_complete)
confint(lm_model_complete)
glance(lm_model_complete)
```

Pooled b1 for app.temp is -0.07 (p=0.79) \[-0.62 : 0.47\]. SE=0.28. Radj = -0.0002, BIC = 37814.

##### 2.1.4.2 Minimally adjusted model

```{r}
# Apply the linear function to each imputed dataset
lm_model_complete <- lm(c66_46_avrsybp ~ y1_sd_temp + c66_46_edu +
                c66_46_occup + sex + 
                c66_46_ht_med, data = c66_project_df_no_na)

summary(lm_model_complete)
confint(lm_model_complete)
glance(lm_model_complete)
```

Pooled b1 for app.temp is -0.25 (p=0.35) \[-1.15 : -0.62\]. SE=0.26. Radj = 0.13, BIC = 37241. **Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
car::Anova(lm_model_complete, type = 3)
```

R2=0.19. All categorical variables are stat. significant except occup and app_sd.

##### 2.1.4.3 Optimized model

```{r}
# Apply the linear function to each imputed dataset
lm_model_complete <- lm(c66_46_avrsybp ~ y1_sd_temp + c66_46_alc +  
                       c66_46_c_bmi_cat +
                       c66_46_edu + c66_46_ht_med +
                       c66_46_q1_ht + 
                       c66_46_smok + c66_46_urban_rural +
                       sex + day_mean, data = c66_project_df_no_na)

summary(lm_model_complete)
confint(lm_model_complete)
glance(lm_model_complete)
```

Pooled b1 for app.temp is 0.07 (p=0.78) \[-0.41 : 0.54\]. SE=0.24. Radj = 0.25, BIC = 36623. **Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
car::Anova(lm_model_complete, type = 3)
```

R2=0.19. All categorical variables are stat. significant except app_sd.

##### 2.1.4.4 Fully adjusted(-)

```{r}
# Apply the linear function to each imputed dataset
lm_model_complete <- lm(c66_46_avrsybp ~ y1_sd_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = c66_project_df_no_na)

summary(lm_model_complete)
confint(lm_model_complete)
glance(lm_model_complete)
```

Pooled b1 for app.temp is 0.72(p=0.04) \[0.017, 1.42\]. SE=0.36. Radj = 0.25, BIC = 36690. **Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
car::Anova(lm_model_complete, type = 3)
```

R2=0.19. All categorical variables are stat. significant except phys, occup, ap_age, diab, hpcl, day_mean.

### 2.2 Diastolic blood pressure

#### 2.2.1 Apparent average temperature

##### 2.2.1.1 Unadjusted model (univariate)

```{r}
# Apply the linear function to each imputed dataset
lm_model_complete <- lm(c66_46_avrdibp ~ y1_app_temp_avg_46, data = c66_project_df_no_na)

summary(lm_model_complete)
confint(lm_model_complete)
glance(lm_model_complete)
```

Pooled b1 for app.temp is -0.73 (p\<0.001) \[-0.91 :-0.54\]. SE=0.1. Radj = 0.01, BIC = 34155.

##### 2.2.1.2 Minimally adjusted model

```{r}
# Apply the linear function to each imputed dataset
lm_model_complete <- lm(c66_46_avrdibp ~ y1_app_temp_avg_46 + c66_46_edu +
                c66_46_occup + sex + 
                c66_46_urban_rural + c66_46_season, data = c66_project_df_no_na)

summary(lm_model_complete)
confint(lm_model_complete)
glance(lm_model_complete)
```

Pooled b1 for app.temp is -0.58 (p\<0.001) \[-0.77 : -0.40\]. SE=0.09. Radj = 0.09, BIC = 33851. **Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
car::Anova(lm_model_complete, type = 3)
```

R2=0.19. All categorical variables are stat. significant except occup.

##### 2.2.1.3 Optimized model

```{r}
# Apply the linear function to each imputed dataset
lm_model_complete <- lm(c66_46_avrdibp ~ y1_app_temp_avg_46 +  
                       c66_46_alc + c66_46_c_bmi_cat +
                       c66_46_ht_med +
                       c66_46_q1_ht + 
                       c66_46_urban_rural + 
                       day_mean + sex, data = c66_project_df_no_na)

summary(lm_model_complete)
confint(lm_model_complete)
glance(lm_model_complete)
```

Pooled b1 for app.temp is -0.77 (p\<0.001) \[-0.94 : -0.60\]. SE=0.09. Radj = 0.26, BIC = 32936. **Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
car::Anova(lm_model_complete, type = 3)
```

R2=0.19. All categorical variables are stat. significant.

##### 2.2.1.4 Fully adjusted

```{r}
# Apply the linear function to each imputed dataset
lm_model_complete <- lm(c66_46_avrdibp ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = c66_project_df_no_na)

summary(lm_model_complete)
confint(lm_model_complete)
glance(lm_model_complete)
```

Pooled b1 for app.temp is -0.63 (p\<0.001) \[-0.82 : -0.45\]. SE=0.09. Radj = 0.27, BIC = 33011. **Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
car::Anova(lm_model_complete, type = 3)
```

R2=0.19. All categorical variables are stat. significant except phys, occup, diab, season, day_mean.

#### 2.2.1 Apparent standard deviation of average temperature

##### 2.2.1.1 Unadjusted model (univariate)

```{r}
# Apply the linear function to each imputed dataset
lm_model_complete <- lm(c66_46_avrdibp ~ y1_app_temp_sd_46, data = c66_project_df_no_na)

summary(lm_model_complete)
confint(lm_model_complete)
glance(lm_model_complete)
```

Pooled b1 for app.temp is 0.2 (p=0.29) \[-0.17 : 0.56\]. SE=0.19. Radj = -0.0002, BIC = 34211.

##### 2.2.1.2 Minimally adjusted model

```{r}
# Apply the linear function to each imputed dataset
lm_model_complete <- lm(c66_46_avrdibp ~ y1_app_temp_sd_46 + c66_46_edu +
                c66_46_occup + sex + 
                c66_46_urban_rural + c66_46_season, data = c66_project_df_no_na)

summary(lm_model_complete)
confint(lm_model_complete)
glance(lm_model_complete)
```

Pooled b1 for app.temp is 0.1 (p=0.58) \[-0.25 : 0.45\]. SE=0.18. Radj = 0.08, BIC = 33890. **Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
car::Anova(lm_model_complete, type = 3)
```

R2=0.19. All categorical variables are stat. significant except occup and app_sd.

##### 2.2.1.3 Optimized model

```{r}
# Apply the linear function to each imputed dataset
lm_model_complete <- lm(c66_46_avrdibp ~ y1_app_temp_sd_46  +  
                       c66_46_alc + c66_46_c_bmi_cat +
                       c66_46_ht_med + 
                       c66_46_q1_ht +
                       c66_46_urban_rural +
                       sex, data = c66_project_df_no_na)

summary(lm_model_complete)
confint(lm_model_complete)
glance(lm_model_complete)
```

Pooled b1 for app.temp is 0.32 (p=0.05) \[-0.0003 : 0.64\]. SE=0.16. Radj = 0.25, BIC = 32980. **Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
car::Anova(lm_model_complete, type = 3)
```

R2=0.19. All categorical variables are stat. significant except app_sd.

##### 2.2.1.4 Fully adjusted(-)

```{r}
# Apply the linear function to each imputed dataset
lm_model_complete <- lm(c66_46_avrdibp ~ y1_app_temp_sd_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = c66_project_df_no_na)

summary(lm_model_complete)
confint(lm_model_complete)
glance(lm_model_complete)
```

Pooled b1 for app.temp is 0.72(p=0.003) \[0.25, 1.19\]. SE=0.24. Radj = 0.26, BIC = 33047. **Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
car::Anova(lm_model_complete, type = 3)
```

R2=0.19. All categorical variables are stat. significant except phys, occup, ap_age, diab, season, day_mean.

#### 2.2.1 Average temperature

##### 2.2.1.1 Unadjusted model (univariate)

```{r}
# Apply the linear function to each imputed dataset
lm_model_complete <- lm(c66_46_avrdibp ~ y1_avg_temp, data = c66_project_df_no_na)

summary(lm_model_complete)
confint(lm_model_complete)
glance(lm_model_complete)
```

Pooled b1 for app.temp is -0.73 (p\<0.001) \[-0.91 :-0.54\]. SE=0.1. Radj = 0.01, BIC = 34155.

##### 2.2.1.2 Minimally adjusted model

```{r}
# Apply the linear function to each imputed dataset
lm_model_complete <- lm(c66_46_avrdibp ~ y1_avg_temp + c66_46_edu +
                c66_46_occup + sex + 
                c66_46_ht_med, data = c66_project_df_no_na)

summary(lm_model_complete)
confint(lm_model_complete)
glance(lm_model_complete)
```

Pooled b1 for app.temp is -0.58 (p\<0.001) \[-0.77 : -0.40\]. SE=0.09. Radj = 0.09, BIC = 33851. **Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
car::Anova(lm_model_complete, type = 3)
```

R2=0.19. All categorical variables are stat. significant except occup.

##### 2.2.1.3 Optimized model

```{r}
# Apply the linear function to each imputed dataset
lm_model_complete <- lm(c66_46_avrdibp ~ y1_avg_temp + c66_46_phys + 
                       c66_46_alc +  +
                       c66_46_c_bmi_cat +
                       c66_46_ht_med + c66_46_q1_ht +
                       sex, data = c66_project_df_no_na)

summary(lm_model_complete)
confint(lm_model_complete)
glance(lm_model_complete)
```

Pooled b1 for app.temp is -0.77 (p\<0.001) \[-0.94 : -0.60\]. SE=0.09. Radj = 0.26, BIC = 32936. **Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
car::Anova(lm_model_complete, type = 3)
```

R2=0.19. All categorical variables are stat. significant.

##### 2.2.1.4 Fully adjusted

```{r}
# Apply the linear function to each imputed dataset
lm_model_complete <- lm(c66_46_avrdibp ~ y1_avg_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = c66_project_df_no_na)

summary(lm_model_complete)
confint(lm_model_complete)
glance(lm_model_complete)
```

Pooled b1 for app.temp is -0.63 (p\<0.001) \[-0.82 : -0.45\]. SE=0.09. Radj = 0.27, BIC = 33011. **Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
car::Anova(lm_model_complete, type = 3)
```

R2=0.19. All categorical variables are stat. significant except phys, occup, diab, season, day_mean.

#### 2.2.1 Standard deviation of average temperature

##### 2.2.1.1 Unadjusted model (univariate)

```{r}
# Apply the linear function to each imputed dataset
lm_model_complete <- lm(c66_46_avrdibp ~ y1_sd_temp, data = c66_project_df_no_na)

summary(lm_model_complete)
confint(lm_model_complete)
glance(lm_model_complete)
```

Pooled b1 for app.temp is 0.2 (p=0.29) \[-0.17 : 0.56\]. SE=0.19. Radj = -0.0002, BIC = 34211.

##### 2.2.1.2 Minimally adjusted model

```{r}
# Apply the linear function to each imputed dataset
lm_model_complete <- lm(c66_46_avrdibp ~ y1_sd_temp + c66_46_edu +
                c66_46_occup + sex + 
                c66_46_ht_med, data = c66_project_df_no_na)

summary(lm_model_complete)
confint(lm_model_complete)
glance(lm_model_complete)
```

Pooled b1 for app.temp is 0.1 (p=0.58) \[-0.25 : 0.45\]. SE=0.18. Radj = 0.08, BIC = 33890. **Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
car::Anova(lm_model_complete, type = 3)
```

R2=0.19. All categorical variables are stat. significant except occup and app_sd.

##### 2.2.1.3 Optimized model

```{r}
# Apply the linear function to each imputed dataset
lm_model_complete <- lm(c66_46_avrdibp ~y1_sd_temp +  
                       c66_46_alc + c66_46_c_bmi_cat +
                       c66_46_edu + c66_46_ht_med +
                       c66_46_q1_ht +
                       c66_46_urban_rural + day_mean +
                       sex, data = c66_project_df_no_na)

summary(lm_model_complete)
confint(lm_model_complete)
glance(lm_model_complete)
```

Pooled b1 for app.temp is 0.32 (p=0.05) \[-0.0003 : 0.64\]. SE=0.16. Radj = 0.25, BIC = 32980. **Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
car::Anova(lm_model_complete, type = 3)
```

R2=0.19. All categorical variables are stat. significant except app_sd.

##### 2.2.1.4 Fully adjusted(-)

```{r}
# Apply the linear function to each imputed dataset
lm_model_complete <- lm(c66_46_avrdibp ~ y1_sd_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = c66_project_df_no_na)

summary(lm_model_complete)
confint(lm_model_complete)
glance(lm_model_complete)
```

Pooled b1 for app.temp is 0.72(p=0.003) \[0.25, 1.19\]. SE=0.24. Radj = 0.26, BIC = 33047. **Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
car::Anova(lm_model_complete, type = 3)
```

R2=0.19. All categorical variables are stat. significant except phys, occup, ap_age, diab, season, day_mean.

# Box-Cox variable transformation

## Assessing lambda

The natural logarithm, square root, and inverse transformations are special cases of the more general **Box-Cox family of transformations**. The method only applies to transformation of the outcome, not predictors. Also, the method assumes that the outcome values are all positive. If there are a few zero or negative values, a modification of the Box-Cox that allows non-positive values (Box-Cox with negatives, BCN) can be used.

![](images/clipboard-4012141847.png){width="591"}

Lets first plot and find the best lambda using averaged dataset:

1\) Syst.BP + app.avg.temp:

```{r}
lm_model_box_cox <- lm(c66_46_avrsybp ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = bp_df_plot)

LAMBDA <- MASS::boxcox(lm_model_box_cox)
LAMBDA$x[which(LAMBDA$y == max(LAMBDA$y))]
```

2\) Syst.BP + app.sd:

```{r}
lm_model_box_cox <- lm(c66_46_avrsybp ~ y1_app_temp_sd_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = bp_df_plot)

LAMBDA <- MASS::boxcox(lm_model_box_cox)
LAMBDA$x[which(LAMBDA$y == max(LAMBDA$y))]
```

3\) Diast.BP + app.avg.temp:

```{r}
lm_model_box_cox <- lm(c66_46_avrdibp ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = bp_df_plot)

LAMBDA <- MASS::boxcox(lm_model_box_cox)
LAMBDA$x[which(LAMBDA$y == max(LAMBDA$y))]
```

4\) Diast.BP + app.sd:

```{r}
lm_model_box_cox <- lm(c66_46_avrdibp ~ y1_app_temp_sd_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = bp_df_plot)

LAMBDA <- MASS::boxcox(lm_model_box_cox)
LAMBDA$x[which(LAMBDA$y == max(LAMBDA$y))]
```

In case of syst.bp. the best lambda is -0.5, whereas diast.bp shows lambda around 0 (ln transformation).

Next we will extract lambda from all imputed datasets with each outcome-exposure combination.

```{r}
# 1. Extract all completed (imputed) datasets as a list
imputed_data_list <- complete(pmm_proj_multiimp, action = "all")
m <- length(imputed_data_list)  # number of imputed datasets

# 2. Define the outcomes
outcomes <- c("c66_46_avrdibp", "c66_46_avrsybp")

# 3. Create a helper function to extract best lambda via MASS::boxcox
boxcox_best_lambda <- function(fit, seq_range = seq(-2, 2, 0.1)) {
  bc <- MASS::boxcox(fit, lambda = seq_range, plotit = F)
  best_lambda <- bc$x[which.max(bc$y)]
  return(best_lambda)
}

# 4. Initialize an empty data frame to store the results
lambda_res <- data.frame(
  imputation  = integer(),
  outcome     = character(),
  model       = character(),
  best_lambda = numeric(),
  stringsAsFactors = FALSE
)

# 5. Loop over each imputed dataset
for (i in seq_len(m)) {
  imputed_df <- imputed_data_list[[i]]  # same as complete(pmm_proj_multiimp, i)

  # Loop over each outcome variable
  for (y in outcomes) {

    # Build the formulas for each model
    formula1 <- as.formula(paste(
      y, "~ c66_46_q1_ht + c66_46_c_bmi_cat + c66_46_ht_med + c66_46_edu +",
      "c66_46_smok + c66_46_alc + c66_46_phys + c66_46_occup +  +",
      "sex + c66_46_diab + c66_46_hpcl + c66_46_season + y1_avg_temp + day_mean + c66_46_urban_rural"
    ))

    formula2 <- as.formula(paste(
      y, "~ c66_46_q1_ht + c66_46_c_bmi_cat + c66_46_ht_med + c66_46_edu +",
      "c66_46_smok + c66_46_alc + c66_46_phys + c66_46_occup +  +",
      "sex + c66_46_diab + c66_46_hpcl + c66_46_season + y1_sd_temp + day_mean + c66_46_urban_rural"
    ))

    formula3 <- as.formula(paste(
      y, "~ c66_46_q1_ht + c66_46_c_bmi_cat + c66_46_ht_med + c66_46_edu +",
      "c66_46_smok + c66_46_alc + c66_46_phys + c66_46_occup +  +",
      "sex + c66_46_diab + c66_46_hpcl + c66_46_season + y1_app_temp_avg_46 + day_mean + c66_46_urban_rural"
    ))

    formula4 <- as.formula(paste(
      y, "~ c66_46_q1_ht + c66_46_c_bmi_cat + c66_46_ht_med + c66_46_edu +",
      "c66_46_smok + c66_46_alc + c66_46_phys + c66_46_occup +  +",
      "sex + c66_46_diab + c66_46_hpcl + c66_46_season + y1_app_temp_sd_46 + day_mean + c66_46_urban_rural"
    ))

    # Fit models for this imputation & outcome
    fit1 <- lm(formula1, data = imputed_df)
    fit2 <- lm(formula2, data = imputed_df)
    fit3 <- lm(formula3, data = imputed_df)
    fit4 <- lm(formula4, data = imputed_df)

    # Get best lambda from each model via our helper
    best_lambda1 <- boxcox_best_lambda(fit1) 
    best_lambda2 <- boxcox_best_lambda(fit2)
    best_lambda3 <- boxcox_best_lambda(fit3)
    best_lambda4 <- boxcox_best_lambda(fit4)

    # Store results in lambda_res
    lambda_res <- rbind(
      lambda_res,
      data.frame(
        imputation  = i,
        outcome     = y,
        model       = "y1_avg_temp",
        best_lambda = best_lambda1,
        stringsAsFactors = FALSE
      ),
      data.frame(
        imputation  = i,
        outcome     = y,
        model       = "y1_sd_temp",
        best_lambda = best_lambda2,
        stringsAsFactors = FALSE
      ),
      data.frame(
        imputation  = i,
        outcome     = y,
        model       = "y1_app_temp_avg_46",
        best_lambda = best_lambda3,
        stringsAsFactors = FALSE
      ),
      data.frame(
        imputation  = i,
        outcome     = y,
        model       = "y1_app_temp_sd_46",
        best_lambda = best_lambda4,
        stringsAsFactors = FALSE
      )
    )
  }
}

# Inspect the collected lambda estimates
lambda_res %>% 
  group_by(outcome, model) %>% 
  summarise(
    mean_lambda = mean(best_lambda)
  )
  
```

So, we see that we should ln-transform the syst.bp, while diast.bp should be transformed by applying power of -0.5. According to "transform-then-impute" rule we will transform the original variable first and then impute like it was done for main analysis.

```{r}
#Ln transformation of diast.bp and -0.5 for systolic
c66_project_df_trans <- c66_project_df %>% 
  mutate(c66_46_avrsybp = -1*c66_46_avrsybp^(-0.5), c66_46_avrdibp = log(c66_46_avrdibp)) 

c66_project_df_trans <- c66_project_df %>% 
  mutate(c66_46_avrsybp = log(c66_46_avrsybp), c66_46_avrdibp = log(c66_46_avrdibp)) 
```

## Imputation procedure

```{r}
#| warning: TRUE

# Record the start time
start_time <- Sys.time()
set.seed(2025)

# 1) First run mice with maxit=0 to get the initial predictor matrix
pmm_proj_multiimp_trans <- futuremice(
  c66_project_df_trans, 
  maxit = 0, 
  parallelseed = 2025, 
  defaultMethod = c("pmm", "rf", "rf", "rf"),
  print = FALSE
)

# 2) Extract the predictor matrix
pred <- pmm_proj_multiimp_trans$pred

# -- PART A: Set combined variables as zero predictors for variables that were used to create them in order to avoid circularity
pred[c("c66_46_avrsybp", "c66_46_avrdibp"), 
     "c66_46_c_ht"] <- 0

pred[c("c66_46_avrsybp", "c66_46_avrdibp", "c66_46_c_ht", 
       "c66_46_q1_ht", "c66_46_ht_med"), 
     "c66_46_ht_comb"] <- 0

pred["c66_46_chol", "c66_46_hpcl"] <- 0

pred[c("c66_46_glc","c66_46_gluc2h", "c66_46_q1_dbt1", "c66_46_q1_dbt2"),
     "c66_46_diab"] <- 0

pred[c("c66_46_season"), "c66_46_season_c_w"] <- 0

pred["c66_46_c_bmi_cat" ,"c66_46_c_bmi_cat"] <- 0

#passive imputation of the bmi_categorical variable
meth = pmm_proj_multiimp_trans$method

meth["c66_46_season_c_w"] <- "~I(
  factor(
    ifelse(
      c66_46_season == 'Winter' | c66_46_season == 'Spring',
      'Cold',
      'Warm'
    ),
    levels = c('Cold', 'Warm')
  )
)"

meth["c66_46_c_ht"] <- "~I(
  factor(
    ifelse(
      c66_46_avrsybp >= 140 | c66_46_avrdibp >= 90,
      'Yes',
      'No'
    ),
    levels = c('No', 'Yes')
  )
)"

meth["c66_46_c_bmi_cat"] <- "~I(
  factor(
    ifelse(c66_46_c_bmi_cat < 30, 'Nonobese', 'Obese'), 
    levels = c('Nonobese', 'Obese')
  )
)"

meth["c66_46_diab"] <- "~I(
  factor(
    ifelse(
      c66_46_gluc2h >= 11.1 | 
      (c66_46_q1_dbt1 == 'Yes' | c66_46_q1_dbt2 == 'Yes') | 
      c66_46_glc >= 7.0,
      'Yes',
      'No'
    ),
    levels = c('No', 'Yes')
  )
)"


meth["c66_46_ht_comb"] <- "~I(
  factor(
    ifelse(c66_46_ht_med == 'Yes' | c66_46_q1_ht == 'Yes' | c66_46_c_ht == 'Yes', 
           'Yes', 
           'No'),
    levels = c('No', 'Yes')
  )
)"


meth["c66_46_hpcl"] <- "~I(
  factor(
    ifelse(c66_46_chol >= 6.216, 'Yes', 'No'),
    levels = c('No','Yes')
  )
)"


# -- PART B: Impute y1_app_temp_avg_46 from original climate variables (no reference to app_temp_sd_46)
#set 0 variables as predictors for apparent temp. variables
pred["y1_app_temp_avg_46", ] <- 0
pred["y1_app_temp_sd_46", ] <- 0

#dont use apparent as predictor due to high correl(>0.9) with avg.temp and temp.sd
pred[ ,"y1_app_temp_avg_46"] <- 0
pred[ ,"y1_app_temp_sd_46"] <- 0

#dont use CIMT as predictor to high missingness(>50%)
pred[ ,"c66_46_cimt_l_avg"] <- 0
pred[ ,"c66_46_cimt_r_avg"] <- 0

pred["y1_app_temp_avg_46", 
     c("y1_avg_temp", "y1_avg_rh", "y1_avg_ws", 
       "y1_sd_temp", "y1_sd_rh", "y1_sd_ws")] <- 1

# Exclude y1_app_temp_avg_46 from predicting those climate variables
pred[c("y1_avg_temp", "y1_avg_rh", "y1_avg_ws",
       "y1_sd_temp", "y1_sd_rh", "y1_sd_ws"), 
     "y1_app_temp_avg_46"] <- 0

# -- PART C: Impute y1_app_temp_sd_46 from original climate vars (no reference to app_temp_avg_46)
pred["y1_app_temp_sd_46", 
     c("y1_sd_temp", "y1_sd_rh", "y1_sd_ws", 
       "y1_avg_temp", "y1_avg_rh", "y1_avg_ws")] <- 1

# Exclude y1_app_temp_sd_46 from predicting those original variables
pred[c("y1_sd_temp", "y1_sd_rh", "y1_sd_ws",
       "y1_avg_temp", "y1_avg_rh", "y1_avg_ws"),
     "y1_app_temp_sd_46"] <- 0

# -- PART D: Explicitly set no cross-prediction between y1_app_temp_avg_46 and y1_app_temp_sd_46
pred["y1_app_temp_avg_46", "y1_app_temp_sd_46"] <- 0
pred["y1_app_temp_sd_46", "y1_app_temp_avg_46"] <- 0

# 3) Check the final predictor matrix if you wish
pred

# 4) Run the actual imputation with maxit=10
pmm_proj_multiimp_trans <- futuremice(
  c66_project_df_trans, 
  pred = pred,
  method = meth,
  defaultMethod = c("pmm", "rf", "rf", "rf"),
  m = 100, 
  maxit = 40, 
  parallelseed = 2025,
  print = FALSE
)

options(max.print = 10000)

# Record the end time
end_time <- Sys.time()

# Calculate and print the elapsed time
elapsed_time <- end_time - start_time
print(elapsed_time)
```

## Filtering our rows with imputed outcome and exposure values

After performing imputation we delete rows with imputed BP outcome values and exposure + delete CIMT outcome variables:

```{r}
# Convert the mids object to long format and include the original data (.imp == 0)
long_data <- complete(pmm_proj_multiimp_trans, "long", include = TRUE)

# Identify rows in the original data (.imp == 0) with missing outcome or exposure
missing_rows <- long_data %>%
  filter(.imp == 0 & (is.na(c66_46_avrsybp) | is.na(y1_avg_temp))) %>%
  pull(.id)  # Extract the .id values of these rows

# Filter out these rows from all imputations
filtered_data <- long_data %>%
  filter(!(.id %in% missing_rows))%>%
  dplyr::select(-c66_46_cimt_r_avg, -c66_46_cimt_l_avg)

# Convert the filtered data back to a mids object
pmm_proj_multiimp_trans <- as.mids(filtered_data)
```

Create an averaged dataset for plots:

```{r}
# A mode function for factor columns
# mfv1 is from statip, or you could define your own:
# get_mode <- function(x) { ... }

bp_df_plot <- pmm_proj_multiimp_trans %>%
  mice::complete("long") %>%
  filter(.imp != 0) %>%   # exclude the original data
  group_by(.id) %>%
  summarise(
    # 1) Retain the project_id for each group 
    #    (assuming one project_id per .id group)
    project_ID = first(project_ID),
    
    # 2) Summarize numeric columns with the mean
    across(
      .cols = where(is.numeric),
      .fns = ~ mean(.x, na.rm = TRUE)
    ),
    
    # 3) Summarize factor columns with the single most frequent value
    across(
      .cols = where(is.factor),
      .fns = ~ mfv1(.x)
    )
  )%>%
  ungroup()
```

### 2.1 Systolic blood pressure

#### 2.1.1 Apparent average temperature

##### 2.1.1.4 Fully adjusted

Pool estimates over all datasets with minimal set of variables:

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp_trans, 
               lm(c66_46_avrsybp ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)
```

Pooled b1 for app.temp is -0.88 (p\<0.001) \[-1.14 : -0.63\]. The effect size decreased by 0.17. c66_46_urban_rural, c66_46_edu, c66_46_q1_ht, sex in combination drop the effect size from 1.05 to -0.61. c66_46_alc and increase the effect size to -1.42. Standard error is 0.13 and did not change.

**Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
mi.anova(pmm_proj_multiimp_trans,
         "c66_46_avrsybp ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean", type = 3)
```

R2=0.18. All categorical variables are stat. significant except phys, occup, diab, hpcl, day_mean. An interesting finding is that the season is stat. significant, however the day_mean is not. At the same time season was not selected in any of the optimized model, but day_mean was.

###### Model diagnostics.

Number of imputations

```{r}
howManyImputations::how_many_imputations(lm_models)
```

7 imputations needed. **Constant variance assumption**

```{r}
imputed_dataset = bp_df_plot
lm_model = lm(c66_46_avrsybp ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = imputed_dataset)

#residual plot of predictors vs resid and fitted vs resid = constant variance
car::residualPlots(lm_model,
                   pch=20, col="gray",
                   fitted = T,
                   ask = F, layout = c(2,2),
                   tests = F, quadratic = F)


ols_test_breusch_pagan(lm_model)
```

We see decrease in variance from 140. However, the violation is not serious? in this case, so we can say that the assumption is satisfied.

**Linearity assumption**

```{r}
car::crPlots(lm_model, terms = ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean,
             pch=20, col="gray",
             smooth = list(smoother=car::gamLine), ask = F)
```

Non-linearity does happen because of the high leveraged values in the range of -5 - -3. In other, plot looks linear.

**Normality assumption**

```{r}
source("Functions_rmph.R")

check_normality(lm_model)
```

In the **left panel** (histogram of residuals) we see that observed residuals almost perfectly normally distributed.

In the **right panel** (a QQ plot) there is a lack of normality in the tails of the distribution. Since the sample size is huge (n = 5661, obs \> predictors), non-normality unlikely impact the results.

**Effect of imputation on estimates (riv, lambda, fmi)**

```{r}
pooled_results
```

All variables have riv (relative increase in variance due to nonresponse), lambda (the proportion of the variation attributable to the missing data), fmi (the fraction of information missing due to nonresponse) less than 0.2 which can be interpreted as modest.

###### Pooled Radj and BIC for linear model:

```{r}
pool.r.squared(lm_models, adjusted = T)

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(lm_models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(lm_models$analyses[[i]])$BIC,
         Radj = glance(lm_models$analyses[[i]])$adj.r.squared)
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = fisher.trans(Radj)) %>%
  summarise(pooled_Radj = fisher.backtrans(mean(Radj_trans)),  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

Pooled Radj is 0.25, pooled BIC is 45957.

###### Visualizing the adjusted relationships

```{r}
car::avPlots(lm_model, terms = c66_46_avrsybp ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, 
             col = "darkgray", 
             col.lines = "blue",
             pch = 16,
             id = F, ask = F)
```

Negative linear relationship.

#### 2.1.2 Apparent standard deviation of average temperature

##### 2.1.2.4 Fully adjusted(-)

Pool estimates over all datasets with minimal set of variables:

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp_trans, 
               lm(c66_46_avrsybp ~ y1_app_temp_sd_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)
```

Pooled b1 for app.temp is 0.8 (p=0.01) \[0.16, 1.44\]. The SE increased from 0.25 to 0.33 but now it is smaller than estimate. This might be due to the fact that important confounders were controlled for and now we have a estimate closed to the true value and we "unmask" true relationship. This change happens due to ap_age variable

**Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
mi.anova(pmm_proj_multiimp_trans,
         "c66_46_avrsybp ~ y1_app_temp_sd_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean", type = 3)
```

R2=0.18. All categorical variables are stat. significant except phys, occup, diab, hpcl, day_mean. An interesting finding is that the season is stat. significant, however the day_mean is not. At the same time season was not selected in any of the optimized model, but day_mean was.

###### Model diagnostics.

Number of imputations

```{r}
howManyImputations::how_many_imputations(lm_models)
```

6 imputations needed.

**Constant variance assumption**

```{r}
imputed_dataset = bp_df_plot
lm_model = lm(c66_46_avrsybp ~ y1_app_temp_sd_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = imputed_dataset)

#residual plot of predictors vs resid and fitted vs resid = constant variance
car::residualPlots(lm_model,
                   pch=20, col="gray",
                   fitted = T,
                   ask = F, layout = c(2,2),
                   tests = F, quadratic = F)
```

We see decrease in variance from 140. However, the violation is not serious? in this case, so we can say that the assumption is satisfied.

**Linearity assumption**

```{r}
car::crPlots(lm_model, terms = ~ y1_app_temp_sd_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                   sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean,
             pch=20, col="gray",
             smooth = list(smoother=car::gamLine), ask = F)
```

Linear relationship.

**Normality assumption**

```{r}
source("Functions_rmph.R")

check_normality(lm_model)
```

In the **left panel** (histogram of residuals) we see that observed residuals almost perfectly normally distributed.

In the **right panel** (a QQ plot) there is a lack of normality in the tails of the distribution. Since the sample size is huge (n = 5661, obs \> predictors), non-normality unlikely impact the results.

**Effect of imputation on estimates (riv, lambda, fmi)**

```{r}
pooled_results
```

All variables have riv (relative increase in variance due to nonresponse), lambda (the proportion of the variation attributable to the missing data), fmi (the fraction of information missing due to nonresponse) less than 0.2 which can be interpreted as modest.

###### Pooled Radj and BIC for linear model:

```{r}
pool.r.squared(lm_models, adjusted = T)

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(lm_models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(lm_models$analyses[[i]])$BIC,
         Radj = glance(lm_models$analyses[[i]])$adj.r.squared)
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = fisher.trans(Radj)) %>%
  summarise(pooled_Radj = fisher.backtrans(mean(Radj_trans)),  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

Pooled Radj is 0.25, pooled BIC is 45998.

###### Visualizing the adjusted relationships

```{r}
car::avPlots(lm_model, terms = c66_46_avrsybp ~ y1_app_temp_sd_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, 
             col = "darkgray", 
             col.lines = "blue",
             pch = 16,
             id = F, ask = F)
```

Positive linear relationship.

### 2.2 Diastolic blood pressure

#### 2.2.1 Apparent average temperature

##### 2.2.1.4 Fully adjusted

Pool estimates over all datasets with minimal set of variables:

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp_trans, 
               lm(c66_46_avrdibp ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)
```

Pooled b1 for app.temp is -0.63 (p\<0.001) \[-0.80 -0.46\]. c66_46_urban_rural, c66_46_edu, c66_46_q1_ht, sex in combination drop the effect size from 1.05 to -0.61. c66_46_alc and increase the effect size. Standard error is 0.09 and did not change.

**Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
mi.anova(pmm_proj_multiimp_trans,
         "c66_46_avrdibp ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean", type = 3)
```

R2=0.18. All categorical variables are stat. significant except phys, occup, diab, season, day_mean. Both day_mean and season are not included in the optimized model.

###### Model diagnostics.

Number of imputations

```{r}
howManyImputations::how_many_imputations(lm_models)
```

7 imputations needed. **Constant variance assumption**

```{r}
imputed_dataset = bp_df_plot
lm_model = lm(c66_46_avrdibp ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = imputed_dataset)

#residual plot of predictors vs resid and fitted vs resid = constant variance
car::residualPlots(lm_model,
                   pch=20, col="gray",
                   fitted = T,
                   ask = F, layout = c(2,2),
                   tests = F, quadratic = F)
```

We see decrease in variance from 95. However, the violation is not serious? in this case, so we can say that the assumption is satisfied.

**Linearity assumption**

```{r}
car::crPlots(lm_model, terms = ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean,
             pch=20, col="gray",
             smooth = list(smoother=car::gamLine), ask = F)
```

Linear.

**Normality assumption**

```{r}
source("Functions_rmph.R")

check_normality(lm_model)
```

In the **left panel** (histogram of residuals) we see that observed residuals almost perfectly normally distributed.

In the **right panel** (a QQ plot) there is a lack of normality in the tails of the distribution. Since the sample size is huge (n = 5661, obs \> predictors), non-normality unlikely impact the results.

**Effect of imputation on estimates (riv, lambda, fmi)**

```{r}
pooled_results
```

All variables have riv (relative increase in variance due to nonresponse), lambda (the proportion of the variation attributable to the missing data), fmi (the fraction of information missing due to nonresponse) less than 0.2 which can be interpreted as modest.

###### Pooled Radj and BIC for linear model:

```{r}
pool.r.squared(lm_models, adjusted = T)

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(lm_models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(lm_models$analyses[[i]])$BIC,
         Radj = glance(lm_models$analyses[[i]])$adj.r.squared)
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = fisher.trans(Radj)) %>%
  summarise(pooled_Radj = fisher.backtrans(mean(Radj_trans)),  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

Pooled Radj is 0.26, pooled BIC is 41400.

###### Visualizing the adjusted relationships

```{r}
car::avPlots(lm_model, terms = c66_46_avrdibp ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, 
             col = "darkgray", 
             col.lines = "blue",
             pch = 16,
             id = F, ask = F)
```

Negative linear relationship.

#### 2.2.2 Apparent standard deviation of average temperature

##### 2.2.2.4 Fully adjusted(-)

Pool estimates over all datasets with minimal set of variables:

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp_trans, 
               lm(c66_46_avrdibp ~ y1_app_temp_sd_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2) 	
```

Pooled b1 for app.temp is 0.8 (p\<0.01) \[0.34, 1.20\].

**Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
mi.anova(pmm_proj_multiimp_trans,
         "c66_46_avrdibp ~ y1_app_temp_sd_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean", type = 3)
```

R2=0.18. All categorical variables are stat. significant except phys, occup, diab, season, day_mean.

###### Model diagnostics.

Number of imputations

```{r}
howManyImputations::how_many_imputations(lm_models)
```

6 imputations needed.

**Constant variance assumption**

```{r}
imputed_dataset = bp_df_plot
lm_model = lm(c66_46_avrdibp ~ y1_app_temp_sd_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = imputed_dataset)

#residual plot of predictors vs resid and fitted vs resid = constant variance
car::residualPlots(lm_model,
                   pch=20, col="gray",
                   fitted = T,
                   ask = F, layout = c(2,2),
                   tests = F, quadratic = F)
```

We see decrease in variance from 140. However, the violation is not serious? in this case, so we can say that the assumption is satisfied.

**Linearity assumption**

```{r}
car::crPlots(lm_model, terms = ~ y1_app_temp_sd_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean,
             pch=20, col="gray",
             smooth = list(smoother=car::gamLine), ask = F)
```

Linear relationship.

**Normality assumption**

```{r}
source("Functions_rmph.R")

check_normality(lm_model)
```

In the **left panel** (histogram of residuals) we see that observed residuals almost perfectly normally distributed.

In the **right panel** (a QQ plot) there is a lack of normality in the tails of the distribution. Since the sample size is huge (n = 5661, obs \> predictors), non-normality unlikely impact the results.

**Effect of imputation on estimates (riv, lambda, fmi)**

```{r}
pooled_results
```

All variables have riv (relative increase in variance due to nonresponse), lambda (the proportion of the variation attributable to the missing data), fmi (the fraction of information missing due to nonresponse) less than 0.2 which can be interpreted as modest.

###### Pooled Radj and BIC for linear model:

```{r}
pool.r.squared(lm_models, adjusted = T)

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(lm_models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(lm_models$analyses[[i]])$BIC,
         Radj = glance(lm_models$analyses[[i]])$adj.r.squared)
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = fisher.trans(Radj)) %>%
  summarise(pooled_Radj = fisher.backtrans(mean(Radj_trans)),  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

Pooled Radj is 0.26, pooled BIC is 41441.

###### Visualizing the adjusted relationships

```{r}
car::avPlots(lm_model, terms = c66_46_avrdibp ~ y1_app_temp_sd_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, 
             col = "darkgray", 
             col.lines = "blue",
             pch = 16,
             id = F, ask = F)
```

Positive linear relationship.

# Outliers and influential observations

## Dataset with outliers and infl. observations

First collect unique outliers in the dataframe. Then apply winsorizing to them in the nonimputed dataset.

```{r}
#stud_res #outliers
#cook_res #cooks distance
#dfbetas_res #dfbetas

#get index of outliers that occur > (number of imp dataset*2*4)/2 times
st_res_obs_num = stud_res %>%
  group_by(obs_num) %>%
  count() %>%
  filter(n >= (m*2*4)/4) %>%
  select(-n)

#get index of outliers that occur > (number of imp dataset*2*4)/2 times

cook_obs_num = cook_res %>%
  group_by(obs_num) %>%
  count() %>%
  filter(n >= (m*2*4)/4) %>%
  select(-n)

#get index of outliers that occur > (number of imp dataset*2*4)/2 times

dfbetas_obs_num = dfbetas_res %>%
  group_by(obs_num) %>%
  count() %>%
  filter(n >= (m*2*4)/4) %>%
  select(-n)

#unique indexes of influential obs + outliers
infl_obs_bp = unique(rbind(st_res_obs_num, cook_obs_num, dfbetas_obs_num))

# Extract valid row indices into a vector
valid_rows <- infl_obs_bp$obs_num

# Extract project IDs corresponding to those row indices in the *imputed* dataset
# Use complete(...) 'pmm_proj_multiimp' is your mids object.
bp_proj_id_infl <- complete(pmm_proj_multiimp)[valid_rows, "project_ID"]
```

## Delete outliers and influential observations

```{r}
non_imputed_dataset_filtered <- c66_project_df %>%
  filter(!project_ID %in% bp_proj_id_infl)
```

## Imputation

Next we will perform imputation of the "cleaned" dataset.

```{r}
#| warning: TRUE

# Record the start time
start_time <- Sys.time()
set.seed(2025)

# 1) First run mice with maxit=0 to get the initial predictor matrix
pmm_proj_multiimp_trans <- futuremice(
  non_imputed_dataset_filtered, 
  maxit = 0, 
  parallelseed = 2025, 
  defaultMethod = c("pmm", "rf", "rf", "rf"),
  print = FALSE
)

# 2) Extract the predictor matrix
pred <- pmm_proj_multiimp_trans$pred

# -- PART A: Set combined variables as zero predictors for variables that were used to create them in order to avoid circularity
pred[c("c66_46_avrsybp", "c66_46_avrdibp"), 
     "c66_46_c_ht"] <- 0

pred[c("c66_46_avrsybp", "c66_46_avrdibp", "c66_46_c_ht", 
       "c66_46_q1_ht", "c66_46_ht_med"), 
     "c66_46_ht_comb"] <- 0

pred["c66_46_chol", "c66_46_hpcl"] <- 0

pred[c("c66_46_glc","c66_46_gluc2h", "c66_46_q1_dbt1", "c66_46_q1_dbt2"),
     "c66_46_diab"] <- 0

pred[c("c66_46_season"), "c66_46_season_c_w"] <- 0

pred["c66_46_c_bmi" ,"c66_46_c_bmi_cat"] <- 0

#passive imputation of the bmi_categorical variable
meth = pmm_proj_multiimp_trans$method

meth["c66_46_season_c_w"] <- "~I(
  factor(
    ifelse(
      c66_46_season == 'Winter' | c66_46_season == 'Spring',
      'Cold',
      'Warm'
    ),
    levels = c('Cold', 'Warm')
  )
)"

meth["c66_46_c_ht"] <- "~I(
  factor(
    ifelse(
      c66_46_avrsybp >= 140 | c66_46_avrdibp >= 90,
      'Yes',
      'No'
    ),
    levels = c('No', 'Yes')
  )
)"

meth["c66_46_c_bmi_cat"] <- "~I(
  factor(
    ifelse(c66_46_c_bmi < 30, 'Nonobese', 'Obese'), 
    levels = c('Nonobese', 'Obese')
  )
)"

meth["c66_46_diab"] <- "~I(
  factor(
    ifelse(
      c66_46_gluc2h >= 11.1 | 
      (c66_46_q1_dbt1 == 'Yes' | c66_46_q1_dbt2 == 'Yes') | 
      c66_46_glc >= 7.0,
      'Yes',
      'No'
    ),
    levels = c('No', 'Yes')
  )
)"


meth["c66_46_ht_comb"] <- "~I(
  factor(
    ifelse(c66_46_ht_med == 'Yes' | c66_46_q1_ht == 'Yes' | c66_46_c_ht == 'Yes', 
           'Yes', 
           'No'),
    levels = c('No', 'Yes')
  )
)"


meth["c66_46_hpcl"] <- "~I(
  factor(
    ifelse(c66_46_chol >= 6.216, 'Yes', 'No'),
    levels = c('No','Yes')
  )
)"


# -- PART B: Impute y1_app_temp_avg_46 from original climate variables (no reference to app_temp_sd_46)
#set 0 variables as predictors for apparent temp. variables
pred["y1_app_temp_avg_46", ] <- 0
pred["y1_app_temp_sd_46", ] <- 0

#dont use apparent as predictor due to high correl(>0.9) with avg.temp and temp.sd
pred[ ,"y1_app_temp_avg_46"] <- 0
pred[ ,"y1_app_temp_sd_46"] <- 0

#dont use CIMT as predictor to high missingness(>50%)
pred[ ,"c66_46_cimt_l_avg"] <- 0
pred[ ,"c66_46_cimt_r_avg"] <- 0

pred["y1_app_temp_avg_46", 
     c("y1_avg_temp", "y1_avg_rh", "y1_avg_ws", 
       "y1_sd_temp", "y1_sd_rh", "y1_sd_ws")] <- 1

# Exclude y1_app_temp_avg_46 from predicting those climate variables
pred[c("y1_avg_temp", "y1_avg_rh", "y1_avg_ws",
       "y1_sd_temp", "y1_sd_rh", "y1_sd_ws"), 
     "y1_app_temp_avg_46"] <- 0

# -- PART C: Impute y1_app_temp_sd_46 from original climate vars (no reference to app_temp_avg_46)
pred["y1_app_temp_sd_46", 
     c("y1_sd_temp", "y1_sd_rh", "y1_sd_ws", 
       "y1_avg_temp", "y1_avg_rh", "y1_avg_ws")] <- 1

# Exclude y1_app_temp_sd_46 from predicting those original variables
pred[c("y1_sd_temp", "y1_sd_rh", "y1_sd_ws",
       "y1_avg_temp", "y1_avg_rh", "y1_avg_ws"),
     "y1_app_temp_sd_46"] <- 0

# -- PART D: Explicitly set no cross-prediction between y1_app_temp_avg_46 and y1_app_temp_sd_46
pred["y1_app_temp_avg_46", "y1_app_temp_sd_46"] <- 0
pred["y1_app_temp_sd_46", "y1_app_temp_avg_46"] <- 0

# 3) Check the final predictor matrix if you wish
pred

# 4) Run the actual imputation with maxit=10
pmm_proj_multiimp_trans <- futuremice(
  non_imputed_dataset_filtered, 
  pred = pred,
  method = meth,
  defaultMethod = c("pmm", "rf", "rf", "rf"),
  m = 100, 
  maxit = 40, 
  parallelseed = 2025,
  print = FALSE
)

options(max.print = 10000)

# Record the end time
end_time <- Sys.time()

# Calculate and print the elapsed time
elapsed_time <- end_time - start_time
print(elapsed_time)
```

## Filtering our rows with imputed outcome and exposure values

After performing imputation we delete rows with imputed BP outcome values and exposure + delete CIMT outcome variables:

```{r}
# Convert the mids object to long format and include the original data (.imp == 0)
long_data <- complete(pmm_proj_multiimp_trans, "long", include = TRUE)

# Identify rows in the original data (.imp == 0) with missing outcome or exposure
missing_rows <- long_data %>%
  filter(.imp == 0 & (is.na(c66_46_avrsybp) | is.na(y1_avg_temp))) %>%
  pull(.id)  # Extract the .id values of these rows

# Filter out these rows from all imputations
filtered_data <- long_data %>%
  filter(!(.id %in% missing_rows))%>%
  dplyr::select(-c66_46_cimt_r_avg, -c66_46_cimt_l_avg)

# Convert the filtered data back to a mids object
pmm_proj_multiimp_trans <- as.mids(filtered_data)
```

Create an averaged dataset for plots:

```{r}
# A mode function for factor columns
# mfv1 is from statip, or you could define your own:
# get_mode <- function(x) { ... }

bp_df_plot <- pmm_proj_multiimp_trans %>%
  mice::complete("long") %>%
  filter(.imp != 0) %>%   # exclude the original data
  group_by(.id) %>%
  summarise(
    # 1) Retain the project_id for each group 
    #    (assuming one project_id per .id group)
    project_ID = first(project_ID),
    
    # 2) Summarize numeric columns with the mean
    across(
      .cols = where(is.numeric),
      .fns = ~ mean(.x, na.rm = TRUE)
    ),
    
    # 3) Summarize factor columns with the single most frequent value
    across(
      .cols = where(is.factor),
      .fns = ~ mfv1(.x)
    )
  )%>%
  ungroup()
```

### 2.1 Systolic blood pressure

#### 2.1.1 Apparent average temperature

##### 2.1.1.1 Unadjusted

Pool estimates over all datasets with minimal set of variables:

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp_trans, 
               lm(c66_46_avrsybp ~ y1_app_temp_avg_46))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)
```

Pooled b1 for app.temp is -0.88 (p\<0.001) \[-1.14 : -0.63\]. The effect size decreased by 0.17. c66_46_urban_rural, c66_46_edu, c66_46_q1_ht, sex in combination drop the effect size from 1.05 to -0.61. c66_46_alc and increase the effect size to -1.42. Standard error is 0.13 and did not change.

###### Pooled Radj and BIC for linear model:

```{r}
pool.r.squared(lm_models, adjusted = T)

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(lm_models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(lm_models$analyses[[i]])$BIC,
         Radj = glance(lm_models$analyses[[i]])$adj.r.squared)
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = fisher.trans(Radj)) %>%
  summarise(pooled_Radj = fisher.backtrans(mean(Radj_trans)),  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

Pooled Radj is 0.25, pooled BIC is 45957.

##### 2.1.1.2 Minimally adjusted

Pool estimates over all datasets with minimal set of variables:

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp_trans, 
               lm(c66_46_avrsybp ~ y1_app_temp_avg_46 + c66_46_edu +
                c66_46_occup + sex + 
                c66_46_urban_rural + c66_46_season))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)
```

Pooled b1 for app.temp is -0.88 (p\<0.001) \[-1.14 : -0.63\]. The effect size decreased by 0.17. c66_46_urban_rural, c66_46_edu, c66_46_q1_ht, sex in combination drop the effect size from 1.05 to -0.61. c66_46_alc and increase the effect size to -1.42. Standard error is 0.13 and did not change.

**Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
mi.anova(pmm_proj_multiimp_trans,
         "c66_46_avrsybp ~ y1_app_temp_avg_46 + c66_46_edu +
                c66_46_occup + sex + 
                c66_46_ht_med", type = 3)
```

R2=0.18. All categorical variables are stat. significant except phys, occup, diab, hpcl, day_mean. An interesting finding is that the season is stat. significant, however the day_mean is not. At the same time season was not selected in any of the optimized model, but day_mean was.

###### Pooled Radj and BIC for linear model:

```{r}
pool.r.squared(lm_models, adjusted = T)

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(lm_models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(lm_models$analyses[[i]])$BIC,
         Radj = glance(lm_models$analyses[[i]])$adj.r.squared)
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = fisher.trans(Radj)) %>%
  summarise(pooled_Radj = fisher.backtrans(mean(Radj_trans)),  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

Pooled Radj is 0.25, pooled BIC is 45957.

##### 2.1.1.3 Optimized

Pool estimates over all datasets with minimal set of variables:

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp_trans, 
               lm(c66_46_avrsybp ~ y1_app_temp_avg_46  +  
                       c66_46_smok + 
                       c66_46_c_bmi_cat + c66_46_edu +
                       c66_46_ht_med + c66_46_q1_ht +
                       c66_46_urban_rural + day_mean +
                       sex + c66_46_alc))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)
```

Pooled b1 for app.temp is -0.88 (p\<0.001) \[-1.14 : -0.63\]. The effect size decreased by 0.17. c66_46_urban_rural, c66_46_edu, c66_46_q1_ht, sex in combination drop the effect size from 1.05 to -0.61. c66_46_alc and increase the effect size to -1.42. Standard error is 0.13 and did not change.

**Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
mi.anova(pmm_proj_multiimp_trans,
         "c66_46_avrsybp ~ y1_app_temp_avg_46 +  
                       c66_46_alc +  +
                       c66_46_c_bmi_cat + c66_46_ht_med +
                       c66_46_q1_ht + c66_46_smok +
                       c66_46_urban_rural + day_mean +
                       sex + c66_46_edu", type = 3)
```

R2=0.18. All categorical variables are stat. significant except phys, occup, diab, hpcl, day_mean. An interesting finding is that the season is stat. significant, however the day_mean is not. At the same time season was not selected in any of the optimized model, but day_mean was.

###### Pooled Radj and BIC for linear model:

```{r}
pool.r.squared(lm_models, adjusted = T)

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(lm_models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(lm_models$analyses[[i]])$BIC,
         Radj = glance(lm_models$analyses[[i]])$adj.r.squared)
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = fisher.trans(Radj)) %>%
  summarise(pooled_Radj = fisher.backtrans(mean(Radj_trans)),  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

Pooled Radj is 0.25, pooled BIC is 45957.

##### 2.1.1.4 Fully adjusted

Pool estimates over all datasets with minimal set of variables:

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp_trans, 
               lm(c66_46_avrsybp ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)
```

Pooled b1 for app.temp is -0.88 (p\<0.001) \[-1.14 : -0.63\]. The effect size decreased by 0.17. c66_46_urban_rural, c66_46_edu, c66_46_q1_ht, sex in combination drop the effect size from 1.05 to -0.61. c66_46_alc and increase the effect size to -1.42. Standard error is 0.13 and did not change.

**Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
mi.anova(pmm_proj_multiimp_trans,
         "c66_46_avrsybp ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean", type = 3)
```

R2=0.18. All categorical variables are stat. significant except phys, occup, diab, hpcl, day_mean. An interesting finding is that the season is stat. significant, however the day_mean is not. At the same time season was not selected in any of the optimized model, but day_mean was.

###### Pooled Radj and BIC for linear model:

```{r}
pool.r.squared(lm_models, adjusted = T)

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(lm_models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(lm_models$analyses[[i]])$BIC,
         Radj = glance(lm_models$analyses[[i]])$adj.r.squared)
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = fisher.trans(Radj)) %>%
  summarise(pooled_Radj = fisher.backtrans(mean(Radj_trans)),  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

Pooled Radj is 0.25, pooled BIC is 45957.

#### 2.1.2 Apparent standard deviation of average temperature

##### 2.1.2.1 Unadjusted

Pool estimates over all datasets with minimal set of variables:

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp_trans, 
               lm(c66_46_avrsybp ~ y1_app_temp_sd_46))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)
```

Pooled b1 for app.temp is -0.88 (p\<0.001) \[-1.14 : -0.63\]. The effect size decreased by 0.17. c66_46_urban_rural, c66_46_edu, c66_46_q1_ht, sex in combination drop the effect size from 1.05 to -0.61. c66_46_alc and increase the effect size to -1.42. Standard error is 0.13 and did not change.

###### Pooled Radj and BIC for linear model:

```{r}
pool.r.squared(lm_models, adjusted = T)

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(lm_models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(lm_models$analyses[[i]])$BIC,
         Radj = glance(lm_models$analyses[[i]])$adj.r.squared)
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = fisher.trans(Radj)) %>%
  summarise(pooled_Radj = fisher.backtrans(mean(Radj_trans)),  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

Pooled Radj is 0.25, pooled BIC is 45957.

##### 2.1.2.2 Minimally adjusted

Pool estimates over all datasets with minimal set of variables:

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp_trans, 
               lm(c66_46_avrsybp ~ y1_app_temp_sd_46 + c66_46_edu +
                c66_46_occup + sex + 
                c66_46_urban_rural + c66_46_season))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)
```

Pooled b1 for app.temp is -0.88 (p\<0.001) \[-1.14 : -0.63\]. The effect size decreased by 0.17. c66_46_urban_rural, c66_46_edu, c66_46_q1_ht, sex in combination drop the effect size from 1.05 to -0.61. c66_46_alc and increase the effect size to -1.42. Standard error is 0.13 and did not change.

**Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
mi.anova(pmm_proj_multiimp_trans,
         "c66_46_avrsybp ~ y1_app_temp_sd_46 + c66_46_edu +
                c66_46_occup + sex + 
                c66_46_ht_med", type = 3)
```

R2=0.18. All categorical variables are stat. significant except phys, occup, diab, hpcl, day_mean. An interesting finding is that the season is stat. significant, however the day_mean is not. At the same time season was not selected in any of the optimized model, but day_mean was.

###### Pooled Radj and BIC for linear model:

```{r}
pool.r.squared(lm_models, adjusted = T)

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(lm_models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(lm_models$analyses[[i]])$BIC,
         Radj = glance(lm_models$analyses[[i]])$adj.r.squared)
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = fisher.trans(Radj)) %>%
  summarise(pooled_Radj = fisher.backtrans(mean(Radj_trans)),  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

Pooled Radj is 0.25, pooled BIC is 45957.

##### 2.1.2.3 Optimized

Pool estimates over all datasets with minimal set of variables:

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp_trans, 
               lm(c66_46_avrsybp ~ y1_app_temp_sd_46    +  
                       c66_46_c_bmi_cat +
                       c66_46_edu + c66_46_ht_med +
                       c66_46_q1_ht + 
                       c66_46_smok + c66_46_urban_rural +
                       sex + day_mean + c66_46_alc))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)
```

Pooled b1 for app.temp is -0.88 (p\<0.001) \[-1.14 : -0.63\]. The effect size decreased by 0.17. c66_46_urban_rural, c66_46_edu, c66_46_q1_ht, sex in combination drop the effect size from 1.05 to -0.61. c66_46_alc and increase the effect size to -1.42. Standard error is 0.13 and did not change.

**Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
mi.anova(pmm_proj_multiimp_trans,
         "c66_46_avrsybp ~ y1_app_temp_sd_46    +  
                       c66_46_c_bmi_cat +
                       c66_46_edu + c66_46_ht_med +
                       c66_46_q1_ht + 
                       c66_46_smok + c66_46_urban_rural +
                       sex + day_mean + c66_46_alc", type = 3)
```

R2=0.18. All categorical variables are stat. significant except phys, occup, diab, hpcl, day_mean. An interesting finding is that the season is stat. significant, however the day_mean is not. At the same time season was not selected in any of the optimized model, but day_mean was.

###### Pooled Radj and BIC for linear model:

```{r}
pool.r.squared(lm_models, adjusted = T)

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(lm_models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(lm_models$analyses[[i]])$BIC,
         Radj = glance(lm_models$analyses[[i]])$adj.r.squared)
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = fisher.trans(Radj)) %>%
  summarise(pooled_Radj = fisher.backtrans(mean(Radj_trans)),  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

Pooled Radj is 0.25, pooled BIC is 45957.

##### 2.1.2.4 Fully adjusted(-)

Pool estimates over all datasets with minimal set of variables:

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp_trans, 
               lm(c66_46_avrsybp ~ y1_app_temp_sd_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)
```

Pooled b1 for app.temp is -0.88 (p\<0.001) \[-1.14 : -0.63\]. The effect size decreased by 0.17. c66_46_urban_rural, c66_46_edu, c66_46_q1_ht, sex in combination drop the effect size from 1.05 to -0.61. c66_46_alc and increase the effect size to -1.42. Standard error is 0.13 and did not change.

**Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
mi.anova(pmm_proj_multiimp_trans,
         "c66_46_avrsybp ~ y1_app_temp_sd_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean", type = 3)
```

R2=0.18. All categorical variables are stat. significant except phys, occup, diab, hpcl, day_mean. An interesting finding is that the season is stat. significant, however the day_mean is not. At the same time season was not selected in any of the optimized model, but day_mean was.

###### Pooled Radj and BIC for linear model:

```{r}
pool.r.squared(lm_models, adjusted = T)

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(lm_models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(lm_models$analyses[[i]])$BIC,
         Radj = glance(lm_models$analyses[[i]])$adj.r.squared)
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = fisher.trans(Radj)) %>%
  summarise(pooled_Radj = fisher.backtrans(mean(Radj_trans)),  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

Pooled Radj is 0.25, pooled BIC is 45957.

#### 2.1.3 Average temperature

##### 2.1.3.1 Unadjusted

Pool estimates over all datasets with minimal set of variables:

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp_trans, 
               lm(c66_46_avrsybp ~ y1_avg_temp))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)
```

Pooled b1 for app.temp is -0.88 (p\<0.001) \[-1.14 : -0.63\]. The effect size decreased by 0.17. c66_46_urban_rural, c66_46_edu, c66_46_q1_ht, sex in combination drop the effect size from 1.05 to -0.61. c66_46_alc and increase the effect size to -1.42. Standard error is 0.13 and did not change.

###### Pooled Radj and BIC for linear model:

```{r}
pool.r.squared(lm_models, adjusted = T)

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(lm_models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(lm_models$analyses[[i]])$BIC,
         Radj = glance(lm_models$analyses[[i]])$adj.r.squared)
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = fisher.trans(Radj)) %>%
  summarise(pooled_Radj = fisher.backtrans(mean(Radj_trans)),  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

Pooled Radj is 0.25, pooled BIC is 45957.

##### 2.1.3.2 Minimally adjusted

Pool estimates over all datasets with minimal set of variables:

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp_trans, 
               lm(c66_46_avrsybp ~ y1_avg_temp + c66_46_edu +
                c66_46_occup + sex + 
                c66_46_ht_med))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)
```

Pooled b1 for app.temp is -0.88 (p\<0.001) \[-1.14 : -0.63\]. The effect size decreased by 0.17. c66_46_urban_rural, c66_46_edu, c66_46_q1_ht, sex in combination drop the effect size from 1.05 to -0.61. c66_46_alc and increase the effect size to -1.42. Standard error is 0.13 and did not change.

**Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
mi.anova(pmm_proj_multiimp_trans,
         "c66_46_avrsybp ~ y1_avg_temp + c66_46_edu +
                c66_46_occup + sex + 
                c66_46_ht_med", type = 3)
```

R2=0.18. All categorical variables are stat. significant except phys, occup, diab, hpcl, day_mean. An interesting finding is that the season is stat. significant, however the day_mean is not. At the same time season was not selected in any of the optimized model, but day_mean was.

###### Pooled Radj and BIC for linear model:

```{r}
pool.r.squared(lm_models, adjusted = T)

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(lm_models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(lm_models$analyses[[i]])$BIC,
         Radj = glance(lm_models$analyses[[i]])$adj.r.squared)
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = fisher.trans(Radj)) %>%
  summarise(pooled_Radj = fisher.backtrans(mean(Radj_trans)),  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

Pooled Radj is 0.25, pooled BIC is 45957.

##### 2.1.3.3 Optimized

Pool estimates over all datasets with minimal set of variables:

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp_trans, 
               lm(c66_46_avrsybp ~ y1_avg_temp   +  
                       c66_46_smok +  +
                       c66_46_c_bmi_cat + c66_46_edu +
                       c66_46_ht_med + c66_46_q1_ht +
                       c66_46_urban_rural + day_mean +
                       sex + c66_46_alc))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)
```

Pooled b1 for app.temp is -0.88 (p\<0.001) \[-1.14 : -0.63\]. The effect size decreased by 0.17. c66_46_urban_rural, c66_46_edu, c66_46_q1_ht, sex in combination drop the effect size from 1.05 to -0.61. c66_46_alc and increase the effect size to -1.42. Standard error is 0.13 and did not change.

**Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
mi.anova(pmm_proj_multiimp_trans,
         "c66_46_avrsybp ~ y1_avg_temp   +  
                       c66_46_smok +  +
                       c66_46_c_bmi_cat + c66_46_edu +
                       c66_46_ht_med + c66_46_q1_ht +
                       c66_46_urban_rural + day_mean +
                       sex + c66_46_alc", type = 3)
```

R2=0.18. All categorical variables are stat. significant except phys, occup, diab, hpcl, day_mean. An interesting finding is that the season is stat. significant, however the day_mean is not. At the same time season was not selected in any of the optimized model, but day_mean was.

###### Pooled Radj and BIC for linear model:

```{r}
pool.r.squared(lm_models, adjusted = T)

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(lm_models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(lm_models$analyses[[i]])$BIC,
         Radj = glance(lm_models$analyses[[i]])$adj.r.squared)
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = fisher.trans(Radj)) %>%
  summarise(pooled_Radj = fisher.backtrans(mean(Radj_trans)),  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

Pooled Radj is 0.25, pooled BIC is 45957.

##### 2.1.3.4 Fully adjusted

Pool estimates over all datasets with minimal set of variables:

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp_trans, 
               lm(c66_46_avrsybp ~ y1_avg_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)
```

Pooled b1 for app.temp is -0.88 (p\<0.001) \[-1.14 : -0.63\]. The effect size decreased by 0.17. c66_46_urban_rural, c66_46_edu, c66_46_q1_ht, sex in combination drop the effect size from 1.05 to -0.61. c66_46_alc and increase the effect size to -1.42. Standard error is 0.13 and did not change.

**Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
mi.anova(pmm_proj_multiimp_trans,
         "c66_46_avrsybp ~ y1_avg_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean", type = 3)
```

R2=0.18. All categorical variables are stat. significant except phys, occup, diab, hpcl, day_mean. An interesting finding is that the season is stat. significant, however the day_mean is not. At the same time season was not selected in any of the optimized model, but day_mean was.

###### Pooled Radj and BIC for linear model:

```{r}
pool.r.squared(lm_models, adjusted = T)

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(lm_models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(lm_models$analyses[[i]])$BIC,
         Radj = glance(lm_models$analyses[[i]])$adj.r.squared)
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = fisher.trans(Radj)) %>%
  summarise(pooled_Radj = fisher.backtrans(mean(Radj_trans)),  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

Pooled Radj is 0.25, pooled BIC is 45957.

#### 2.1.4 Standard deviation of average temperature

##### 2.1.4.1 Unadjusted

Pool estimates over all datasets with minimal set of variables:

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp_trans, 
               lm(c66_46_avrsybp ~ y1_sd_temp))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)
```

Pooled b1 for app.temp is -0.88 (p\<0.001) \[-1.14 : -0.63\]. The effect size decreased by 0.17. c66_46_urban_rural, c66_46_edu, c66_46_q1_ht, sex in combination drop the effect size from 1.05 to -0.61. c66_46_alc and increase the effect size to -1.42. Standard error is 0.13 and did not change.

###### Pooled Radj and BIC for linear model:

```{r}
pool.r.squared(lm_models, adjusted = T)

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(lm_models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(lm_models$analyses[[i]])$BIC,
         Radj = glance(lm_models$analyses[[i]])$adj.r.squared)
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = fisher.trans(Radj)) %>%
  summarise(pooled_Radj = fisher.backtrans(mean(Radj_trans)),  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

Pooled Radj is 0.25, pooled BIC is 45957.

##### 2.1.4.2 Minimally adjusted

Pool estimates over all datasets with minimal set of variables:

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp_trans, 
               lm(c66_46_avrsybp ~ y1_sd_temp + c66_46_edu +
                c66_46_occup + sex + 
                c66_46_ht_med))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)
```

Pooled b1 for app.temp is -0.88 (p\<0.001) \[-1.14 : -0.63\]. The effect size decreased by 0.17. c66_46_urban_rural, c66_46_edu, c66_46_q1_ht, sex in combination drop the effect size from 1.05 to -0.61. c66_46_alc and increase the effect size to -1.42. Standard error is 0.13 and did not change.

**Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
mi.anova(pmm_proj_multiimp_trans,
         "c66_46_avrsybp ~ y1_sd_temp + c66_46_edu +
                c66_46_occup + sex + 
                c66_46_ht_med", type = 3)
```

R2=0.18. All categorical variables are stat. significant except phys, occup, diab, hpcl, day_mean. An interesting finding is that the season is stat. significant, however the day_mean is not. At the same time season was not selected in any of the optimized model, but day_mean was.

###### Pooled Radj and BIC for linear model:

```{r}
pool.r.squared(lm_models, adjusted = T)

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(lm_models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(lm_models$analyses[[i]])$BIC,
         Radj = glance(lm_models$analyses[[i]])$adj.r.squared)
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = fisher.trans(Radj)) %>%
  summarise(pooled_Radj = fisher.backtrans(mean(Radj_trans)),  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

Pooled Radj is 0.25, pooled BIC is 45957.

##### 2.1.4.3 Optimized

Pool estimates over all datasets with minimal set of variables:

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp_trans, 
               lm(c66_46_avrsybp ~ y1_sd_temp + c66_46_alc +  
                       c66_46_c_bmi_cat +
                       c66_46_edu + c66_46_ht_med +
                       c66_46_q1_ht + 
                       c66_46_smok + c66_46_urban_rural +
                       sex + day_mean))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)
```

Pooled b1 for app.temp is -0.88 (p\<0.001) \[-1.14 : -0.63\]. The effect size decreased by 0.17. c66_46_urban_rural, c66_46_edu, c66_46_q1_ht, sex in combination drop the effect size from 1.05 to -0.61. c66_46_alc and increase the effect size to -1.42. Standard error is 0.13 and did not change.

**Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
mi.anova(pmm_proj_multiimp_trans,
         "c66_46_avrsybp ~ y1_sd_temp + c66_46_alc +  
                       c66_46_c_bmi_cat +
                       c66_46_edu + c66_46_ht_med +
                       c66_46_q1_ht + 
                       c66_46_smok + c66_46_urban_rural +
                       sex + day_mean", type = 3)
```

R2=0.18. All categorical variables are stat. significant except phys, occup, diab, hpcl, day_mean. An interesting finding is that the season is stat. significant, however the day_mean is not. At the same time season was not selected in any of the optimized model, but day_mean was.

###### Pooled Radj and BIC for linear model:

```{r}
pool.r.squared(lm_models, adjusted = T)

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(lm_models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(lm_models$analyses[[i]])$BIC,
         Radj = glance(lm_models$analyses[[i]])$adj.r.squared)
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = fisher.trans(Radj)) %>%
  summarise(pooled_Radj = fisher.backtrans(mean(Radj_trans)),  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

Pooled Radj is 0.25, pooled BIC is 45957.

##### 2.1.4.4 Fully adjusted(-)

Pool estimates over all datasets with minimal set of variables:

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp_trans, 
               lm(c66_46_avrsybp ~ y1_sd_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)
```

Pooled b1 for app.temp is -0.88 (p\<0.001) \[-1.14 : -0.63\]. The effect size decreased by 0.17. c66_46_urban_rural, c66_46_edu, c66_46_q1_ht, sex in combination drop the effect size from 1.05 to -0.61. c66_46_alc and increase the effect size to -1.42. Standard error is 0.13 and did not change.

**Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
mi.anova(pmm_proj_multiimp_trans,
         "c66_46_avrsybp ~ y1_sd_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean", type = 3)
```

R2=0.18. All categorical variables are stat. significant except phys, occup, diab, hpcl, day_mean. An interesting finding is that the season is stat. significant, however the day_mean is not. At the same time season was not selected in any of the optimized model, but day_mean was.

###### Pooled Radj and BIC for linear model:

```{r}
pool.r.squared(lm_models, adjusted = T)

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(lm_models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(lm_models$analyses[[i]])$BIC,
         Radj = glance(lm_models$analyses[[i]])$adj.r.squared)
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = fisher.trans(Radj)) %>%
  summarise(pooled_Radj = fisher.backtrans(mean(Radj_trans)),  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

Pooled Radj is 0.25, pooled BIC is 45957.

### 2.2 Diastolic blood pressure

#### 2.2.1 Apparent average temperature

##### 2.2.1.1 Unadjusted

Pool estimates over all datasets with minimal set of variables:

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp_trans, 
               lm(c66_46_avrdibp ~ y1_app_temp_avg_46))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)
```

Pooled b1 for app.temp is -0.88 (p\<0.001) \[-1.14 : -0.63\]. The effect size decreased by 0.17. c66_46_urban_rural, c66_46_edu, c66_46_q1_ht, sex in combination drop the effect size from 1.05 to -0.61. c66_46_alc and increase the effect size to -1.42. Standard error is 0.13 and did not change.

###### Pooled Radj and BIC for linear model:

```{r}
pool.r.squared(lm_models, adjusted = T)

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(lm_models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(lm_models$analyses[[i]])$BIC,
         Radj = glance(lm_models$analyses[[i]])$adj.r.squared)
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = fisher.trans(Radj)) %>%
  summarise(pooled_Radj = fisher.backtrans(mean(Radj_trans)),  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

Pooled Radj is 0.25, pooled BIC is 45957.

##### 2.2.1.2 Minimally adjusted

Pool estimates over all datasets with minimal set of variables:

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp_trans, 
               lm(c66_46_avrdibp ~ y1_app_temp_avg_46 + c66_46_edu +
                c66_46_occup + sex + 
                c66_46_urban_rural + c66_46_season))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)
```

Pooled b1 for app.temp is -0.88 (p\<0.001) \[-1.14 : -0.63\]. The effect size decreased by 0.17. c66_46_urban_rural, c66_46_edu, c66_46_q1_ht, sex in combination drop the effect size from 1.05 to -0.61. c66_46_alc and increase the effect size to -1.42. Standard error is 0.13 and did not change.

**Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
mi.anova(pmm_proj_multiimp_trans,
         "c66_46_avrdibp ~ y1_app_temp_avg_46 + c66_46_edu +
                c66_46_occup + sex + 
                c66_46_ht_med", type = 3)
```

R2=0.18. All categorical variables are stat. significant except phys, occup, diab, hpcl, day_mean. An interesting finding is that the season is stat. significant, however the day_mean is not. At the same time season was not selected in any of the optimized model, but day_mean was.

###### Pooled Radj and BIC for linear model:

```{r}
pool.r.squared(lm_models, adjusted = T)

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(lm_models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(lm_models$analyses[[i]])$BIC,
         Radj = glance(lm_models$analyses[[i]])$adj.r.squared)
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = fisher.trans(Radj)) %>%
  summarise(pooled_Radj = fisher.backtrans(mean(Radj_trans)),  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

Pooled Radj is 0.25, pooled BIC is 45957.

##### 2.2.1.3 Optimized

Pool estimates over all datasets with minimal set of variables:

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp_trans, 
               lm(c66_46_avrdibp ~ y1_app_temp_avg_46 +  
                       c66_46_alc + c66_46_c_bmi_cat +
                       c66_46_ht_med +
                       c66_46_q1_ht + 
                       c66_46_urban_rural + 
                       day_mean + sex))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)
```

Pooled b1 for app.temp is -0.88 (p\<0.001) \[-1.14 : -0.63\]. The effect size decreased by 0.17. c66_46_urban_rural, c66_46_edu, c66_46_q1_ht, sex in combination drop the effect size from 1.05 to -0.61. c66_46_alc and increase the effect size to -1.42. Standard error is 0.13 and did not change.

**Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
mi.anova(pmm_proj_multiimp_trans,
         "c66_46_avrdibp ~ y1_app_temp_avg_46 +  
                       c66_46_alc +  +
                       c66_46_c_bmi_cat + 
                       c66_46_ht_med + c66_46_q1_ht +
                       sex + c66_46_phys", type = 3)
```

R2=0.18. All categorical variables are stat. significant except phys, occup, diab, hpcl, day_mean. An interesting finding is that the season is stat. significant, however the day_mean is not. At the same time season was not selected in any of the optimized model, but day_mean was.

###### Pooled Radj and BIC for linear model:

```{r}
pool.r.squared(lm_models, adjusted = T)

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(lm_models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(lm_models$analyses[[i]])$BIC,
         Radj = glance(lm_models$analyses[[i]])$adj.r.squared)
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = fisher.trans(Radj)) %>%
  summarise(pooled_Radj = fisher.backtrans(mean(Radj_trans)),  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

Pooled Radj is 0.25, pooled BIC is 45957.

##### 2.2.1.4 Fully adjusted

Pool estimates over all datasets with minimal set of variables:

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp_trans, 
               lm(c66_46_avrdibp ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)
```

Pooled b1 for app.temp is -0.88 (p\<0.001) \[-1.14 : -0.63\]. The effect size decreased by 0.17. c66_46_urban_rural, c66_46_edu, c66_46_q1_ht, sex in combination drop the effect size from 1.05 to -0.61. c66_46_alc and increase the effect size to -1.42. Standard error is 0.13 and did not change.

**Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
mi.anova(pmm_proj_multiimp_trans,
         "c66_46_avrdibp ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean", type = 3)
```

R2=0.18. All categorical variables are stat. significant except phys, occup, diab, hpcl, day_mean. An interesting finding is that the season is stat. significant, however the day_mean is not. At the same time season was not selected in any of the optimized model, but day_mean was.

###### Pooled Radj and BIC for linear model:

```{r}
pool.r.squared(lm_models, adjusted = T)

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(lm_models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(lm_models$analyses[[i]])$BIC,
         Radj = glance(lm_models$analyses[[i]])$adj.r.squared)
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = fisher.trans(Radj)) %>%
  summarise(pooled_Radj = fisher.backtrans(mean(Radj_trans)),  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

Pooled Radj is 0.25, pooled BIC is 45957.

#### 2.2.2 Apparent standard deviation of average temperature

##### 2.2.2.1 Unadjusted

Pool estimates over all datasets with minimal set of variables:

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp_trans, 
               lm(c66_46_avrdibp ~ y1_app_temp_sd_46))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)
```

Pooled b1 for app.temp is -0.88 (p\<0.001) \[-1.14 : -0.63\]. The effect size decreased by 0.17. c66_46_urban_rural, c66_46_edu, c66_46_q1_ht, sex in combination drop the effect size from 1.05 to -0.61. c66_46_alc and increase the effect size to -1.42. Standard error is 0.13 and did not change.

###### Pooled Radj and BIC for linear model:

```{r}
pool.r.squared(lm_models, adjusted = T)

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(lm_models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(lm_models$analyses[[i]])$BIC,
         Radj = glance(lm_models$analyses[[i]])$adj.r.squared)
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = fisher.trans(Radj)) %>%
  summarise(pooled_Radj = fisher.backtrans(mean(Radj_trans)),  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

Pooled Radj is 0.25, pooled BIC is 45957.

##### 2.2.2.2 Minimally adjusted

Pool estimates over all datasets with minimal set of variables:

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp_trans, 
               lm(c66_46_avrdibp ~ y1_app_temp_sd_46 + c66_46_edu +
                c66_46_occup + sex + 
                c66_46_urban_rural + c66_46_season))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)
```

Pooled b1 for app.temp is -0.88 (p\<0.001) \[-1.14 : -0.63\]. The effect size decreased by 0.17. c66_46_urban_rural, c66_46_edu, c66_46_q1_ht, sex in combination drop the effect size from 1.05 to -0.61. c66_46_alc and increase the effect size to -1.42. Standard error is 0.13 and did not change.

**Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
mi.anova(pmm_proj_multiimp_trans,
         "c66_46_avrdibp ~ y1_app_temp_sd_46 + c66_46_edu +
                c66_46_occup + sex + 
                c66_46_ht_med", type = 3)
```

R2=0.18. All categorical variables are stat. significant except phys, occup, diab, hpcl, day_mean. An interesting finding is that the season is stat. significant, however the day_mean is not. At the same time season was not selected in any of the optimized model, but day_mean was.

###### Pooled Radj and BIC for linear model:

```{r}
pool.r.squared(lm_models, adjusted = T)

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(lm_models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(lm_models$analyses[[i]])$BIC,
         Radj = glance(lm_models$analyses[[i]])$adj.r.squared)
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = fisher.trans(Radj)) %>%
  summarise(pooled_Radj = fisher.backtrans(mean(Radj_trans)),  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

Pooled Radj is 0.25, pooled BIC is 45957.

##### 2.2.2.3 Optimized

Pool estimates over all datasets with minimal set of variables:

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp_trans, 
               lm(c66_46_avrdibp ~ y1_app_temp_sd_46  +  
                       c66_46_alc + c66_46_c_bmi_cat +
                       c66_46_ht_med + 
                       c66_46_q1_ht +
                       c66_46_urban_rural +
                       sex))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)
```

Pooled b1 for app.temp is -0.88 (p\<0.001) \[-1.14 : -0.63\]. The effect size decreased by 0.17. c66_46_urban_rural, c66_46_edu, c66_46_q1_ht, sex in combination drop the effect size from 1.05 to -0.61. c66_46_alc and increase the effect size to -1.42. Standard error is 0.13 and did not change.

**Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
mi.anova(pmm_proj_multiimp_trans,
         "c66_46_avrdibp ~ y1_app_temp_sd_46  +  
                       c66_46_alc + c66_46_c_bmi_cat +
                       c66_46_ht_med + c66_46_edu +
                       c66_46_q1_ht +
                       c66_46_urban_rural +
                       day_mean + sex", type = 3)
```

R2=0.18. All categorical variables are stat. significant except phys, occup, diab, hpcl, day_mean. An interesting finding is that the season is stat. significant, however the day_mean is not. At the same time season was not selected in any of the optimized model, but day_mean was.

###### Pooled Radj and BIC for linear model:

```{r}
pool.r.squared(lm_models, adjusted = T)

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(lm_models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(lm_models$analyses[[i]])$BIC,
         Radj = glance(lm_models$analyses[[i]])$adj.r.squared)
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = fisher.trans(Radj)) %>%
  summarise(pooled_Radj = fisher.backtrans(mean(Radj_trans)),  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

Pooled Radj is 0.25, pooled BIC is 45957.

##### 2.2.2.4 Fully adjusted(-)

Pool estimates over all datasets with minimal set of variables:

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp_trans, 
               lm(c66_46_avrdibp ~ y1_app_temp_sd_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)
```

Pooled b1 for app.temp is -0.88 (p\<0.001) \[-1.14 : -0.63\]. The effect size decreased by 0.17. c66_46_urban_rural, c66_46_edu, c66_46_q1_ht, sex in combination drop the effect size from 1.05 to -0.61. c66_46_alc and increase the effect size to -1.42. Standard error is 0.13 and did not change.

**Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
mi.anova(pmm_proj_multiimp_trans,
         "c66_46_avrdibp ~ y1_app_temp_sd_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean", type = 3)
```

R2=0.18. All categorical variables are stat. significant except phys, occup, diab, hpcl, day_mean. An interesting finding is that the season is stat. significant, however the day_mean is not. At the same time season was not selected in any of the optimized model, but day_mean was.

###### Pooled Radj and BIC for linear model:

```{r}
pool.r.squared(lm_models, adjusted = T)

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(lm_models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(lm_models$analyses[[i]])$BIC,
         Radj = glance(lm_models$analyses[[i]])$adj.r.squared)
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = fisher.trans(Radj)) %>%
  summarise(pooled_Radj = fisher.backtrans(mean(Radj_trans)),  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

Pooled Radj is 0.25, pooled BIC is 45957.

#### 2.2.3 Average temperature

##### 2.2.3.1 Unadjusted

Pool estimates over all datasets with minimal set of variables:

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp_trans, 
               lm(c66_46_avrdibp ~ y1_avg_temp))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)
```

Pooled b1 for app.temp is -0.88 (p\<0.001) \[-1.14 : -0.63\]. The effect size decreased by 0.17. c66_46_urban_rural, c66_46_edu, c66_46_q1_ht, sex in combination drop the effect size from 1.05 to -0.61. c66_46_alc and increase the effect size to -1.42. Standard error is 0.13 and did not change.

###### Pooled Radj and BIC for linear model:

```{r}
pool.r.squared(lm_models, adjusted = T)

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(lm_models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(lm_models$analyses[[i]])$BIC,
         Radj = glance(lm_models$analyses[[i]])$adj.r.squared)
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = fisher.trans(Radj)) %>%
  summarise(pooled_Radj = fisher.backtrans(mean(Radj_trans)),  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

Pooled Radj is 0.25, pooled BIC is 45957.

##### 2.2.3.2 Minimally adjusted

Pool estimates over all datasets with minimal set of variables:

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp_trans, 
               lm(c66_46_avrdibp ~ y1_avg_temp + c66_46_edu +
                c66_46_occup + sex + 
                c66_46_ht_med))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)
```

Pooled b1 for app.temp is -0.88 (p\<0.001) \[-1.14 : -0.63\]. The effect size decreased by 0.17. c66_46_urban_rural, c66_46_edu, c66_46_q1_ht, sex in combination drop the effect size from 1.05 to -0.61. c66_46_alc and increase the effect size to -1.42. Standard error is 0.13 and did not change.

**Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
mi.anova(pmm_proj_multiimp_trans,
         "c66_46_avrdibp ~ y1_avg_temp + c66_46_edu +
                c66_46_occup + sex + 
                c66_46_ht_med", type = 3)
```

R2=0.18. All categorical variables are stat. significant except phys, occup, diab, hpcl, day_mean. An interesting finding is that the season is stat. significant, however the day_mean is not. At the same time season was not selected in any of the optimized model, but day_mean was.

###### Pooled Radj and BIC for linear model:

```{r}
pool.r.squared(lm_models, adjusted = T)

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(lm_models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(lm_models$analyses[[i]])$BIC,
         Radj = glance(lm_models$analyses[[i]])$adj.r.squared)
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = fisher.trans(Radj)) %>%
  summarise(pooled_Radj = fisher.backtrans(mean(Radj_trans)),  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

Pooled Radj is 0.25, pooled BIC is 45957.

##### 2.2.3.3 Optimized

Pool estimates over all datasets with minimal set of variables:

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp_trans, 
               lm(c66_46_avrdibp ~ y1_avg_temp + c66_46_phys + 
                       c66_46_alc +  +
                       c66_46_c_bmi_cat +
                       c66_46_ht_med + c66_46_q1_ht +
                       sex))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)
```

Pooled b1 for app.temp is -0.88 (p\<0.001) \[-1.14 : -0.63\]. The effect size decreased by 0.17. c66_46_urban_rural, c66_46_edu, c66_46_q1_ht, sex in combination drop the effect size from 1.05 to -0.61. c66_46_alc and increase the effect size to -1.42. Standard error is 0.13 and did not change.

**Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
mi.anova(pmm_proj_multiimp_trans,
         "c66_46_avrdibp ~ y1_avg_temp + c66_46_phys + 
                       c66_46_alc +  +
                       c66_46_c_bmi_cat +
                       c66_46_ht_med + c66_46_q1_ht +
                       sex", type = 3)
```

R2=0.18. All categorical variables are stat. significant except phys, occup, diab, hpcl, day_mean. An interesting finding is that the season is stat. significant, however the day_mean is not. At the same time season was not selected in any of the optimized model, but day_mean was.

###### Pooled Radj and BIC for linear model:

```{r}
pool.r.squared(lm_models, adjusted = T)

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(lm_models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(lm_models$analyses[[i]])$BIC,
         Radj = glance(lm_models$analyses[[i]])$adj.r.squared)
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = fisher.trans(Radj)) %>%
  summarise(pooled_Radj = fisher.backtrans(mean(Radj_trans)),  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

Pooled Radj is 0.25, pooled BIC is 45957.

##### 2.2.3.4 Fully adjusted

Pool estimates over all datasets with minimal set of variables:

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp_trans, 
               lm(c66_46_avrdibp ~ y1_avg_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)
```

Pooled b1 for app.temp is -0.88 (p\<0.001) \[-1.14 : -0.63\]. The effect size decreased by 0.17. c66_46_urban_rural, c66_46_edu, c66_46_q1_ht, sex in combination drop the effect size from 1.05 to -0.61. c66_46_alc and increase the effect size to -1.42. Standard error is 0.13 and did not change.

**Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
mi.anova(pmm_proj_multiimp_trans,
         "c66_46_avrdibp ~ y1_avg_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean", type = 3)
```

R2=0.18. All categorical variables are stat. significant except phys, occup, diab, hpcl, day_mean. An interesting finding is that the season is stat. significant, however the day_mean is not. At the same time season was not selected in any of the optimized model, but day_mean was.

###### Pooled Radj and BIC for linear model:

```{r}
pool.r.squared(lm_models, adjusted = T)

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(lm_models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(lm_models$analyses[[i]])$BIC,
         Radj = glance(lm_models$analyses[[i]])$adj.r.squared)
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = fisher.trans(Radj)) %>%
  summarise(pooled_Radj = fisher.backtrans(mean(Radj_trans)),  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

Pooled Radj is 0.25, pooled BIC is 45957.

#### 2.2.4 Standard deviation of average temperature

##### 2.2.4.1 Unadjusted

Pool estimates over all datasets with minimal set of variables:

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp_trans, 
               lm(c66_46_avrdibp ~ y1_sd_temp))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)
```

Pooled b1 for app.temp is -0.88 (p\<0.001) \[-1.14 : -0.63\]. The effect size decreased by 0.17. c66_46_urban_rural, c66_46_edu, c66_46_q1_ht, sex in combination drop the effect size from 1.05 to -0.61. c66_46_alc and increase the effect size to -1.42. Standard error is 0.13 and did not change.

###### Pooled Radj and BIC for linear model:

```{r}
pool.r.squared(lm_models, adjusted = T)

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(lm_models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(lm_models$analyses[[i]])$BIC,
         Radj = glance(lm_models$analyses[[i]])$adj.r.squared)
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = fisher.trans(Radj)) %>%
  summarise(pooled_Radj = fisher.backtrans(mean(Radj_trans)),  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

Pooled Radj is 0.25, pooled BIC is 45957.

##### 2.2.4.2 Minimally adjusted

Pool estimates over all datasets with minimal set of variables:

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp_trans, 
               lm(c66_46_avrdibp ~ y1_sd_temp + c66_46_edu +
                c66_46_occup + sex + 
                c66_46_ht_med))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)
```

Pooled b1 for app.temp is -0.88 (p\<0.001) \[-1.14 : -0.63\]. The effect size decreased by 0.17. c66_46_urban_rural, c66_46_edu, c66_46_q1_ht, sex in combination drop the effect size from 1.05 to -0.61. c66_46_alc and increase the effect size to -1.42. Standard error is 0.13 and did not change.

**Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
mi.anova(pmm_proj_multiimp_trans,
         "c66_46_avrdibp ~ y1_sd_temp + c66_46_edu +
                c66_46_occup + sex + 
                c66_46_ht_med", type = 3)
```

R2=0.18. All categorical variables are stat. significant except phys, occup, diab, hpcl, day_mean. An interesting finding is that the season is stat. significant, however the day_mean is not. At the same time season was not selected in any of the optimized model, but day_mean was.

###### Pooled Radj and BIC for linear model:

```{r}
pool.r.squared(lm_models, adjusted = T)

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(lm_models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(lm_models$analyses[[i]])$BIC,
         Radj = glance(lm_models$analyses[[i]])$adj.r.squared)
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = fisher.trans(Radj)) %>%
  summarise(pooled_Radj = fisher.backtrans(mean(Radj_trans)),  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

Pooled Radj is 0.25, pooled BIC is 45957.

##### 2.2.4.3 Optimized

Pool estimates over all datasets with minimal set of variables:

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp_trans, 
               lm(c66_46_avrdibp ~ y1_sd_temp +  
                       c66_46_alc + c66_46_c_bmi_cat +
                       c66_46_edu + c66_46_ht_med +
                       c66_46_q1_ht +
                       c66_46_urban_rural + day_mean +
                       sex))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)
```

Pooled b1 for app.temp is -0.88 (p\<0.001) \[-1.14 : -0.63\]. The effect size decreased by 0.17. c66_46_urban_rural, c66_46_edu, c66_46_q1_ht, sex in combination drop the effect size from 1.05 to -0.61. c66_46_alc and increase the effect size to -1.42. Standard error is 0.13 and did not change.

**Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
mi.anova(pmm_proj_multiimp_trans,
         "c66_46_avrdibp ~ y1_sd_temp +  
                       c66_46_alc + c66_46_c_bmi_cat +
                       c66_46_edu + c66_46_ht_med +
                       c66_46_q1_ht +
                       c66_46_urban_rural + day_mean +
                       sex", type = 3)
```

R2=0.18. All categorical variables are stat. significant except phys, occup, diab, hpcl, day_mean. An interesting finding is that the season is stat. significant, however the day_mean is not. At the same time season was not selected in any of the optimized model, but day_mean was.

###### Pooled Radj and BIC for linear model:

```{r}
pool.r.squared(lm_models, adjusted = T)

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(lm_models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(lm_models$analyses[[i]])$BIC,
         Radj = glance(lm_models$analyses[[i]])$adj.r.squared)
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = fisher.trans(Radj)) %>%
  summarise(pooled_Radj = fisher.backtrans(mean(Radj_trans)),  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

Pooled Radj is 0.25, pooled BIC is 45957.

##### 2.2.4.4 Fully adjusted(-)

Pool estimates over all datasets with minimal set of variables:

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp_trans, 
               lm(c66_46_avrdibp ~ y1_sd_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)
```

Pooled b1 for app.temp is -0.88 (p\<0.001) \[-1.14 : -0.63\]. The effect size decreased by 0.17. c66_46_urban_rural, c66_46_edu, c66_46_q1_ht, sex in combination drop the effect size from 1.05 to -0.61. c66_46_alc and increase the effect size to -1.42. Standard error is 0.13 and did not change.

**Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
mi.anova(pmm_proj_multiimp_trans,
         "c66_46_avrdibp ~ y1_sd_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean", type = 3)
```

R2=0.18. All categorical variables are stat. significant except phys, occup, diab, hpcl, day_mean. An interesting finding is that the season is stat. significant, however the day_mean is not. At the same time season was not selected in any of the optimized model, but day_mean was.

###### Pooled Radj and BIC for linear model:

```{r}
pool.r.squared(lm_models, adjusted = T)

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(lm_models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(lm_models$analyses[[i]])$BIC,
         Radj = glance(lm_models$analyses[[i]])$adj.r.squared)
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = fisher.trans(Radj)) %>%
  summarise(pooled_Radj = fisher.backtrans(mean(Radj_trans)),  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

Pooled Radj is 0.25, pooled BIC is 45957.

# Effect modification (interaction)

# Obese vs non-obese

## Systolic blood pressure

### Apparent average temperature

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrsybp ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean +
                    y1_app_temp_avg_46:c66_46_urban_rural + y1_app_temp_avg_46:c66_46_diab +
                    y1_app_temp_avg_46:c66_46_c_bmi_cat +
                    y1_app_temp_avg_46:c66_46_q1_ht + y1_app_temp_avg_46:sex))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)

#c66_46_q1_ht, c66_46_edu, c66_46_alc, , sex, c66_46_urban_rural 
```

-   **Intercept:** The estimated mean systolic blood pressure among non-obese with app_avg_temp = 0 is 63.18.

-   **Coefficient for app_avg_temp (“app_avg_temp main effect”):** *Among non-obese* (the value at which the other term in the interaction is 0 or at its reference level), a 1 unit difference in app_avg_temp is associated with a -0.90 unit difference (decrease) in systolic blood pressure

-   **Coefficient for Bmi_cat (“Bmi_cat main effect”):** *Among those with* app_avg_temp *= 0 degrees* (the value at which the other term in the interaction is 0 or at its reference level), obese people have an average systolic blood pressure that is 6.44 units greater (since the coefficient is positive) than non-obese.

-   **Coefficient for app_avg_temp× Bmi_cat(“interaction effect”):** This model allows the effect of each predictor in the interaction to differ depending on the level of the other. The interaction effect describes the rate at which that occurs. The interaction has two interpretations. First, the app_avg_tempeffect on syst.bp. is 0.19 different (smaller) for obese than for non-obese. Second, the bmi_cat difference in mean syst.bp changes by 0.19 bp units for each 1 degree difference in app_avg_temp.

#### Visualizing an interaction

The syntax `terms = c("y1_app_temp_avg_46", "c66_46_c_bmi_cat")` requests a plot in which the first variable (`y1_app_temp_avg_46`) is the x-axis variable and which displays the effect of the first variable on the outcome at each level of the second (`c66_46_c_bmi_cat`). Using `type = "pred"` displays the predicted values assuming that all other variables in the model are at a fixed value while `type = "eff"` displays predicted values that are averaged over predictions at different levels of the other predictors. When any numeric predictors are not centered, `type = "pred"` could lead to unrealistic predictions, so we will use `type = "eff"`.

```{r}
# Apply the linear function to each imputed dataset
lm_model_no_inter <- lm(c66_46_avrsybp ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = bp_df_plot)

lm_model_inter <- lm(c66_46_avrsybp ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean + y1_app_temp_avg_46:c66_46_urban_rural + y1_app_temp_avg_46:c66_46_diab +
                    y1_app_temp_avg_46:c66_46_c_bmi_cat +
                    y1_app_temp_avg_46:c66_46_q1_ht + y1_app_temp_avg_46:sex, data = bp_df_plot)

# Effect of BMXWAIST at each level of RIAGENDR
# in the model with no interaction
plot_model(lm_model_no_inter,
           type = "eff",
           terms = c("y1_app_temp_avg_46", "c66_46_c_bmi_cat"),
           title = "No Interaction",
           axis.title = c("Apparent average temperature (°C)", 
                          "Systolic blood pressure (mmHg)"))

# Effect of BMXWAIST at each level of RIAGENDR
# in the model with an interaction
plot_model(lm_model_inter,
           type = "eff",
           terms = c("y1_app_temp_avg_46", "c66_46_c_bmi_cat"),
           title = "Interaction",
           axis.title = c("Apparent average temperature (°C)", 
                          "Systolic blood pressure (mmHg)"))
```

**Model with no interaction:** In the *left panel* , the app*\_*avg_temp effects (slopes) for obese and non-obese are *assumed* to be the same, with a common estimate of -0.86, which comes from the model with no interaction . Also, the obese line is slightly above the non-obese line by an amount equal to the bmi_cat effect (6.75). The assumption of no interaction is equivalent to assuming that the lines are *parallel*.

**Model with an interaction:** In the *right panel*, including an interaction term has the effect of fitting separate regression lines for nonobese and obese. The app*\_*avg_temp slope for nonobese (-0.90) is much steeper than for obese (-0.71), indicating that app*\_*avg_temp has a greater negative effect on syst.bp for nonobese than for obese.

#### Testing the difference between the slopes

In the right panel, is the difference between the nonobese and obee slopes statistically significant? This is equivalent to asking, “Are the lines in the plot significantly different from parallel?” The p-value for the regression coefficient for the interaction tests the null hypothesis H0:β3=0. Under the null hypothesis, the lines are parallel. Thus, to test the difference between the slopes, examine the interaction p-value.

From the output, the p-value for the test of interaction is 0.51. In this case, since the interaction was between a continuous predictor and a binary predictor, the interaction is just one term, so the test can be obtained directly from the regression coefficient table. When the interaction involves a categorical predictor with more than two levels, we should perform `car::Anova(type=3)`.

Since p \> .05, we can make the following two inferences:

-   There is a non-significant difference in the app*\_*avg_temp effect on syst.bp between nonobese and obese (the lines are not significantly different from parallel).

-   There is a non significant difference in the bmi_cat effect on syst.bp between those with differing app*\_*avg_temp (the vertical distance between the lines does not depend on how large or small app*\_*avg_temp is).

-   Effect of app*\_*avg_temp on syst.bp does not depend on bmi_cat.

#### Estimating and testing the significance of the slope at each level of a moderator

We just showed that the app*\_*avg_temp effect is significantly greater for nonobese than for obese (in absolute terms). What if you want to estimate and test the significance of the app*\_*avg_temp effect for each bmi_cat individually (testing whether the line for nonobese is significantly different from a horizontal line and, separately, testing whether the line for obese is significantly different from a horizontal line)?

The null hypothesis for the test of the app*\_*avg_temp effect among nonobese is a test of the slope for nonobese H0:b1=0. Since this involves just one regression coefficient, you can read this right off the output in the `app_avg_temp` row of the `COEFFICIENTS` table since that row corresponds to b1. Thus, the app*\_*avg_temp effect for nonobese is statistically significant (B = -0.90; 95% CI =-1.18 -0.62; p \<.001).

However, the slope for females is the sum of two regression coefficients (b1+b3). How do we conduct the test for obese? You *could* re-fit the model after setting “Obese” to be the reference level instead of “Nonobese”. In this new model, the app*\_*avg_temp main effect would be the slope for females. Another option is the function `gmodels::estimable()` can be used to estimate and test any linear combination of regression coefficients.

```{r}
# Test slope for obese
lm_models <- with(pmm_proj_multiimp, 
  lm(c66_46_avrsybp ~ y1_app_temp_avg_46 +
       c66_46_q1_ht + 
       relevel(c66_46_c_bmi_cat, ref = "Obese") + 
       c66_46_ht_med + c66_46_edu +  
       c66_46_smok + c66_46_alc +
       c66_46_phys + c66_46_occup + 
        + sex + 
       c66_46_diab + c66_46_hpcl + 
       c66_46_season + c66_46_urban_rural + day_mean +
       y1_app_temp_avg_46:relevel(c66_46_c_bmi_cat, ref = "Obese")))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)
```

##### **Conclusion** 

The app*\_*avg_temp effects for nonobese (Est = -0.90; 95% CI = -1.18 -0.62; p \<.01) and obese(Est = -0.71; 95% CI = -1.23 -0.19; p = .01) are statistically significant.

The magnitude of the association between app*\_*avg_temp and syst.bp differs non-significantly between nonobese and obese (p = .51). The association between app*\_*avg_temp and syst.bp is significant for both nonobese(Slope = -0.90; 95% CI = -1.18 -0.62; p \<.01) and obese (Slope = -0.71; 95% CI = -1.23 -0.19; p = .01), but is non-significantly lower in absolute terms for obese (b-interaction (difference in slopes) = 0.19; 95% CI = -0.37, 0.75; p = .51).

#### Overall test of a predictor involved in an interaction

We assessed the significance of an interaction with a test of the null hypothesis that one predictor effect does not vary with another predictor. A test of interaction answers a question such as, “Does the effect of app*\_*avg_temp on syst.bp depend on bmi_cat?”

But what if we want to answer the question, “Is app*\_*avg_temp significantly associated with syst.bp?” in a model which contains app*\_*avg_temp interacted with another predictor? This is a comparison of two models – one that includes app*\_*avg_temp and one that does not. When there is no interaction, this would simply be a test of the app*\_*avg_temp regression coefficient. When there is an interaction, however, this is a simultaneous test of the app*\_*avg_temp main effect and the app*\_*avg_temp interaction with the other predictor.

To carry out this test, use `anova()` (lowercase a), which compares two nested models: a full model compared to a reduced model with some terms from the full model removed. In this example, we want to compare the full model (with both main effects and the interaction) to the model that does not have app*\_*avg_temp at all (excludes both the app*\_*avg_temp main effect and the interaction, but retains the gender main effect). 

```{r}
# Get a list of all imputed datasets
imputed_datasets <- mice::complete(pmm_proj_multiimp, "all")

# Replace Y, predictors1, and predictors2 with your formulas
# Example:
compare_models <- function(data) {
  fit0 <- lm(c66_46_avrsybp ~ c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = data)
  fit1 <- lm(c66_46_avrsybp ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean + y1_app_temp_avg_46:c66_46_c_bmi_cat, data = data)
  anova(fit0, fit1)
}

# Apply comparison function to each imputed dataset
anova_results <- lapply(imputed_datasets, compare_models)

p_values = c()

# Inspect results
for (i in seq(1,100,1)) {
  pvalue = anova_results[[i]]$`Pr(>F)`[2]
  p_values = c(p_values, pvalue)
  
}

mean(p_values)  # Average p-value across imputations
sum(p_values < 0.05, na.rm = TRUE)

```

100/100 anova tests have p-value less than 0.05 =\> Thus, we conclude that the model with both app_avg_temp and app_avg_temp \*bmi_cat interaction is significantly better than the model with just bmi_cat(p \<.05), or that app_avg_temp is significantly associated with syst.bp. in a model which contains interaction.

### Apparent standard deviation of average temperature (-)

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrsybp ~ y1_app_temp_sd_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean +
                    y1_app_temp_sd_46:c66_46_c_bmi_cat))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)

#c66_46_q1_ht, c66_46_edu, c66_46_alc, , sex, c66_46_urban_rural 
```

-   **Intercept:** The estimated mean systolic blood pressure among non-obese with app_sd_temp = 0 is 121,03.

-   **Coefficient for app_avg_temp (“app_avg_temp main effect”):** *Among non-obese* (the value at which the other term in the interaction is 0 or at its reference level), a 1 standard deviation difference (increase) in app_sd_temp is associated with a 0.25 unit difference (increase) in systolic blood pressure

-   **Coefficient for Bmi_cat (“Bmi_cat main effect”):** *Among those with* app_sd_temp *= 0 degrees* (the value at which the other term in the interaction is 0 or at its reference level), obese people have an average systolic blood pressure that is 11.85 units greater (since the coefficient is positive) than non-obese.

-   **Coefficient for app_sd_temp× Bmi_cat(“interaction effect”):** This model allows the effect of each predictor in the interaction to differ depending on the level of the other. The interaction effect describes the rate at which that occurs. The interaction has two interpretations. First, the app_avg_temp effect on syst.bp. is -0.48 different (smaller) for obese than for non-obese. Second, the bmi_cat difference in mean syst.bp changes by -0.48 bp units for each 1 degree difference in app_sd_temp. **The impact of BMI category on systolic blood pressure depends on temperature** — as temperature increases, the difference in SBP between obese and non-obese people becomes smaller (by 0.48 mmHg per °C).

#### Visualizing an interaction

The syntax `terms = c("y1_app_temp_sd_46", "c66_46_c_bmi_cat")` requests a plot in which the first variable (`y1_app_temp_sd_46`) is the x-axis variable and which displays the effect of the first variable on the outcome at each level of the second (`c66_46_c_bmi_cat`). Using `type = "pred"` displays the predicted values assuming that all other variables in the model are at a fixed value while `type = "eff"` displays predicted values that are averaged over predictions at different levels of the other predictors. When any numeric predictors are not centered, `type = "pred"` could lead to unrealistic predictions, so we will use `type = "eff"`.

```{r}
library(sjPlot)
library(ggplot2)

# Apply the linear function to each imputed dataset
lm_model_no_inter <- lm(c66_46_avrsybp ~ y1_app_temp_sd_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = bp_df_plot)

lm_model_inter <- lm(c66_46_avrsybp ~ y1_app_temp_sd_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean +
                    y1_app_temp_sd_46:c66_46_urban_rural + y1_app_temp_sd_46:c66_46_diab +
                    y1_app_temp_sd_46:c66_46_c_bmi_cat +
                    y1_app_temp_sd_46:c66_46_q1_ht + y1_app_temp_sd_46:sex, data = bp_df_plot)


plot_model(
  lm_model_no_inter,
  type = "eff",
  terms = c("y1_app_temp_sd_46", "c66_46_c_bmi_cat"),
  title = "No Interaction term",
  axis.title = c("Standard deviation of apparent temperature", 
                 "Systolic blood pressure (mmHg)"),
  pred.labels = c("Mean Apparent Temp (°C)", "Urbanicity"),
  legend.title = "Urbanicity",
  legend.labels = c("Rural", "Urban")
) +
  # Add a minimal theme
  theme_minimal() +
  # Customize some text sizing and alignment
  theme(
    plot.title = element_text(hjust = 0.5, size = 14),
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 11),
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 11)
  )

plot_model(
  lm_model_inter,
  type = "eff",
  terms = c("y1_app_temp_sd_46", "c66_46_c_bmi_cat"),
  title = "ATV and SBP under interaction with obesity",
  axis.title = c("Standard deviation of the apparent temperature", 
                 "Systolic blood pressure (mmHg)"),
  pred.labels = c("Mean Apparent Temp (°C)", "Urbanicity"),
  legend.title = "Obesity",
  legend.labels = c("Nonobese", "Obese")
) +
  # Add a minimal theme
  theme_minimal() +
  # Customize some text sizing and alignment
  theme(
    plot.title = element_text(hjust = 0.5, size = 14),
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 12),
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 11)
  )
```



**Model with no interaction:** In the *left panel* , the app*\_*sd_temp effects (slopes) for obese and non-obese are *assumed* to be the same, with a common estimate of 0.14, which comes from the model with no interaction . Also, the obese line is slightly above the non-obese line by an amount equal to the bmi_cat effect (6.68). The assumption of no interaction is equivalent to assuming that the lines are *parallel*.

**Model with an interaction:** In the *right panel*, including an interaction term has the effect of fitting separate regression lines for nonobese and obese. The app*\_*avg_temp slope for nonobese (0.25) is not steeper than for obese (-0.23), indicating that app*\_*sd_temp has not a greater effect on syst.bp for nonobese than for obese.

#### Testing the difference between the slopes

In the right panel, is the difference between the nonobese and obese slopes statistically significant? This is equivalent to asking, “Are the lines in the plot significantly different from parallel?” The p-value for the regression coefficient for the interaction tests the null hypothesis H0:βinter=0. Under the null hypothesis, the lines are parallel. Thus, to test the difference between the slopes, examine the interaction p-value.

From the output, the p-value for the test of interaction is **0.38**. In this case, since the interaction was between a continuous predictor and a binary predictor, the interaction is just one term, so the test can be obtained directly from the regression coefficient table. When the interaction involves a categorical predictor with more than two levels, we should perform `car::Anova(type=3)`.

Since p \> .05, we can make the following two inferences:

-   There is a non-significant difference in the app*\_*sd_temp effect on syst.bp between nonobese and obese (the lines are not significantly different from parallel).

-   There is a non significant difference in the bmi_cat effect on syst.bp between those with differing app*\_*sd_temp (the vertical distance between the lines does not depend on how large or small app*\_*sd_temp is).

-   Effect of app*\_*sd_temp on syst.bp does not depend on bmi_cat.

#### Estimating and testing the significance of the slope at each level of a moderator

We just showed that the app*\_*sd_temp effect is NOT greater for nonobese than for obese (in absolute terms). What if you want to estimate and test the significance of the app*\_*sd_temp effect for each bmi_cat individually (testing whether the line for nonobese is significantly different from a horizontal line and, separately, testing whether the line for obese is significantly different from a horizontal line)?

The null hypothesis for the test of the app*\_*sd_temp effect among nonobese is a test of the slope for nonobese H0:b1=0. Since this involves just one regression coefficient, you can read this right off the output in the `app_sd_temp` row of the `COEFFICIENTS` table since that row corresponds to b1. Thus, the app*\_*sd_temp effect for nonobese is NOT statistically significant (B = 0.25; 95% CI =-0.26, 0.77; p=0.34)

However, the slope for obese is the sum of two regression coefficients (b1+b3). How do we conduct the test for obese? You *could* re-fit the model after setting “Obese” to be the reference level instead of “Nonobese”. In this new model, the app*\_*avg_temp main effect would be the slope for females. Another option is the function `gmodels::estimable()` can be used to estimate and test any linear combination of regression coefficients.

```{r}
# Test slope for obese
lm_models <- with(pmm_proj_multiimp, 
  lm(c66_46_avrsybp ~ y1_app_temp_sd_46 +
       c66_46_q1_ht + 
       relevel(c66_46_c_bmi_cat, ref = "Obese") + 
       c66_46_ht_med + c66_46_edu +  
       c66_46_smok + c66_46_alc +
       c66_46_phys + c66_46_occup + 
       sex + 
       c66_46_diab + c66_46_hpcl + 
       c66_46_season + c66_46_urban_rural + day_mean +
       y1_app_temp_sd_46:relevel(c66_46_c_bmi_cat, ref = "Obese") +
       y1_app_temp_sd_46:c66_46_urban_rural +
       y1_app_temp_sd_46:c66_46_diab +
       y1_app_temp_sd_46:c66_46_c_bmi_cat +
                    y1_app_temp_sd_46:c66_46_q1_ht + y1_app_temp_sd_46:sex))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2) 	
```

##### **Conclusion** 

The app*\_*sd_temp effects for nonobese (Est = 0.25; 95% CI = -0.26, 0.77; p =0.34) and obese(Est = -0.23; 95% CI = -1.19, 0.73; p = .64) are NOT statistically significant.

The magnitude of the association between app*\_*sd_temp and syst.bp differs non-significantly between nonobese and obese (p = .38). The association between app*\_*sd_temp and syst.bp is NOT significant for both nonobese (Est = 0.25; 95% CI = -0.26, 0.77; p =0.34) and obese (Est = -0.23; 95% CI = -1.19, 0.73; p = .64), but is non-significantly similar in absolute terms for obese (b-interaction (difference in slopes) = -0.48; 95% CI = -1.55 0.59; p = .51).

#### Overall test of a predictor involved in an interaction

We assessed the significance of an interaction with a test of the null hypothesis that one predictor effect does not vary with another predictor. A test of interaction answers a question such as, “Does the effect of app*\_*avg_temp on syst.bp depend on bmi_cat?”

But what if we want to answer the question, “Is app*\_*sd_temp significantly associated with syst.bp?” in a model which contains app*\_*avg_temp interacted with another predictor? This is a comparison of two models – one that includes app*\_*avg_temp and one that does not. When there is no interaction, this would simply be a test of the app*\_*avg_temp regression coefficient. When there is an interaction, however, this is a simultaneous test of the app*\_*avg_temp main effect and the app*\_*avg_temp interaction with the other predictor.

To carry out this test, use `anova()` (lowercase a), which compares two nested models: a full model compared to a reduced model with some terms from the full model removed. In this example, we want to compare the full model (with both main effects and the interaction) to the model that does not have app*\_*avg_temp at all (excludes both the app*\_*avg_temp main effect and the interaction, but retains the gender main effect). 

```{r}
# Get a list of all imputed datasets
imputed_datasets <- mice::complete(pmm_proj_multiimp, "all")

# Replace Y, predictors1, and predictors2 with your formulas
# Example:
compare_models <- function(data) {
  fit0 <- lm(c66_46_avrsybp ~ c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = data)
  fit1 <- lm(c66_46_avrsybp ~ y1_app_temp_sd_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean + y1_app_temp_sd_46:c66_46_c_bmi_cat, data = data)
  anova(fit0, fit1)
}

# Apply comparison function to each imputed dataset
anova_results <- lapply(imputed_datasets, compare_models)

p_values = c()

# Inspect results
for (i in seq(1,100,1)) {
  pvalue = anova_results[[i]]$`Pr(>F)`[2]
  p_values = c(p_values, pvalue)
  
}

mean(p_values)  # Average p-value across imputations
sum(p_values < 0.05, na.rm = TRUE)

```

0/100 anova tests have p-value less than 0.05 =\> Thus, we conclude that the model with both app_sd_temp and app_sd_temp \*bmi_cat interaction is NOT significantly better than the model with just bmi_cat(p \>.05), or that app_sd_temp is NOT significantly associated with syst.bp. in a model which contains interaction.

### Average temperature

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrsybp ~ y1_avg_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean +
                    y1_avg_temp:c66_46_c_bmi_cat))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)

#c66_46_q1_ht, c66_46_edu, c66_46_alc, , sex, c66_46_urban_rural 
```

-   **Intercept:** The estimated mean systolic blood pressure among non-obese with avg_temp = 0 is 64.26.

-   **Coefficient for avg_temp (“avg_temp main effect”):** *Among non-obese* (the value at which the other term in the interaction is 0 or at its reference level), a 1 unit difference in avg_temp is associated with a -0.93 unit difference (decrease) in systolic blood pressure

-   **Coefficient for Bmi_cat (“Bmi_cat main effect”):** *Among those with* avg_temp *= 0 degrees* (the value at which the other term in the interaction is 0 or at its reference level), obese people have an average systolic blood pressure that is 5.86 units greater (since the coefficient is positive) than non-obese.

-   **Coefficient for app_avg_temp× Bmi_cat(“interaction effect”):** This model allows the effect of each predictor in the interaction to differ depending on the level of the other. The interaction effect describes the rate at which that occurs. The interaction has two interpretations. First, the avg_temp effect on syst.bp. is 0.22 different (smaller i nabs. terms) for obese than for non-obese. Second, the bmi_cat difference in mean syst.bp changes by 0.22 bp units for each 1 degree difference in app_avg_temp. **The impact of BMI category on systolic blood pressure depends on temperature** — as temperature increases, the difference in SBP between obese and non-obese people becomes bigger (by 0.22 mmHg per °C).

#### Visualizing an interaction

The syntax `terms = c("y1_avg_temp", "c66_46_c_bmi_cat")` requests a plot in which the first variable (`y1_avg_temp`) is the x-axis variable and which displays the effect of the first variable on the outcome at each level of the second (`c66_46_c_bmi_cat`). Using `type = "pred"` displays the predicted values assuming that all other variables in the model are at a fixed value while `type = "eff"` displays predicted values that are averaged over predictions at different levels of the other predictors. When any numeric predictors are not centered, `type = "pred"` could lead to unrealistic predictions, so we will use `type = "eff"`.

```{r}
# Apply the linear function to each imputed dataset
lm_model_no_inter <- lm(c66_46_avrsybp ~ y1_avg_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = bp_df_plot)

lm_model_inter <- lm(c66_46_avrsybp ~ y1_avg_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean + y1_avg_temp:c66_46_c_bmi_cat, data = bp_df_plot)

# Effect of BMXWAIST at each level of RIAGENDR
# in the model with no interaction
plot_model(lm_model_no_inter,
           type = "eff",
           terms = c("y1_avg_temp", "c66_46_c_bmi_cat"),
           title = "No Interaction",
           axis.title = c("Apparent average temperature (°C)", 
                          "Systolic blood pressure (mmHg)"))

# Effect of BMXWAIST at each level of RIAGENDR
# in the model with an interaction
plot_model(lm_model_inter,
           type = "eff",
           terms = c("y1_avg_temp", "c66_46_c_bmi_cat"),
           title = "Interaction",
           axis.title = c("Apparent average temperature (°C)", 
                          "Systolic blood pressure (mmHg)"))
```

**Model with no interaction:** In the *left panel* , the app*\_*avg_temp effects (slopes) for obese and non-obese are *assumed* to be the same, with a common estimate of -0.88, which comes from the model with no interaction . Also, the obese line is slightly above the non-obese line by an amount equal to the bmi_cat effect (6.8). The assumption of no interaction is equivalent to assuming that the lines are *parallel*.

**Model with an interaction:** In the *right panel*, including an interaction term has the effect of fitting separate regression lines for nonobese and obese. The avg_temp slope for nonobese (-0.93) is much steeper than for obese (-0.71), indicating that avg_temp has a greater negative effect on syst.bp for nonobese than for obese.

#### Testing the difference between the slopes

In the right panel, is the difference between the nonobese and obeыe slopes statistically significant? This is equivalent to asking, “Are the lines in the plot significantly different from parallel?” The p-value for the regression coefficient for the interaction tests the null hypothesis H0:β3=0. Under the null hypothesis, the lines are parallel. Thus, to test the difference between the slopes, examine the interaction p-value.

From the output, the p-value for the test of interaction is **0.47**. In this case, since the interaction was between a continuous predictor and a binary predictor, the interaction is just one term, so the test can be obtained directly from the regression coefficient table. When the interaction involves a categorical predictor with more than two levels, we should perform `car::Anova(type=3)`.

Since p \> .05, we can make the following two inferences:

-   There is a non-significant difference in the avg_temp effect on syst.bp between nonobese and obese (the lines are not significantly different from parallel).

-   There is a non significant difference in the bmi_cat effect on syst.bp between those with differing avg_temp (the vertical distance between the lines does not depend on how large or small avg_temp is).

-   Effect of avg_temp on syst.bp does not depend on bmi_cat.

#### Estimating and testing the significance of the slope at each level of a moderator

We just showed that the avg_temp effect is significantly greater for nonobese than for obese (in absolute terms). What if you want to estimate and test the significance of the avg_temp effect for each bmi_cat individually (testing whether the line for nonobese is significantly different from a horizontal line and, separately, testing whether the line for obese is significantly different from a horizontal line)?

The null hypothesis for the test of the avg_temp effect among nonobese is a test of the slope for nonobese H0:b1=0. Since this involves just one regression coefficient, you can read this right off the output in the `avg_temp` row of the `COEFFICIENTS` table since that row corresponds to b1. Thus, the app*\_*avg_temp effect for nonobese is statistically significant (B = -0.93; 95% CI =-1.23 -0.62; p \<.001).

However, the slope for females is the sum of two regression coefficients (b1+b3). How do we conduct the test for obese? You *could* re-fit the model after setting “Obese” to be the reference level instead of “Nonobese”. In this new model, the app*\_*avg_temp main effect would be the slope for females. Another option is the function `gmodels::estimable()` can be used to estimate and test any linear combination of regression coefficients.

```{r}
# Test slope for obese
lm_models <- with(pmm_proj_multiimp, 
  lm(c66_46_avrsybp ~ y1_avg_temp +
       c66_46_q1_ht + 
       relevel(c66_46_c_bmi_cat, ref = "Obese") + 
       c66_46_ht_med + c66_46_edu +  
       c66_46_smok + c66_46_alc +
       c66_46_phys + c66_46_occup + 
        + sex + 
       c66_46_diab + c66_46_hpcl + 
       c66_46_season + c66_46_urban_rural + day_mean +
       y1_avg_temp:relevel(c66_46_c_bmi_cat, ref = "Obese")))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)
```

##### **Conclusion** 

The app*\_*avg_temp effects for nonobese (B = -0.93; 95% CI =-1.23 -0.62; p \<.001) and obese (Est = -0.71; 95% CI = -1.27 -0.15; p = .01) are statistically significant.

The magnitude of the association between avg_temp and syst.bp differs non-significantly between nonobese and obese (p = .51). The association between avg_temp and syst.bp is significant for both nonobese (B = -0.93; 95% CI =-1.23 -0.62; p \<.001) and obese (Est = -0.71; 95% CI = -1.27 -0.15; p = .01), but is non-significantly lower in absolute terms for obese (b-interaction (difference in slopes) = 0.22; 95% CI = -0.39, 0.83; p = .47).

#### Overall test of a predictor involved in an interaction

We assessed the significance of an interaction with a test of the null hypothesis that one predictor effect does not vary with another predictor. A test of interaction answers a question such as, “Does the effect of avg_temp on syst.bp depend on bmi_cat?”

But what if we want to answer the question, “Is avg_temp significantly associated with syst.bp?” in a model which contains avg_temp interacted with another predictor? This is a comparison of two models – one that includes avg_temp and one that does not. When there is no interaction, this would simply be a test of the avg_temp regression coefficient. When there is an interaction, however, this is a simultaneous test of the avg_temp main effect and the avg_temp interaction with the other predictor.

To carry out this test, use `anova()` (lowercase a), which compares two nested models: a full model compared to a reduced model with some terms from the full model removed. In this example, we want to compare the full model (with both main effects and the interaction) to the model that does not have avg_temp at all (excludes both the app*\_*avg_temp main effect and the interaction, but retains the gender main effect). 

```{r}
# Get a list of all imputed datasets
imputed_datasets <- mice::complete(pmm_proj_multiimp, "all")

# Replace Y, predictors1, and predictors2 with your formulas
# Example:
compare_models <- function(data) {
  fit0 <- lm(c66_46_avrsybp ~ c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = data)
  fit1 <- lm(c66_46_avrsybp ~ y1_avg_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean + y1_avg_temp:c66_46_c_bmi_cat, data = data)
  anova(fit0, fit1)
}

# Apply comparison function to each imputed dataset
anova_results <- lapply(imputed_datasets, compare_models)

p_values = c()

# Inspect results
for (i in seq(1,100,1)) {
  pvalue = anova_results[[i]]$`Pr(>F)`[2]
  p_values = c(p_values, pvalue)
  
}

mean(p_values)  # Average p-value across imputations
sum(p_values < 0.05, na.rm = TRUE)

```

100/100 anova tests have p-value less than 0.05 =\> Thus, we conclude that the model with both avg_temp and avg_temp \*bmi_cat interaction is significantly better than the model with just bmi_cat(p \<.05), or that avg_temp is significantly associated with syst.bp. in a model which contains interaction.

### Standard deviation of average temperature (-)

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrsybp ~ y1_sd_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean +
                    y1_sd_temp:c66_46_c_bmi_cat))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)

#c66_46_q1_ht, c66_46_edu, c66_46_alc, , sex, c66_46_urban_rural 
```

-   **Intercept:** The estimated mean systolic blood pressure among non-obese with sd_temp = 0 is 121,88.

-   **Coefficient for app_avg_temp (“app_avg_temp main effect”):** *Among non-obese* (the value at which the other term in the interaction is 0 or at its reference level), a 1 standard deviation difference (increase) in sd_temp is associated with a 0.2 unit difference (increase) in systolic blood pressure

-   **Coefficient for Bmi_cat (“Bmi_cat main effect”):** *Among those with* sd_temp *= 0 degrees* (the value at which the other term in the interaction is 0 or at its reference level), obese people have ana average systolic blood pressure that is 11.23 units greater (since the coefficient is positive) than non-obese.

-   **Coefficient for sd_temp× Bmi_cat(“interaction effect”):** This model allows the effect of each predictor in the interaction to differ depending on the level of the other. The interaction effect describes the rate at which that occurs. The interaction has two interpretations. First, the app_avg_temp effect on syst.bp. is -0.49 different (smaller) for obese than for non-obese. Second, the bmi_cat difference in mean syst.bp changes by -0.49 bp units for each 1 degree difference in sd_temp. **The impact of BMI category on systolic blood pressure depends on temperature** — as temperature increases, the difference in SBP between obese and non-obese people becomes smaller (by 0.49 mmHg per °C).

#### Visualizing an interaction

The syntax `terms = c("y1_app_temp_sd_46", "c66_46_c_bmi_cat")` requests a plot in which the first variable (`y1_app_temp_sd_46`) is the x-axis variable and which displays the effect of the first variable on the outcome at each level of the second (`c66_46_c_bmi_cat`). Using `type = "pred"` displays the predicted values assuming that all other variables in the model are at a fixed value while `type = "eff"` displays predicted values that are averaged over predictions at different levels of the other predictors. When any numeric predictors are not centered, `type = "pred"` could lead to unrealistic predictions, so we will use `type = "eff"`.

```{r}
# Apply the linear function to each imputed dataset
lm_model_no_inter <- lm(c66_46_avrsybp ~ y1_sd_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = bp_df_plot)

lm_model_inter <- lm(c66_46_avrsybp ~ y1_sd_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean + y1_sd_temp:c66_46_c_bmi_cat, data = bp_df_plot)

# Effect of BMXWAIST at each level of RIAGENDR
# in the model with no interaction
plot_model(lm_model_no_inter,
           type = "eff",
           terms = c("y1_sd_temp", "c66_46_c_bmi_cat"),
           title = "No Interaction",
           axis.title = c("Standard deviation of apparent average temperature (°C)", 
                          "Systolic blood pressure (mmHg)"))

# Effect of BMXWAIST at each level of RIAGENDR
# in the model with an interaction
plot_model(lm_model_inter,
           type = "eff",
           terms = c("y1_sd_temp", "c66_46_c_bmi_cat"),
           title = "Interaction",
           axis.title = c("Standard deviation of apparent average temperature (°C)", 
                          "Systolic blood pressure (mmHg)"))
```

**Model with no interaction:** In the *left panel* , the sd_temp effects (slopes) for obese and non-obese are *assumed* to be the same, with a common estimate of 0.09, which comes from the model with no interaction . Also, the obese line is slightly above the non-obese line by an amount equal to the bmi_cat effect (6.74). The assumption of no interaction is equivalent to assuming that the lines are *parallel*.

**Model with an interaction:** In the *right panel*, including an interaction term has the effect of fitting separate regression lines for nonobese and obese. The sd_temp slope for nonobese (0.2) is not steeper than for obese (-0.29), indicating that sd_temp has not a greater effect on syst.bp for nonobese than for obese.

#### Testing the difference between the slopes

In the right panel, is the difference between the nonobese and obese slopes statistically significant? This is equivalent to asking, “Are the lines in the plot significantly different from parallel?” The p-value for the regression coefficient for the interaction tests the null hypothesis H0:βinter=0. Under the null hypothesis, the lines are parallel. Thus, to test the difference between the slopes, examine the interaction p-value.

From the output, the p-value for the test of interaction is **0.36**. In this case, since the interaction was between a continuous predictor and a binary predictor, the interaction is just one term, so the test can be obtained directly from the regression coefficient table. When the interaction involves a categorical predictor with more than two levels, we should perform `car::Anova(type=3)`.

Since p \> .05, we can make the following two inferences:

-   There is a non-significant difference in the sd_temp effect on syst.bp between nonobese and obese (the lines are not significantly different from parallel).

-   There is a non significant difference in the bmi_cat effect on syst.bp between those with differing sd_temp (the vertical distance between the lines does not depend on how large or small sd_temp is).

-   Effect of sd_temp on syst.bp does not depend on bmi_cat.

#### Estimating and testing the significance of the slope at each level of a moderator

We just showed that the sd_temp effect is NOT greater for nonobese than for obese (in absolute terms). What if you want to estimate and test the significance of the sd_temp effect for each bmi_cat individually (testing whether the line for nonobese is significantly different from a horizontal line and, separately, testing whether the line for obese is significantly different from a horizontal line)?

The null hypothesis for the test of the sd_temp effect among nonobese is a test of the slope for nonobese H0:b1=0. Since this involves just one regression coefficient, you can read this right off the output in the `sd_temp` row of the `COEFFICIENTS` table since that row corresponds to b1. Thus, the sd_temp effect for nonobese is NOT statistically significant (B = 0.25; 95% CI =-0.26, 0.77; p=0.34)

However, the slope for obese is the sum of two regression coefficients (b1+b3). How do we conduct the test for obese? You *could* re-fit the model after setting “Obese” to be the reference level instead of “Nonobese”. In this new model, the app*\_*avg_temp main effect would be the slope for females. Another option is the function `gmodels::estimable()` can be used to estimate and test any linear combination of regression coefficients.

```{r}
# Test slope for obese
lm_models <- with(pmm_proj_multiimp, 
  lm(c66_46_avrsybp ~ y1_sd_temp +
       c66_46_q1_ht + 
       relevel(c66_46_c_bmi_cat, ref = "Obese") + 
       c66_46_ht_med + c66_46_edu +  
       c66_46_smok + c66_46_alc +
       c66_46_phys + c66_46_occup + 
       sex + 
       c66_46_diab + c66_46_hpcl + 
       c66_46_season + c66_46_urban_rural + day_mean +
       y1_sd_temp:relevel(c66_46_c_bmi_cat, ref = "Obese")))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2) 	
```

##### **Conclusion** 

The sd_temp effects for nonobese (Est = 0.2; 95% CI = -0.31, 0.71; p =0.45) and obese(Est = -0.29; 95% CI = -1.24, 0.65; p = .54) are NOT statistically significant.

The magnitude of the association between sd_temp and syst.bp differs non-significantly between nonobese and obese (p = .38). The association between sd_temp and syst.bp is NOT significant for both nonobese (Est = 0.2; 95% CI = -0.31, 0.71; p =0.45) and obese (Est = -0.29; 95% CI = -1.24, 0.65; p = .54) , but is non-significantly similar in absolute terms for obese (b-interaction (difference in slopes) = -0.49; 95% CI = -1.54, 0.56; p = .36).

#### Overall test of a predictor involved in an interaction

We assessed the significance of an interaction with a test of the null hypothesis that one predictor effect does not vary with another predictor. A test of interaction answers a question such as, “Does the effect of app*\_*avg_temp on syst.bp depend on bmi_cat?”

But what if we want to answer the question, “Is sd_temp significantly associated with syst.bp?” in a model which contains app*\_*avg_temp interacted with another predictor? This is a comparison of two models – one that includes app*\_*avg_temp and one that does not. When there is no interaction, this would simply be a test of the app*\_*avg_temp regression coefficient. When there is an interaction, however, this is a simultaneous test of the app*\_*avg_temp main effect and the app*\_*avg_temp interaction with the other predictor.

To carry out this test, use `anova()` (lowercase a), which compares two nested models: a full model compared to a reduced model with some terms from the full model removed. In this example, we want to compare the full model (with both main effects and the interaction) to the model that does not have app*\_*avg_temp at all (excludes both the app*\_*avg_temp main effect and the interaction, but retains the gender main effect). 

```{r}
# Get a list of all imputed datasets
imputed_datasets <- mice::complete(pmm_proj_multiimp, "all")

# Replace Y, predictors1, and predictors2 with your formulas
# Example:
compare_models <- function(data) {
  fit0 <- lm(c66_46_avrsybp ~ c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = data)
  fit1 <- lm(c66_46_avrsybp ~ y1_sd_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean + y1_sd_temp:c66_46_c_bmi_cat, data = data)
  anova(fit0, fit1)
}

# Apply comparison function to each imputed dataset
anova_results <- lapply(imputed_datasets, compare_models)

p_values = c()

# Inspect results
for (i in seq(1,100,1)) {
  pvalue = anova_results[[i]]$`Pr(>F)`[2]
  p_values = c(p_values, pvalue)
  
}

mean(p_values)  # Average p-value across imputations
sum(p_values < 0.05, na.rm = TRUE)

```

0/100 anova tests have p-value less than 0.05 =\> Thus, we conclude that the model with both sd_temp and sd_temp \*bmi_cat interaction is NOT significantly better than the model with just bmi_cat(p \>.05), or that sd_temp is NOT significantly associated with syst.bp. in a model which contains interaction.

## Diastolic blood pressure

### Apparent average temperature

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrdibp ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean +
                    y1_app_temp_avg_46:c66_46_c_bmi_cat))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)

#c66_46_q1_ht, c66_46_edu, c66_46_alc, , sex, c66_46_urban_rural 
```

-   **Intercept:** The estimated mean diastolic blood pressure among non-obese with app_avg_temp = 0 is 44.07.

-   **Coefficient for app_avg_temp (“app_avg_temp main effect”):** *Among non-obese* (the value at which the other term in the interaction is 0 or at its reference level), a 1 unit difference in app_avg_temp is associated with a -0.65 unit difference (decrease) in diastolic blood pressure

-   **Coefficient for Bmi_cat (“Bmi_cat main effect”):** *Among those with* app_avg_temp *= 0 degrees* (the value at which the other term in the interaction is 0 or at its reference level), obese people have an average diastolic blood pressure that is 5.66 units greater (since the coefficient is positive) than non-obese.

-   **Coefficient for app_avg_temp× Bmi_cat(“interaction effect”):** This model allows the effect of each predictor in the interaction to differ depending on the level of the other. The interaction effect describes the rate at which that occurs. The interaction has two interpretations. First, the app_avg_temp effect on diast.bp. is 0.19 different (smaller) for obese than for non-obese. Second, the bmi_cat difference in mean diast.bp changes by 0.19 bp units for each 1 degree difference in app_avg_temp.

#### Visualizing an interaction

The syntax `terms = c("y1_app_temp_avg_46", "c66_46_c_bmi_cat")` requests a plot in which the first variable (`y1_app_temp_avg_46`) is the x-axis variable and which displays the effect of the first variable on the outcome at each level of the second (`c66_46_c_bmi_cat`). Using `type = "pred"` displays the predicted values assuming that all other variables in the model are at a fixed value while `type = "eff"` displays predicted values that are averaged over predictions at different levels of the other predictors. When any numeric predictors are not centered, `type = "pred"` could lead to unrealistic predictions, so we will use `type = "eff"`.

```{r}
# Apply the linear function to each imputed dataset
lm_model_no_inter <- lm(c66_46_avrdibp ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = bp_df_plot)

lm_model_inter <- lm(c66_46_avrdibp ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean + y1_app_temp_avg_46:c66_46_urban_rural + y1_app_temp_avg_46:c66_46_diab +
                    y1_app_temp_avg_46:c66_46_c_bmi_cat +
                    y1_app_temp_avg_46:c66_46_q1_ht + y1_app_temp_avg_46:sex, data = bp_df_plot)

# Effect of BMXWAIST at each level of RIAGENDR
# in the model with no interaction
plot_model(lm_model_no_inter,
           type = "eff",
           terms = c("y1_app_temp_avg_46", "c66_46_c_bmi_cat"),
           title = "No Interaction",
           axis.title = c("Apparent average temperature (°C)", 
                          "diastolic blood pressure (mmHg)"))

# Effect of BMXWAIST at each level of RIAGENDR
# in the model with an interaction
plot_model(lm_model_inter,
           type = "eff",
           terms = c("y1_app_temp_avg_46", "c66_46_c_bmi_cat"),
           title = "Interaction",
           axis.title = c("Apparent average temperature (°C)", 
                          "diastolic blood pressure (mmHg)"))
```

**Model with no interaction:** In the *left panel* , the app*\_*avg_temp effects (slopes) for obese and non-obese are *assumed* to be the same, with a common estimate of -0.61, which comes from the model with no interaction . Also, the obese line is slightly above the non-obese line by an amount equal to the bmi_cat effect (6.03). The assumption of no interaction is equivalent to assuming that the lines are *parallel*.

**Model with an interaction:** In the *right panel*, including an interaction term has the effect of fitting separate regression lines for nonobese and obese. The app*\_*avg_temp slope for nonobese (-0.65) is much steeper than for obese (-0.46), indicating that app*\_*avg_temp has a greater negative effect on diast.bp for nonobese than for obese.

#### Testing the difference between the slopes

In the right panel, is the difference between the nonobese and obese slopes statistically significant? This is equivalent to asking, “Are the lines in the plot significantly different from parallel?” The p-value for the regression coefficient for the interaction tests the null hypothesis H0:β3=0. Under the null hypothesis, the lines are parallel. Thus, to test the difference between the slopes, examine the interaction p-value.

From the output, the p-value for the test of interaction is 0.33. In this case, since the interaction was between a continuous predictor and a binary predictor, the interaction is just one term, so the test can be obtained directly from the regression coefficient table. When the interaction involves a categorical predictor with more than two levels, we should perform `car::Anova(type=3)`.

Since p \> .05, we can make the following two inferences:

-   There is a non-significant difference in the app*\_*avg_temp effect on diast.bp between nonobese and obese (the lines are not significantly different from parallel).

-   There is a non significant difference in the bmi_cat effect on diast.bp between those with differing app*\_*avg_temp (the vertical distance between the lines does not depend on how large or small app*\_*avg_temp is).

-   Effect of app*\_*avg_temp on diast.bp does not depend on bmi_cat.

#### Estimating and testing the significance of the slope at each level of a moderator

We just showed that the app*\_*avg_temp effect is significantly greater for nonobese than for obese (in absolute terms). What if you want to estimate and test the significance of the app*\_*avg_temp effect for each bmi_cat individually (testing whether the line for nonobese is significantly different from a horizontal line and, separately, testing whether the line for obese is significantly different from a horizontal line)?

The null hypothesis for the test of the app*\_*avg_temp effect among nonobese is a test of the slope for nonobese H0:b1=0. Since this involves just one regression coefficient, you can read this right off the output in the `app_avg_temp` row of the `COEFFICIENTS` table since that row corresponds to b1. Thus, the app*\_*avg_temp effect for nonobese is statistically significant (B = -0.65; 95% CI =-0.84,-0.46; p\<0.01)

However, the slope for females is the sum of two regression coefficients (b1+b3). How do we conduct the test for obese? You *could* re-fit the model after setting “Obese” to be the reference level instead of “Nonobese”. In this new model, the app*\_*avg_temp main effect would be the slope for females. Another option is the function `gmodels::estimable()` can be used to estimate and test any linear combination of regression coefficients.

```{r}
# Test slope for obese
lm_models <- with(pmm_proj_multiimp, 
  lm(c66_46_avrdibp ~ y1_app_temp_avg_46 +
       c66_46_q1_ht + 
       relevel(c66_46_c_bmi_cat, ref = "Obese") + 
       c66_46_ht_med + c66_46_edu +  
       c66_46_smok + c66_46_alc +
       c66_46_phys + c66_46_occup + 
        + sex + 
       c66_46_diab + c66_46_hpcl + 
       c66_46_season + c66_46_urban_rural + day_mean +
       y1_app_temp_avg_46:relevel(c66_46_c_bmi_cat, ref = "Obese") +
       y1_app_temp_avg_46:c66_46_urban_rural + y1_app_temp_avg_46:c66_46_diab +
                    y1_app_temp_avg_46:c66_46_q1_ht + y1_app_temp_avg_46:sex))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)
```

##### **Conclusion** 

The app*\_*avg_temp effects for nonobese (Est = -0.65; 95% CI =-0.84,-0.46; p\<0.01) and obese(Est = -0.46; 95% CI = -0.81, -0.11; p = 0.01) are statistically significant.

The magnitude of the association between app*\_*avg_temp and diast.bp differs non-significantly between nonobese and obese (p = .33). The association between app*\_*avg_temp and diast.bp is significant for both nonobese(Slope = -0.65; 95% CI =-0.84,-0.46; p\<0.01) and obese (Slope = -0.46; 95% CI = -0.81, -0.11; p = 0.01), but is non-significantly lower in absolute terms for obese (b-interaction (difference in slopes) = 0.19; 95% CI = -0.19, 0.57; p = .33).

#### Overall test of a predictor involved in an interaction

We assessed the significance of an interaction with a test of the null hypothesis that one predictor effect does not vary with another predictor. A test of interaction answers a question such as, “Does the effect of app*\_*avg_temp on diast.bp depend on bmi_cat?”

But what if we want to answer the question, “Is app*\_*avg_temp significantly associated with diast.bp?” in a model which contains app*\_*avg_temp interacted with another predictor? This is a comparison of two models – one that includes app*\_*avg_temp and one that does not. When there is no interaction, this would simply be a test of the app*\_*avg_temp regression coefficient. When there is an interaction, however, this is a simultaneous test of the app*\_*avg_temp main effect and the app*\_*avg_temp interaction with the other predictor.

To carry out this test, use `anova()` (lowercase a), which compares two nested models: a full model compared to a reduced model with some terms from the full model removed. In this example, we want to compare the full model (with both main effects and the interaction) to the model that does not have app*\_*avg_temp at all (excludes both the app*\_*avg_temp main effect and the interaction, but retains the gender main effect). 

```{r}
# Get a list of all imputed datasets
imputed_datasets <- mice::complete(pmm_proj_multiimp, "all")

# Replace Y, predictors1, and predictors2 with your formulas
# Example:
compare_models <- function(data) {
  fit0 <- lm(c66_46_avrdibp ~ c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = data)
  fit1 <- lm(c66_46_avrdibp ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean + y1_app_temp_avg_46:c66_46_c_bmi_cat, data = data)
  anova(fit0, fit1)
}

# Apply comparison function to each imputed dataset
anova_results <- lapply(imputed_datasets, compare_models)

p_values = c()

# Inspect results
for (i in seq(1,100,1)) {
  pvalue = anova_results[[i]]$`Pr(>F)`[2]
  p_values = c(p_values, pvalue)
  
}

mean(p_values)  # Average p-value across imputations
sum(p_values < 0.05, na.rm = TRUE)

```

100/100 anova tests have p-value less than 0.05 =\> Thus, we conclude that the model with both app_avg_temp and app_avg_temp \*bmi_cat interaction is significantly better than the model with just bmi_cat(p \<.05), or that app_avg_temp is significantly associated with diast.bp. in a model which contains interaction.

### Apparent standard deviation of average temperature (-)

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrdibp ~ y1_app_temp_sd_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean +
                    y1_app_temp_sd_46:c66_46_c_bmi_cat))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)

#c66_46_q1_ht, c66_46_edu, c66_46_alc, , sex, c66_46_urban_rural 
```

-   **Intercept:** The estimated mean diastolic blood pressure among non-obese with app_sd_temp = 0 is 77.32.

-   **Coefficient for app_avg_temp (“app_avg_temp main effect”):** *Among non-obese* (the value at which the other term in the interaction is 0 or at its reference level), a 1 standard deviation difference (increase) in app_sd_temp is associated with a 0.4 unit difference (increase) in diastolic blood pressure

-   **Coefficient for Bmi_cat (“Bmi_cat main effect”):** *Among those with* app_sd_temp *= 0 degrees* (the value at which the other term in the interaction is 0 or at its reference level), obese people have an average diastolic blood pressure that is 12.9 units greater (since the coefficient is positive) than non-obese.

-   **Coefficient for app_sd_temp× Bmi_cat(“interaction effect”):** This model allows the effect of each predictor in the interaction to differ depending on the level of the other. The interaction effect describes the rate at which that occurs. The interaction has two interpretations. First, the app_avg_temp effect on diast.bp. is -0.65 different (smaller) for obese than for non-obese. Second, the bmi_cat difference in mean diast.bp changes by -0.65 bp units for each 1 degree difference in app_sd_temp. **The impact of BMI category on diastolic blood pressure depends on temperature** — as temperature increases, the difference in SBP between obese and non-obese people becomes smaller (by 0.65 mmHg per °C).

#### Visualizing an interaction

The syntax `terms = c("y1_app_temp_sd_46", "c66_46_c_bmi_cat")` requests a plot in which the first variable (`y1_app_temp_sd_46`) is the x-axis variable and which displays the effect of the first variable on the outcome at each level of the second (`c66_46_c_bmi_cat`). Using `type = "pred"` displays the predicted values assuming that all other variables in the model are at a fixed value while `type = "eff"` displays predicted values that are averaged over predictions at different levels of the other predictors. When any numeric predictors are not centered, `type = "pred"` could lead to unrealistic predictions, so we will use `type = "eff"`.


```{r}
library(sjPlot)
library(ggplot2)

# Apply the linear function to each imputed dataset
lm_model_no_inter <- lm(c66_46_avrdibp ~ y1_app_temp_sd_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = bp_df_plot)

lm_model_inter <- lm(c66_46_avrdibp ~ y1_app_temp_sd_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean +
                    y1_app_temp_sd_46:c66_46_urban_rural + y1_app_temp_sd_46:c66_46_diab +
                    y1_app_temp_sd_46:c66_46_c_bmi_cat +
                    y1_app_temp_sd_46:c66_46_q1_ht + y1_app_temp_sd_46:sex, data = bp_df_plot)


plot_model(
  lm_model_no_inter,
  type = "eff",
  terms = c("y1_app_temp_sd_46", "c66_46_c_bmi_cat"),
  title = "No Interaction term",
  axis.title = c("Standard deviation of apparent temperature", 
                 "Systolic blood pressure (mmHg)"),
  pred.labels = c("Mean Apparent Temp (°C)", "Urbanicity"),
  legend.title = "Urbanicity",
  legend.labels = c("Rural", "Urban")
) +
  # Add a minimal theme
  theme_minimal() +
  # Customize some text sizing and alignment
  theme(
    plot.title = element_text(hjust = 0.5, size = 14),
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 11),
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 11)
  )

plot_model(
  lm_model_inter,
  type = "eff",
  terms = c("y1_app_temp_sd_46", "c66_46_c_bmi_cat"),
  title = "ATV and DBP under interaction with obesity",
  axis.title = c("Standard deviation of the apparent temperature", 
                 "Diastolic blood pressure (mmHg)"),
  pred.labels = c("Mean Apparent Temp (°C)", "Urbanicity"),
  legend.title = "Obesity",
  legend.labels = c("Nonobese", "Obese")
) +
  # Add a minimal theme
  theme_minimal() +
  # Customize some text sizing and alignment
  theme(
    plot.title = element_text(hjust = 0.5, size = 14),
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 12),
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 11)
  )
```


**Model with no interaction:** In the *left panel* , the app*\_*sd_temp effects (slopes) for obese and non-obese are *assumed* to be the same, with a common estimate of 0.26, which comes from the model with no interaction . Also, the obese line is slightly above the non-obese line by an amount equal to the bmi_cat effect (5.99). The assumption of no interaction is equivalent to assuming that the lines are *parallel*.

**Model with an interaction:** In the *right panel*, including an interaction term has the effect of fitting separate regression lines for nonobese and obese. The app*\_*avg_temp slope for nonobese (0.4) is not steeper than for obese (-0.25), indicating that app*\_*sd_temp has not a greater effect on diast.bp for nonobese than for obese.

#### Testing the difference between the slopes

In the right panel, is the difference between the nonobese and obese slopes statistically significant? This is equivalent to asking, “Are the lines in the plot significantly different from parallel?” The p-value for the regression coefficient for the interaction tests the null hypothesis H0:βinter=0. Under the null hypothesis, the lines are parallel. Thus, to test the difference between the slopes, examine the interaction p-value.

From the output, the p-value for the test of interaction is **0.08**. In this case, since the interaction was between a continuous predictor and a binary predictor, the interaction is just one term, so the test can be obtained directly from the regression coefficient table. When the interaction involves a categorical predictor with more than two levels, we should perform `car::Anova(type=3)`.

Since p \> .05, we can make the following two inferences:

-   There is a non-significant difference in the app*\_*sd_temp effect on diast.bp between nonobese and obese (the lines are not significantly different from parallel).

-   There is a non significant difference in the bmi_cat effect on diast.bp between those with differing app*\_*sd_temp (the vertical distance between the lines does not depend on how large or small app*\_*sd_temp is).

-   Effect of app*\_*sd_temp on diast.bp does not depend on bmi_cat.

#### Estimating and testing the significance of the slope at each level of a moderator

We just showed that the app*\_*sd_temp effect is NOT greater for nonobese than for obese (in absolute terms). What if you want to estimate and test the significance of the app*\_*sd_temp effect for each bmi_cat individually (testing whether the line for nonobese is significantly different from a horizontal line and, separately, testing whether the line for obese is significantly different from a horizontal line)?

The null hypothesis for the test of the app*\_*sd_temp effect among nonobese is a test of the slope for nonobese H0:b1=0. Since this involves just one regression coefficient, you can read this right off the output in the `app_sd_temp` row of the `COEFFICIENTS` table since that row corresponds to b1. Thus, the app*\_*sd_temp effect for nonobese is statistically significant (B = 0.4; 95% CI =0.05, 0.75, p=0.03)

However, the slope for obese is the sum of two regression coefficients (b1+b3). How do we conduct the test for obese? You *could* re-fit the model after setting “Obese” to be the reference level instead of “Nonobese”. In this new model, the app*\_*avg_temp main effect would be the slope for females. Another option is the function `gmodels::estimable()` can be used to estimate and test any linear combination of regression coefficients.

```{r}
# Test slope for obese
lm_models <- with(pmm_proj_multiimp, 
  lm(c66_46_avrdibp ~ y1_app_temp_sd_46 +
       c66_46_q1_ht + 
       relevel(c66_46_c_bmi_cat, ref = "Obese") + 
       c66_46_ht_med + c66_46_edu +  
       c66_46_smok + c66_46_alc +
       c66_46_phys + c66_46_occup + 
       sex + 
       c66_46_diab + c66_46_hpcl + 
       c66_46_season + c66_46_urban_rural + day_mean +
       y1_app_temp_sd_46:relevel(c66_46_c_bmi_cat, ref = "Obese") +
       y1_app_temp_sd_46:c66_46_urban_rural +
       y1_app_temp_sd_46:c66_46_diab +
       y1_app_temp_sd_46:c66_46_c_bmi_cat +
                    y1_app_temp_sd_46:c66_46_q1_ht + y1_app_temp_sd_46:sex))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2) 	
```

##### **Conclusion** 

The app*\_*sd_temp effects for nonobese (Est = 0.4; 95% CI =0.05, 0.75, p=0.03) and obese(Est = -0.23; 95% CI = -0.91, 0.39; p = .44) are NOT statistically significant.

The magnitude of the association between app*\_*sd_temp and diast.bp differs non-significantly between nonobese and obese (p = .08). The association between app*\_*sd_temp and diast.bp is significant for nonobese (Est = 0.4; 95% CI =0.05, 0.75, p=0.03) and NOT significant for obese (Est = -0.23; 95% CI = -0.91, 0.39; p = .44), but is non-significantly similar in absolute terms for obese (b-interaction (difference in slopes) = -0.65; 95% CI = -1.38, 0.07; p = .08).

#### Overall test of a predictor involved in an interaction

We assessed the significance of an interaction with a test of the null hypothesis that one predictor effect does not vary with another predictor. A test of interaction answers a question such as, “Does the effect of app*\_*avg_temp on diast.bp depend on bmi_cat?”

But what if we want to answer the question, “Is app*\_*sd_temp significantly associated with diast.bp?” in a model which contains app*\_*avg_temp interacted with another predictor? This is a comparison of two models – one that includes app*\_*avg_temp and one that does not. When there is no interaction, this would simply be a test of the app*\_*avg_temp regression coefficient. When there is an interaction, however, this is a simultaneous test of the app*\_*avg_temp main effect and the app*\_*avg_temp interaction with the other predictor.

To carry out this test, use `anova()` (lowercase a), which compares two nested models: a full model compared to a reduced model with some terms from the full model removed. In this example, we want to compare the full model (with both main effects and the interaction) to the model that does not have app*\_*avg_temp at all (excludes both the app*\_*avg_temp main effect and the interaction, but retains the gender main effect). 

```{r}
# Get a list of all imputed datasets
imputed_datasets <- mice::complete(pmm_proj_multiimp, "all")

# Replace Y, predictors1, and predictors2 with your formulas
# Example:
compare_models <- function(data) {
  fit0 <- lm(c66_46_avrdibp ~ c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = data)
  fit1 <- lm(c66_46_avrdibp ~ y1_app_temp_sd_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean + y1_app_temp_sd_46:c66_46_c_bmi_cat, data = data)
  anova(fit0, fit1)
}

# Apply comparison function to each imputed dataset
anova_results <- lapply(imputed_datasets, compare_models)

p_values = c()

# Inspect results
for (i in seq(1,100,1)) {
  pvalue = anova_results[[i]]$`Pr(>F)`[2]
  p_values = c(p_values, pvalue)
  
}

mean(p_values)  # Average p-value across imputations
sum(p_values < 0.05, na.rm = TRUE)

```

32/100 anova tests have p-value less than 0.05 =\> Thus, we conclude that the model with both app_sd_temp and app_sd_temp \*bmi_cat interaction is NOT significantly better than the model with just bmi_cat(p \>.05), or that app_sd_temp is NOT significantly associated with diast.bp. in a model which contains interaction.

### Average temperature

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrdibp ~ y1_avg_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean +
                    y1_avg_temp:c66_46_c_bmi_cat))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)

#c66_46_q1_ht, c66_46_edu, c66_46_alc, , sex, c66_46_urban_rural 
```

-   **Intercept:** The estimated mean diastolic blood pressure among non-obese with avg_temp = 0 is 44.24.

-   **Coefficient for avg_temp (“avg_temp main effect”):** *Among non-obese* (the value at which the other term in the interaction is 0 or at its reference level), a 1 unit difference in avg_temp is associated with a -0.69 unit difference (decrease) in diastolic blood pressure

-   **Coefficient for Bmi_cat (“Bmi_cat main effect”):** *Among those with* avg_temp *= 0 degrees* (the value at which the other term in the interaction is 0 or at its reference level), obese people have an average diastolic blood pressure that is 5.05 units greater (since the coefficient is positive) than non-obese.

-   **Coefficient for app_avg_temp× Bmi_cat(“interaction effect”):** This model allows the effect of each predictor in the interaction to differ depending on the level of the other. The interaction effect describes the rate at which that occurs. The interaction has two interpretations. First, the avg_temp effect on diast.bp. is 0.22 different (smaller i nabs. terms) for obese than for non-obese. Second, the bmi_cat difference in mean diast.bp changes by 0.23 bp units for each 1 degree difference in app_avg_temp. **The impact of BMI category on diastolic blood pressure depends on temperature** — as temperature increases, the difference in SBP between obese and non-obese people becomes bigger (by 0.23 mmHg per °C).

#### Visualizing an interaction

The syntax `terms = c("y1_avg_temp", "c66_46_c_bmi_cat")` requests a plot in which the first variable (`y1_avg_temp`) is the x-axis variable and which displays the effect of the first variable on the outcome at each level of the second (`c66_46_c_bmi_cat`). Using `type = "pred"` displays the predicted values assuming that all other variables in the model are at a fixed value while `type = "eff"` displays predicted values that are averaged over predictions at different levels of the other predictors. When any numeric predictors are not centered, `type = "pred"` could lead to unrealistic predictions, so we will use `type = "eff"`.

```{r}
# Apply the linear function to each imputed dataset
lm_model_no_inter <- lm(c66_46_avrdibp ~ y1_avg_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = bp_df_plot)

lm_model_inter <- lm(c66_46_avrdibp ~ y1_avg_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean + y1_avg_temp:c66_46_c_bmi_cat, data = bp_df_plot)

# Effect of BMXWAIST at each level of RIAGENDR
# in the model with no interaction
plot_model(lm_model_no_inter,
           type = "eff",
           terms = c("y1_avg_temp", "c66_46_c_bmi_cat"),
           title = "No Interaction",
           axis.title = c("Apparent average temperature (°C)", 
                          "diastolic blood pressure (mmHg)"))

# Effect of BMXWAIST at each level of RIAGENDR
# in the model with an interaction
plot_model(lm_model_inter,
           type = "eff",
           terms = c("y1_avg_temp", "c66_46_c_bmi_cat"),
           title = "Interaction",
           axis.title = c("Apparent average temperature (°C)", 
                          "diastolic blood pressure (mmHg)"))
```

**Model with no interaction:** In the *left panel* , the app*\_*avg_temp effects (slopes) for obese and non-obese are *assumed* to be the same, with a common estimate of -0.88, which comes from the model with no interaction . Also, the obese line is slightly above the non-obese line by an amount equal to the bmi_cat effect (6.8). The assumption of no interaction is equivalent to assuming that the lines are *parallel*.

**Model with an interaction:** In the *right panel*, including an interaction term has the effect of fitting separate regression lines for nonobese and obese. The avg_temp slope for nonobese (-0.69) is much steeper than for obese (-0.46), indicating that avg_temp has a greater negative effect on diast.bp for nonobese than for obese.

#### Testing the difference between the slopes

In the right panel, is the difference between the nonobese and obeыe slopes statistically significant? This is equivalent to asking, “Are the lines in the plot significantly different from parallel?” The p-value for the regression coefficient for the interaction tests the null hypothesis H0:β3=0. Under the null hypothesis, the lines are parallel. Thus, to test the difference between the slopes, examine the interaction p-value.

From the output, the p-value for the test of interaction is **0.27**. In this case, since the interaction was between a continuous predictor and a binary predictor, the interaction is just one term, so the test can be obtained directly from the regression coefficient table. When the interaction involves a categorical predictor with more than two levels, we should perform `car::Anova(type=3)`.

Since p \> .05, we can make the following two inferences:

-   There is a non-significant difference in the avg_temp effect on diast.bp between nonobese and obese (the lines are not significantly different from parallel).

-   There is a non significant difference in the bmi_cat effect on diast.bp between those with differing avg_temp (the vertical distance between the lines does not depend on how large or small avg_temp is).

-   Effect of avg_temp on diast.bp does not depend on bmi_cat.

#### Estimating and testing the significance of the slope at each level of a moderator

We just showed that the avg_temp effect is significantly greater for nonobese than for obese (in absolute terms). What if you want to estimate and test the significance of the avg_temp effect for each bmi_cat individually (testing whether the line for nonobese is significantly different from a horizontal line and, separately, testing whether the line for obese is significantly different from a horizontal line)?

The null hypothesis for the test of the avg_temp effect among nonobese is a test of the slope for nonobese H0:b1=0. Since this involves just one regression coefficient, you can read this right off the output in the `avg_temp` row of the `COEFFICIENTS` table since that row corresponds to b1. Thus, the app*\_*avg_temp effect for nonobese is statistically significant (B = -0.69; 95% CI =-0.9 -0.48; p \<.001).

However, the slope for females is the sum of two regression coefficients (b1+b3). How do we conduct the test for obese? You *could* re-fit the model after setting “Obese” to be the reference level instead of “Nonobese”. In this new model, the app*\_*avg_temp main effect would be the slope for females. Another option is the function `gmodels::estimable()` can be used to estimate and test any linear combination of regression coefficients.

```{r}
# Test slope for obese
lm_models <- with(pmm_proj_multiimp, 
  lm(c66_46_avrdibp ~ y1_avg_temp +
       c66_46_q1_ht + 
       relevel(c66_46_c_bmi_cat, ref = "Obese") + 
       c66_46_ht_med + c66_46_edu +  
       c66_46_smok + c66_46_alc +
       c66_46_phys + c66_46_occup + 
        + sex + 
       c66_46_diab + c66_46_hpcl + 
       c66_46_season + c66_46_urban_rural + day_mean +
       y1_avg_temp:relevel(c66_46_c_bmi_cat, ref = "Obese")))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)
```

##### **Conclusion** 

The app*\_*avg_temp effects for nonobese (B = -0.69; 95% CI =-0.9 -0.48; p \<.001) and obese (Est = -0.46; 95% CI = -0.84, -0.08; p = .02) are statistically significant.

The magnitude of the association between avg_temp and diast.bp differs non-significantly between nonobese and obese (p = .27). The association between avg_temp and diast.bp is significant for both nonobese (B = -0.69; 95% CI =-0.9 -0.48; p \<.001) and obese (Est =-0.46; 95% CI = -0.84, -0.08; p = .02), but is non-significantly lower in absolute terms for obese (b-interaction (difference in slopes) = 0.23; 95% CI = -0.18, 0.64; p = .27).

#### Overall test of a predictor involved in an interaction

We assessed the significance of an interaction with a test of the null hypothesis that one predictor effect does not vary with another predictor. A test of interaction answers a question such as, “Does the effect of avg_temp on diast.bp depend on bmi_cat?”

But what if we want to answer the question, “Is avg_temp significantly associated with diast.bp?” in a model which contains avg_temp interacted with another predictor? This is a comparison of two models – one that includes avg_temp and one that does not. When there is no interaction, this would simply be a test of the avg_temp regression coefficient. When there is an interaction, however, this is a simultaneous test of the avg_temp main effect and the avg_temp interaction with the other predictor.

To carry out this test, use `anova()` (lowercase a), which compares two nested models: a full model compared to a reduced model with some terms from the full model removed. In this example, we want to compare the full model (with both main effects and the interaction) to the model that does not have avg_temp at all (excludes both the app*\_*avg_temp main effect and the interaction, but retains the gender main effect). 

```{r}
# Get a list of all imputed datasets
imputed_datasets <- mice::complete(pmm_proj_multiimp, "all")

# Replace Y, predictors1, and predictors2 with your formulas
# Example:
compare_models <- function(data) {
  fit0 <- lm(c66_46_avrdibp ~ c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = data)
  fit1 <- lm(c66_46_avrdibp ~ y1_avg_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean + y1_avg_temp:c66_46_c_bmi_cat, data = data)
  anova(fit0, fit1)
}

# Apply comparison function to each imputed dataset
anova_results <- lapply(imputed_datasets, compare_models)

p_values = c()

# Inspect results
for (i in seq(1,100,1)) {
  pvalue = anova_results[[i]]$`Pr(>F)`[2]
  p_values = c(p_values, pvalue)
  
}

mean(p_values)  # Average p-value across imputations
sum(p_values < 0.05, na.rm = TRUE)

```

100/100 anova tests have p-value less than 0.05 =\> Thus, we conclude that the model with both avg_temp and avg_temp \*bmi_cat interaction is significantly better than the model with just bmi_cat(p \<.05), or that avg_temp is significantly associated with diast.bp. in a model which contains interaction.

### Standard deviation of average temperature (-)

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrdibp ~ y1_sd_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean +
                    y1_sd_temp:c66_46_c_bmi_cat))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)

#c66_46_q1_ht, c66_46_edu, c66_46_alc, , sex, c66_46_urban_rural 
```

-   **Intercept:** The estimated mean diastolic blood pressure among non-obese with sd_temp = 0 is 77,88.

-   **Coefficient for app_avg_temp (“app_avg_temp main effect”):** *Among non-obese* (the value at which the other term in the interaction is 0 or at its reference level), a 1 standard deviation difference (increase) in sd_temp is associated with a 0.41 unit difference (increase) in diastolic blood pressure

-   **Coefficient for Bmi_cat (“Bmi_cat main effect”):** *Among those with* sd_temp *= 0 degrees* (the value at which the other term in the interaction is 0 or at its reference level), obese people have ana average diastolic blood pressure that is 12.02 units greater (since the coefficient is positive) than non-obese.

-   **Coefficient for sd_temp× Bmi_cat(“interaction effect”):** This model allows the effect of each predictor in the interaction to differ depending on the level of the other. The interaction effect describes the rate at which that occurs. The interaction has two interpretations. First, the app_avg_temp effect on diast.bp. is -0.66 different (smaller) for obese than for non-obese. Second, the bmi_cat difference in mean diast.bp changes by -0.66 bp units for each 1 degree difference in sd_temp. **The impact of BMI category on diastolic blood pressure depends on temperature** — as temperature increases, the difference in SBP between obese and non-obese people becomes smaller (by 0.66 mmHg per °C).

#### Visualizing an interaction

The syntax `terms = c("y1_app_temp_sd_46", "c66_46_c_bmi_cat")` requests a plot in which the first variable (`y1_app_temp_sd_46`) is the x-axis variable and which displays the effect of the first variable on the outcome at each level of the second (`c66_46_c_bmi_cat`). Using `type = "pred"` displays the predicted values assuming that all other variables in the model are at a fixed value while `type = "eff"` displays predicted values that are averaged over predictions at different levels of the other predictors. When any numeric predictors are not centered, `type = "pred"` could lead to unrealistic predictions, so we will use `type = "eff"`.

```{r}
# Apply the linear function to each imputed dataset
lm_model_no_inter <- lm(c66_46_avrdibp ~ y1_sd_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = bp_df_plot)

lm_model_inter <- lm(c66_46_avrdibp ~ y1_sd_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean + y1_sd_temp:c66_46_c_bmi_cat, data = bp_df_plot)

# Effect of BMXWAIST at each level of RIAGENDR
# in the model with no interaction
plot_model(lm_model_no_inter,
           type = "eff",
           terms = c("y1_sd_temp", "c66_46_c_bmi_cat"),
           title = "No Interaction",
           axis.title = c("Standard deviation of average temperature (°C)", 
                          "diastolic blood pressure (mmHg)"))

# Effect of BMXWAIST at each level of RIAGENDR
# in the model with an interaction
plot_model(lm_model_inter,
           type = "eff",
           terms = c("y1_sd_temp", "c66_46_c_bmi_cat"),
           title = "Interaction",
           axis.title = c("Standard deviation of average temperature (°C)", 
                          "diastolic blood pressure (mmHg)"))
```

**Model with no interaction:** In the *left panel* , the sd_temp effects (slopes) for obese and non-obese are *assumed* to be the same, with a common estimate of 0.27, which comes from the model with no interaction . Also, the obese line is slightly above the non-obese line by an amount equal to the bmi_cat effect (5.99). The assumption of no interaction is equivalent to assuming that the lines are *parallel*.

**Model with an interaction:** In the *right panel*, including an interaction term has the effect of fitting separate regression lines for nonobese and obese. The sd_temp slope for nonobese (0.4) is not steeper than for obese (-0.25), indicating that sd_temp has not a greater effect on diast.bp for nonobese than for obese.

#### Testing the difference between the slopes

In the right panel, is the difference between the nonobese and obese slopes statistically significant? This is equivalent to asking, “Are the lines in the plot significantly different from parallel?” The p-value for the regression coefficient for the interaction tests the null hypothesis H0:βinter=0. Under the null hypothesis, the lines are parallel. Thus, to test the difference between the slopes, examine the interaction p-value.

From the output, the p-value for the test of interaction is **0.07**. In this case, since the interaction was between a continuous predictor and a binary predictor, the interaction is just one term, so the test can be obtained directly from the regression coefficient table. When the interaction involves a categorical predictor with more than two levels, we should perform `car::Anova(type=3)`.

Since p \> .05, we can make the following two inferences:

-   There is a non-significant difference in the sd_temp effect on diast.bp between nonobese and obese (the lines are not significantly different from parallel).

-   There is a non significant difference in the bmi_cat effect on diast.bp between those with differing sd_temp (the vertical distance between the lines does not depend on how large or small sd_temp is).

-   Effect of sd_temp on diast.bp does not depend on bmi_cat.

#### Estimating and testing the significance of the slope at each level of a moderator

We just showed that the sd_temp effect is NOT greater for nonobese than for obese (in absolute terms). What if you want to estimate and test the significance of the sd_temp effect for each bmi_cat individually (testing whether the line for nonobese is significantly different from a horizontal line and, separately, testing whether the line for obese is significantly different from a horizontal line)?

The null hypothesis for the test of the sd_temp effect among nonobese is a test of the slope for nonobese H0:b1=0. Since this involves just one regression coefficient, you can read this right off the output in the `sd_temp` row of the `COEFFICIENTS` table since that row corresponds to b1. Thus, the sd_temp effect for nonobese is statistically significant (B = 0.41; 95% CI =0.07, 0.75; p=0.02)

However, the slope for obese is the sum of two regression coefficients (b1+b3). How do we conduct the test for obese? You *could* re-fit the model after setting “Obese” to be the reference level instead of “Nonobese”. In this new model, the app*\_*avg_temp main effect would be the slope for females. Another option is the function `gmodels::estimable()` can be used to estimate and test any linear combination of regression coefficients.

```{r}
# Test slope for obese
lm_models <- with(pmm_proj_multiimp, 
  lm(c66_46_avrdibp ~ y1_sd_temp +
       c66_46_q1_ht + 
       relevel(c66_46_c_bmi_cat, ref = "Obese") + 
       c66_46_ht_med + c66_46_edu +  
       c66_46_smok + c66_46_alc +
       c66_46_phys + c66_46_occup + 
       sex + 
       c66_46_diab + c66_46_hpcl + 
       c66_46_season + c66_46_urban_rural + day_mean +
       y1_sd_temp:relevel(c66_46_c_bmi_cat, ref = "Obese")))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2) 	
```

##### **Conclusion** 

The sd_temp effects for nonobese (Est = 0.41; 95% CI =0.07, 0.75; p=0.02) and obese(Est = -0.25; 95% CI = -0.89, 0.39; p = .44) are NOT statistically significant.

The magnitude of the association between sd_temp and diast.bp differs non-significantly between nonobese and obese (p = .38). The association between sd_temp and diast.bp is NOT significant for both nonobese (Est = 0.41; 95% CI =0.07, 0.75; p=0.02) and obese (Est = -0.25; 95% CI = -0.89, 0.39; p = .44) , but is non-significantly similar in absolute terms for obese (b-interaction (difference in slopes) = -0.66; 95% CI = -1.37, 0.05; p = .07).

#### Overall test of a predictor involved in an interaction

We assessed the significance of an interaction with a test of the null hypothesis that one predictor effect does not vary with another predictor. A test of interaction answers a question such as, “Does the effect of app*\_*avg_temp on diast.bp depend on bmi_cat?”

But what if we want to answer the question, “Is sd_temp significantly associated with diast.bp?” in a model which contains app*\_*avg_temp interacted with another predictor? This is a comparison of two models – one that includes app*\_*avg_temp and one that does not. When there is no interaction, this would simply be a test of the app*\_*avg_temp regression coefficient. When there is an interaction, however, this is a simultaneous test of the app*\_*avg_temp main effect and the app*\_*avg_temp interaction with the other predictor.

To carry out this test, use `anova()` (lowercase a), which compares two nested models: a full model compared to a reduced model with some terms from the full model removed. In this example, we want to compare the full model (with both main effects and the interaction) to the model that does not have app*\_*avg_temp at all (excludes both the app*\_*avg_temp main effect and the interaction, but retains the gender main effect). 

```{r}
# Get a list of all imputed datasets
imputed_datasets <- mice::complete(pmm_proj_multiimp, "all")

# Replace Y, predictors1, and predictors2 with your formulas
# Example:
compare_models <- function(data) {
  fit0 <- lm(c66_46_avrdibp ~ c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = data)
  fit1 <- lm(c66_46_avrdibp ~ y1_sd_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean + y1_sd_temp:c66_46_c_bmi_cat, data = data)
  anova(fit0, fit1)
}

# Apply comparison function to each imputed dataset
anova_results <- lapply(imputed_datasets, compare_models)

p_values = c()

# Inspect results
for (i in seq(1,100,1)) {
  pvalue = anova_results[[i]]$`Pr(>F)`[2]
  p_values = c(p_values, pvalue)
  
}

mean(p_values)  # Average p-value across imputations
sum(p_values < 0.05, na.rm = TRUE)

```

78/100 anova tests have p-value less than 0.05 =\> Thus, we conclude that the model with both sd_temp and sd_temp \*bmi_cat interaction is significantly better than the model with just bmi_cat(p \>.05), or that sd_temp is significantly associated with diast.bp. in a model which contains interaction.

# Sex

## Systolic blood pressure

### Apparent average temperature

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrsybp ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean +
                    y1_app_temp_avg_46:sex))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)

#c66_46_q1_ht, c66_46_edu, c66_46_alc, , sex, c66_46_urban_rural 
```

-   **Intercept:** The estimated mean systolic blood pressure among male with app_avg_temp = 0 is 46.3.

-   **Coefficient for app_avg_temp (“app_avg_temp main effect”):** *Among male* (the value at which the other term in the interaction is 0 or at its reference level), a 1 unit difference in app_avg_temp is associated with a -1.02 unit difference (decrease) in systolic blood pressure

-   **Coefficient for Bmi_cat (“sex main effect”):** *Among those with* app_avg_temp *= 0 degrees* (the value at which the other term in the interaction is 0 or at its reference level), female people have an average systolic blood pressure that is -8.28 units lower (since the coefficient is negative) than male.

-   **Coefficient for app_avg_temp× Bmi_cat(“interaction effect”):** This model allows the effect of each predictor in the interaction to differ depending on the level of the other. The interaction effect describes the rate at which that occurs. The interaction has two interpretations. First, the app_avg_tempeffect on syst.bp. is 0.24 different (smaller) for female than for male. Second, the bmi_cat difference in mean syst.bp changes by 0.24 bp units for each 1 degree difference in app_avg_temp.

#### Visualizing an interaction

```{r}
# Apply the linear function to each imputed dataset
lm_model_no_inter <- lm(c66_46_avrsybp ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = bp_df_plot)

lm_model_inter <- lm(c66_46_avrsybp ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean + y1_app_temp_avg_46:c66_46_urban_rural + y1_app_temp_avg_46:c66_46_diab +
                    y1_app_temp_avg_46:c66_46_c_bmi_cat +
                    y1_app_temp_avg_46:c66_46_q1_ht + y1_app_temp_avg_46:sex, data = bp_df_plot)

# Effect of BMXWAIST at each level of RIAGENDR
# in the model with no interaction
plot_model(lm_model_no_inter,
           type = "eff",
           terms = c("y1_app_temp_avg_46", "sex"),
           title = "No Interaction",
           axis.title = c("Apparent average temperature (°C)", 
                          "Systolic blood pressure (mmHg)"))

# Effect of BMXWAIST at each level of RIAGENDR
# in the model with an interaction
plot_model(lm_model_inter,
           type = "eff",
           terms = c("y1_app_temp_avg_46", "sex"),
           title = "Interaction",
           axis.title = c("Apparent average temperature (°C)", 
                          "Systolic blood pressure (mmHg)"))
```

#### Testing the difference between the slopes

In the right panel, is the difference between the male and female slopes statistically significant? This is equivalent to asking, “Are the lines in the plot significantly different from parallel?” The p-value for the regression coefficient for the interaction tests the null hypothesis H0:β3=0. Under the null hypothesis, the lines are parallel. Thus, to test the difference between the slopes, examine the interaction p-value.

From the output, the p-value for the test of interaction is 0.3. In this case, since the interaction was between a continuous predictor and a binary predictor, the interaction is just one term, so the test can be obtained directly from the regression coefficient table. When the interaction involves a categorical predictor with more than two levels, we should perform `car::Anova(type=3)`.

Since p \> .05, we can make the following two inferences:

-   There is a non-significant difference in the app*\_*avg_temp effect on syst.bp between male and female (the lines are not significantly different from parallel).

-   There is a non significant difference in the sex effect on syst.bp between those with differing app*\_*avg_temp (the vertical distance between the lines does not depend on how large or small app*\_*avg_temp is).

-   Effect of app*\_*avg_temp on syst.bp does not depend on sex.

#### Estimating and testing the significance of the slope at each level of a moderator

```{r}
# Test slope for female
lm_models <- with(pmm_proj_multiimp, 
  lm(c66_46_avrsybp ~ y1_app_temp_avg_46 +
       c66_46_q1_ht + c66_46_c_bmi_cat +
       relevel(sex, ref = "Female") + 
       c66_46_ht_med + c66_46_edu +  
       c66_46_smok + c66_46_alc +
       c66_46_phys + c66_46_occup + 
        +  
       c66_46_diab + c66_46_hpcl + 
       c66_46_season + c66_46_urban_rural + day_mean +
       y1_app_temp_avg_46:relevel(sex, ref = "Female")))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)
```

##### **Conclusion** 

Female(Est = -0.71; 95% p = \<01) is statistically significant.

#### Overall test of a predictor involved in an interaction

```{r}
# Get a list of all imputed datasets
imputed_datasets <- mice::complete(pmm_proj_multiimp, "all")

# Replace Y, predictors1, and predictors2 with your formulas
# Example:
compare_models <- function(data) {
  fit0 <- lm(c66_46_avrsybp ~ c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = data)
  fit1 <- lm(c66_46_avrsybp ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean + y1_app_temp_avg_46:sex, data = data)
  anova(fit0, fit1)
}

# Apply comparison function to each imputed dataset
anova_results <- lapply(imputed_datasets, compare_models)

p_values = c()

# Inspect results
for (i in seq(1,100,1)) {
  pvalue = anova_results[[i]]$`Pr(>F)`[2]
  p_values = c(p_values, pvalue)
  
}

mean(p_values)  # Average p-value across imputations
sum(p_values < 0.05, na.rm = TRUE)

```

100/100 anova tests have p-value less than 0.05 =\> Thus, we conclude that the model with both app_avg_temp and app_avg_temp sex interaction is significantly better than the model with just bmi_cat(p \<.05), or that app_avg_temp is significantly associated with syst.bp. in a model which contains interaction.

### Apparent standard deviation of average temperature (-)

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrsybp ~ y1_app_temp_sd_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean +
                    y1_app_temp_sd_46:sex))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)

#c66_46_q1_ht, c66_46_edu, c66_46_alc, , sex, c66_46_urban_rural 
```

-   **Intercept:** The estimated mean systolic blood pressure among male with app_sd_temp = 0 is 99.33.

-   **Coefficient for app_avg_temp (“app_avg_temp main effect”):** *Among male* (the value at which the other term in the interaction is 0 or at its reference level), a 1 standard deviation difference (increase) in app_sd_temp is associated with a 0.39 unit difference (increase) in systolic blood pressure

-   **Coefficient for Bmi_cat (“sex main effect”):** *Among those with* app_sd_temp *= 0 degrees* (the value at which the other term in the interaction is 0 or at its reference level), female people have an average systolic blood pressure that is -4.88 units greater (since the coefficient is positive) than male.

-   **Coefficient for app_sd_temp× Bmi_cat(“interaction effect”):** This model allows the effect of each predictor in the interaction to differ depending on the level of the other. The interaction effect describes the rate at which that occurs. The interaction has two interpretations. First, the app_avg_temp effect on syst.bp. is -0.29 different (smaller) for female than for male. Second, the bmi_cat difference in mean syst.bp changes by -0.29 bp units for each 1 degree difference in app_sd_temp. **The impact of BMI category on systolic blood pressure depends on temperature** — as temperature increases, the difference in SBP between female and male people becomes smaller (by 0.29 mmHg per °C).

#### Visualizing an interaction

```{r}
# Apply the linear function to each imputed dataset
lm_model_no_inter <- lm(c66_46_avrsybp ~ y1_app_temp_sd_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = bp_df_plot)

lm_model_inter <- lm(c66_46_avrsybp ~ y1_app_temp_sd_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean + y1_app_temp_sd_46:sex, data = bp_df_plot)

# Effect of BMXWAIST at each level of RIAGENDR
# in the model with no interaction
plot_model(lm_model_no_inter,
           type = "eff",
           terms = c("y1_app_temp_sd_46", "sex"),
           title = "No Interaction",
           axis.title = c("Standard deviation of apparent average temperature (°C)", 
                          "Systolic blood pressure (mmHg)"))

# Effect of BMXWAIST at each level of RIAGENDR
# in the model with an interaction
plot_model(lm_model_inter,
           type = "eff",
           terms = c("y1_app_temp_sd_46", "sex"),
           title = "Interaction",
           axis.title = c("Standard deviation of apparent average temperature (°C)", 
                          "Systolic blood pressure (mmHg)"))
```

#### Testing the difference between the slopes

In the right panel, is the difference between the male and female slopes statistically significant? This is equivalent to asking, “Are the lines in the plot significantly different from parallel?” The p-value for the regression coefficient for the interaction tests the null hypothesis H0:βinter=0. Under the null hypothesis, the lines are parallel. Thus, to test the difference between the slopes, examine the interaction p-value.

From the output, the p-value for the test of interaction is **0.52**. In this case, since the interaction was between a continuous predictor and a binary predictor, the interaction is just one term, so the test can be obtained directly from the regression coefficient table. When the interaction involves a categorical predictor with more than two levels, we should perform `car::Anova(type=3)`.

Since p \> .05, we can make the following two inferences:

-   There is a non-significant difference in the app*\_*sd_temp effect on syst.bp between male and female (the lines are not significantly different from parallel).

-   There is a non significant difference in the sex effect on syst.bp between those with differing app*\_*sd_temp (the vertical distance between the lines does not depend on how large or small app*\_*sd_temp is).

-   Effect of app*\_*sd_temp on syst.bp does not depend on sex.

#### Estimating and testing the significance of the slope at each level of a moderator

```{r}
# Test slope for female
lm_models <- with(pmm_proj_multiimp, 
  lm(c66_46_avrsybp ~ y1_app_temp_sd_46 +
       c66_46_q1_ht + c66_46_c_bmi_cat +
       relevel(sex, ref = "Female") + 
       c66_46_ht_med + c66_46_edu +  
       c66_46_smok + c66_46_alc +
       c66_46_phys + c66_46_occup + 
       c66_46_diab + c66_46_hpcl + 
       c66_46_season + c66_46_urban_rural + day_mean +
       y1_app_temp_sd_46:relevel(sex, ref = "Female")))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2) 	
```

##### **Conclusion** 

Female(Est = -0.13; p = .67) are NOT statistically significant.

#### Overall test of a predictor involved in an interaction

```{r}
# Get a list of all imputed datasets
imputed_datasets <- mice::complete(pmm_proj_multiimp, "all")

# Replace Y, predictors1, and predictors2 with your formulas
# Example:
compare_models <- function(data) {
  fit0 <- lm(c66_46_avrsybp ~ c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = data)
  fit1 <- lm(c66_46_avrsybp ~ y1_app_temp_sd_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean + y1_app_temp_sd_46:sex, data = data)
  anova(fit0, fit1)
}

# Apply comparison function to each imputed dataset
anova_results <- lapply(imputed_datasets, compare_models)

p_values = c()

# Inspect results
for (i in seq(1,100,1)) {
  pvalue = anova_results[[i]]$`Pr(>F)`[2]
  p_values = c(p_values, pvalue)
  
}

mean(p_values)  # Average p-value across imputations
sum(p_values < 0.05, na.rm = TRUE)

```

0/100 anova tests have p-value less than 0.05 =\> Thus, we conclude that the model with both app_sd_temp and app_sd_temp \*sex interaction is NOT significantly better than the model with just sex(p \>.05), or that app_sd_temp is NOT significantly associated with syst.bp. in a model which contains interaction.

### Average temperature

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrsybp ~ y1_avg_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean +
                    y1_avg_temp:sex))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)

#c66_46_q1_ht, c66_46_edu, c66_46_alc, , sex, c66_46_urban_rural 
```

-   **Intercept:** The estimated mean systolic blood pressure among male with avg_temp = 0 is 47.31.

-   **Coefficient for avg_temp (“avg_temp main effect”):** *Among male* (the value at which the other term in the interaction is 0 or at its reference level), a 1 unit difference in avg_temp is associated with a -1.05 unit difference (decrease) in systolic blood pressure

-   **Coefficient for Bmi_cat (“sex main effect”):** *Among those with* avg_temp *= 0 degrees* (the value at which the other term in the interaction is 0 or at its reference level), female people have an average systolic blood pressure that is -8.83 units greater (since the coefficient is positive) than male.

-   **Coefficient for app_avg_temp× Bmi_cat(“interaction effect”):** This model allows the effect of each predictor in the interaction to differ depending on the level of the other. The interaction effect describes the rate at which that occurs. The interaction has two interpretations. First, the avg_temp effect on syst.bp. is 0.24 different (smaller i nabs. terms) for female than for male. Second, the bmi_cat difference in mean syst.bp changes by 0.24 bp units for each 1 degree difference in app_avg_temp. **The impact of BMI category on systolic blood pressure depends on temperature** — as temperature increases, the difference in SBP between female and male people becomes bigger (by 0.24 mmHg per °C).

#### Visualizing an interaction

```{r}
# Apply the linear function to each imputed dataset
lm_model_no_inter <- lm(c66_46_avrsybp ~ y1_avg_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = bp_df_plot)

lm_model_inter <- lm(c66_46_avrsybp ~ y1_avg_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean + y1_avg_temp:sex, data = bp_df_plot)

# Effect of BMXWAIST at each level of RIAGENDR
# in the model with no interaction
plot_model(lm_model_no_inter,
           type = "eff",
           terms = c("y1_avg_temp", "sex"),
           title = "No Interaction",
           axis.title = c("Apparent average temperature (°C)", 
                          "Systolic blood pressure (mmHg)"))

# Effect of BMXWAIST at each level of RIAGENDR
# in the model with an interaction
plot_model(lm_model_inter,
           type = "eff",
           terms = c("y1_avg_temp", "sex"),
           title = "Interaction",
           axis.title = c("Apparent average temperature (°C)", 
                          "Systolic blood pressure (mmHg)"))
```

#### Testing the difference between the slopes

In the right panel, is the difference between the male and obeыe slopes statistically significant? This is equivalent to asking, “Are the lines in the plot significantly different from parallel?” The p-value for the regression coefficient for the interaction tests the null hypothesis H0:β3=0. Under the null hypothesis, the lines are parallel. Thus, to test the difference between the slopes, examine the interaction p-value.

From the output, the p-value for the test of interaction is **0.33**. In this case, since the interaction was between a continuous predictor and a binary predictor, the interaction is just one term, so the test can be obtained directly from the regression coefficient table. When the interaction involves a categorical predictor with more than two levels, we should perform `car::Anova(type=3)`.

Since p \> .05, we can make the following two inferences:

-   There is a non-significant difference in the avg_temp effect on syst.bp between male and female (the lines are not significantly different from parallel).

-   There is a non significant difference in the sex effect on syst.bp between those with differing avg_temp (the vertical distance between the lines does not depend on how large or small avg_temp is).

-   Effect of avg_temp on syst.bp does not depend on sex.

#### Estimating and testing the significance of the slope at each level of a moderator

```{r}
# Test slope for female
lm_models <- with(pmm_proj_multiimp, 
  lm(c66_46_avrsybp ~ y1_avg_temp +
       c66_46_q1_ht + c66_46_c_bmi_cat +
       relevel(sex, ref = "Female") + 
       c66_46_ht_med + c66_46_edu +  
       c66_46_smok + c66_46_alc +
       c66_46_phys + c66_46_occup + 
        + 
       c66_46_diab + c66_46_hpcl + 
       c66_46_season + c66_46_urban_rural + day_mean +
       y1_avg_temp:relevel(sex, ref = "Female")))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)
```

##### **Conclusion** 

Female (Est = -0.73; p \< .00) is statistically significant.

#### Overall test of a predictor involved in an interaction

```{r}
# Get a list of all imputed datasets
imputed_datasets <- mice::complete(pmm_proj_multiimp, "all")

# Replace Y, predictors1, and predictors2 with your formulas
# Example:
compare_models <- function(data) {
  fit0 <- lm(c66_46_avrsybp ~ c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = data)
  fit1 <- lm(c66_46_avrsybp ~ y1_avg_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean + y1_avg_temp:sex, data = data)
  anova(fit0, fit1)
}

# Apply comparison function to each imputed dataset
anova_results <- lapply(imputed_datasets, compare_models)

p_values = c()

# Inspect results
for (i in seq(1,100,1)) {
  pvalue = anova_results[[i]]$`Pr(>F)`[2]
  p_values = c(p_values, pvalue)
  
}

mean(p_values)  # Average p-value across imputations
sum(p_values < 0.05, na.rm = TRUE)

```

100/100 anova tests have p-value less than 0.05 =\> Thus, we conclude that the model with both avg_temp and avg_temp\* sex interaction is significantly better than the model with just sex(p \<.05), or that avg_temp is significantly associated with syst.bp. in a model which contains interaction.

### Standard deviation of average temperature (-)

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrsybp ~ y1_sd_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean +
                    y1_sd_temp:sex))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)

#c66_46_q1_ht, c66_46_edu, c66_46_alc, , sex, c66_46_urban_rural 
```

-   **Intercept:** The estimated mean systolic blood pressure among male with sd_temp = 0 is 100.55.

-   **Coefficient for app_avg_temp (“app_avg_temp main effect”):** *Among male* (the value at which the other term in the interaction is 0 or at its reference level), a 1 standard deviation difference (increase) in sd_temp is associated with a 0.32 unit difference (increase) in systolic blood pressure

-   **Coefficient for Bmi_cat (“sex main effect”):** *Among those with* sd_temp *= 0 degrees* (the value at which the other term in the interaction is 0 or at its reference level), female people have ana average systolic blood pressure that is -5.63 units greater (since the coefficient is positive) than male.

-   **Coefficient for sd_temp× Bmi_cat(“interaction effect”):** This model allows the effect of each predictor in the interaction to differ depending on the level of the other. The interaction effect describes the rate at which that occurs. The interaction has two interpretations. First, the app_avg_temp effect on syst.bp. is -0.25 different (smaller) for female than for male. Second, the bmi_cat difference in mean syst.bp changes by -0.25 bp units for each 1 degree difference in sd_temp. **The impact of BMI category on systolic blood pressure depends on temperature** — as temperature increases, the difference in SBP between female and male people becomes smaller (by 0.25 mmHg per °C).

#### Visualizing an interaction

```{r}
# Apply the linear function to each imputed dataset
lm_model_no_inter <- lm(c66_46_avrsybp ~ y1_sd_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = bp_df_plot)

lm_model_inter <- lm(c66_46_avrsybp ~ y1_sd_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean + y1_sd_temp:sex, data = bp_df_plot)

# Effect of BMXWAIST at each level of RIAGENDR
# in the model with no interaction
plot_model(lm_model_no_inter,
           type = "eff",
           terms = c("y1_sd_temp", "sex"),
           title = "No Interaction",
           axis.title = c("Standard deviation of apparent average temperature (°C)", 
                          "Systolic blood pressure (mmHg)"))

# Effect of BMXWAIST at each level of RIAGENDR
# in the model with an interaction
plot_model(lm_model_inter,
           type = "eff",
           terms = c("y1_sd_temp", "sex"),
           title = "Interaction",
           axis.title = c("Standard deviation of apparent average temperature (°C)", 
                          "Systolic blood pressure (mmHg)"))
```

#### Testing the difference between the slopes

In the right panel, is the difference between the male and female slopes statistically significant? This is equivalent to asking, “Are the lines in the plot significantly different from parallel?” The p-value for the regression coefficient for the interaction tests the null hypothesis H0:βinter=0. Under the null hypothesis, the lines are parallel. Thus, to test the difference between the slopes, examine the interaction p-value.

From the output, the p-value for the test of interaction is **0.57**. In this case, since the interaction was between a continuous predictor and a binary predictor, the interaction is just one term, so the test can be obtained directly from the regression coefficient table. When the interaction involves a categorical predictor with more than two levels, we should perform `car::Anova(type=3)`.

Since p \> .05, we can make the following two inferences:

-   There is a non-significant difference in the sd_temp effect on syst.bp between male and female (the lines are not significantly different from parallel).

-   There is a non significant difference in the sex effect on syst.bp between those with differing sd_temp (the vertical distance between the lines does not depend on how large or small sd_temp is).

-   Effect of sd_temp on syst.bp does not depend on sex.

#### Estimating and testing the significance of the slope at each level of a moderator

```{r}
# Test slope for female
lm_models <- with(pmm_proj_multiimp, 
  lm(c66_46_avrsybp ~ y1_sd_temp +
       c66_46_q1_ht + c66_46_c_bmi_cat +
       relevel(sex, ref = "Female") + 
       c66_46_ht_med + c66_46_edu +  
       c66_46_smok + c66_46_alc +
       c66_46_phys + c66_46_occup + 
       c66_46_diab + c66_46_hpcl + 
       c66_46_season + c66_46_urban_rural + day_mean +
       y1_sd_temp:relevel(sex, ref = "Female")))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2) 	
```

##### **Conclusion** 

Female(Est = -0.18; p = .56) are NOT statistically significant.

#### Overall test of a predictor involved in an interaction

```{r}
# Get a list of all imputed datasets
imputed_datasets <- mice::complete(pmm_proj_multiimp, "all")

# Replace Y, predictors1, and predictors2 with your formulas
# Example:
compare_models <- function(data) {
  fit0 <- lm(c66_46_avrsybp ~ c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = data)
  fit1 <- lm(c66_46_avrsybp ~ y1_sd_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean + y1_sd_temp:sex, data = data)
  anova(fit0, fit1)
}

# Apply comparison function to each imputed dataset
anova_results <- lapply(imputed_datasets, compare_models)

p_values = c()

# Inspect results
for (i in seq(1,100,1)) {
  pvalue = anova_results[[i]]$`Pr(>F)`[2]
  p_values = c(p_values, pvalue)
  
}

mean(p_values)  # Average p-value across imputations
sum(p_values < 0.05, na.rm = TRUE)

```

0/100 anova tests have p-value less than 0.05 =\> Thus, we conclude that the model with both sd_temp and sd_temp \*sex interaction is NOT significantly better than the model with just sex (p \>.05), or that sd_temp is NOT significantly associated with syst.bp. in a model which contains interaction.

## Diastolic blood pressure

### Apparent average temperature

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrdibp ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean +
                    y1_app_temp_avg_46:sex))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)

#c66_46_q1_ht, c66_46_edu, c66_46_alc, , sex, c66_46_urban_rural 
```

-   **Intercept:** The estimated mean diastolic blood pressure among male with app_avg_temp = 0 is 29.1.

-   **Coefficient for app_avg_temp (“app_avg_temp main effect”):** *Among male* (the value at which the other term in the interaction is 0 or at its reference level), a 1 unit difference in app_avg_temp is associated with a -0.6 unit difference (decrease) in diastolic blood pressure

-   **Coefficient for Bmi_cat (“sex main effect”):** *Among those with* app_avg_temp *= 0 degrees* (the value at which the other term in the interaction is 0 or at its reference level), female people have an average diastolic blood pressure that is -2.33 units greater (since the coefficient is positive) than male.

-   **Coefficient for app_avg_temp× Bmi_cat(“interaction effect”):** This model allows the effect of each predictor in the interaction to differ depending on the level of the other. The interaction effect describes the rate at which that occurs. The interaction has two interpretations. First, the app_avg_temp effect on diast.bp. is -0.05 different (smaller) for female than for male. Second, the bmi_cat difference in mean diast.bp changes by -0.05 bp units for each 1 degree difference in app_avg_temp. T**he impact of BMI category on systolic blood pressure depends on temperature** — as temperature increases, the difference in SBP between female and male people becomes bigger (by 0.05 mmHg per °C).

#### Visualizing an interaction

```{r}
# Apply the linear function to each imputed dataset
lm_model_no_inter <- lm(c66_46_avrdibp ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = bp_df_plot)

lm_model_inter <- lm(c66_46_avrdibp ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean + y1_app_temp_avg_46:c66_46_urban_rural + y1_app_temp_avg_46:c66_46_diab +
                    y1_app_temp_avg_46:c66_46_c_bmi_cat +
                    y1_app_temp_avg_46:c66_46_q1_ht + y1_app_temp_avg_46:sex, data = bp_df_plot)

# Effect of BMXWAIST at each level of RIAGENDR
# in the model with no interaction
plot_model(lm_model_no_inter,
           type = "eff",
           terms = c("y1_app_temp_avg_46", "sex"),
           title = "No Interaction",
           axis.title = c("Apparent average temperature (°C)", 
                          "diastolic blood pressure (mmHg)"))

# Effect of BMXWAIST at each level of RIAGENDR
# in the model with an interaction
plot_model(lm_model_inter,
           type = "eff",
           terms = c("y1_app_temp_avg_46", "sex"),
           title = "Interaction",
           axis.title = c("Apparent average temperature (°C)", 
                          "diastolic blood pressure (mmHg)"))
```

#### Testing the difference between the slopes

In the right panel, is the difference between the male and female slopes statistically significant? This is equivalent to asking, “Are the lines in the plot significantly different from parallel?” The p-value for the regression coefficient for the interaction tests the null hypothesis H0:β3=0. Under the null hypothesis, the lines are parallel. Thus, to test the difference between the slopes, examine the interaction p-value.

From the output, the p-value for the test of interaction is **0.72**. In this case, since the interaction was between a continuous predictor and a binary predictor, the interaction is just one term, so the test can be obtained directly from the regression coefficient table. When the interaction involves a categorical predictor with more than two levels, we should perform `car::Anova(type=3)`.

Since p \> .05, we can make the following two inferences:

-   There is a non-significant difference in the app*\_*avg_temp effect on diast.bp between male and female (the lines are not significantly different from parallel).

-   There is a non significant difference in the sex effect on diast.bp between those with differing app*\_*avg_temp (the vertical distance between the lines does not depend on how large or small app*\_*avg_temp is).

-   Effect of app*\_*avg_temp on diast.bp does not depend on sex.

#### Estimating and testing the significance of the slope at each level of a moderator

```{r}
# Test slope for female
lm_models <- with(pmm_proj_multiimp, 
  lm(c66_46_avrdibp ~ y1_app_temp_avg_46 +
       c66_46_q1_ht + c66_46_c_bmi_cat +
       relevel(sex, ref = "Female") + 
       c66_46_ht_med + c66_46_edu +  
       c66_46_smok + c66_46_alc +
       c66_46_phys + c66_46_occup + 
        + 
       c66_46_diab + c66_46_hpcl + 
       c66_46_season + c66_46_urban_rural + day_mean +
       y1_app_temp_avg_46:relevel(sex, ref = "Female")))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)
```

##### **Conclusion** 

The app*\_*avg_temp effects for female(Est = -0.6; 95% p \< 0.01) are statistically significant.

#### Overall test of a predictor involved in an interaction

```{r}
# Get a list of all imputed datasets
imputed_datasets <- mice::complete(pmm_proj_multiimp, "all")

# Replace Y, predictors1, and predictors2 with your formulas
# Example:
compare_models <- function(data) {
  fit0 <- lm(c66_46_avrdibp ~ c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = data)
  fit1 <- lm(c66_46_avrdibp ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean + y1_app_temp_avg_46:sex, data = data)
  anova(fit0, fit1)
}

# Apply comparison function to each imputed dataset
anova_results <- lapply(imputed_datasets, compare_models)

p_values = c()

# Inspect results
for (i in seq(1,100,1)) {
  pvalue = anova_results[[i]]$`Pr(>F)`[2]
  p_values = c(p_values, pvalue)
  
}

mean(p_values)  # Average p-value across imputations
sum(p_values < 0.05, na.rm = TRUE)

```

100/100 anova tests have p-value less than 0.05 =\> Thus, we conclude that the model with both app_avg_temp and app_avg_temp \*sex interaction is significantly better than the model with just sex(p \<.05), or that app_avg_temp is significantly associated with diast.bp. in a model which contains interaction.

### Apparent standard deviation of average temperature (-)

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrdibp ~ y1_app_temp_sd_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean +
                    y1_app_temp_sd_46:sex))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)

#c66_46_q1_ht, c66_46_edu, c66_46_alc, , sex, c66_46_urban_rural 
```

-   **Intercept:** The estimated mean diastolic blood pressure among male with app_sd_temp = 0 is 61.49.

-   **Coefficient for app_avg_temp (“app_avg_temp main effect”):** *Among male* (the value at which the other term in the interaction is 0 or at its reference level), a 1 standard deviation difference (increase) in app_sd_temp is associated with a 0.23 unit difference (increase) in diastolic blood pressure

-   **Coefficient for Bmi_cat (“sex main effect”):** *Among those with* app_sd_temp *= 0 degrees* (the value at which the other term in the interaction is 0 or at its reference level), female people have an average diastolic blood pressure that is -4.45 units greater (since the coefficient is positive) than male.

-   **Coefficient for app_sd_temp× Bmi_cat(“interaction effect”):** This model allows the effect of each predictor in the interaction to differ depending on the level of the other. The interaction effect describes the rate at which that occurs. The interaction has two interpretations. First, the app_avg_temp effect on diast.bp. is 0.18 different (smaller) for female than for male. Second, the bmi_cat difference in mean diast.bp changes by 0.18 bp units for each 1 degree difference in app_sd_temp. **The impact of BMI category on diastolic blood pressure depends on temperature** — as temperature increases, the difference in SBP between female and male people becomes smaller (by 0.18 mmHg per °C).

#### Visualizing an interaction

```{r}
# Apply the linear function to each imputed dataset
lm_model_no_inter <- lm(c66_46_avrdibp ~ y1_app_temp_sd_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = bp_df_plot)

lm_model_inter <- lm(c66_46_avrdibp ~ y1_app_temp_sd_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean + y1_app_temp_sd_46:sex, data = bp_df_plot)

# Effect of BMXWAIST at each level of RIAGENDR
# in the model with no interaction
plot_model(lm_model_no_inter,
           type = "eff",
           terms = c("y1_app_temp_sd_46", "sex"),
           title = "No Interaction",
           axis.title = c("Standard deviation of apparent average temperature (°C)", 
                          "diastolic blood pressure (mmHg)"))

# Effect of BMXWAIST at each level of RIAGENDR
# in the model with an interaction
plot_model(lm_model_inter,
           type = "eff",
           terms = c("y1_app_temp_sd_46", "sex"),
           title = "Interaction",
           axis.title = c("Standard deviation of apparent average temperature (°C)", 
                          "diastolic blood pressure (mmHg)"))
```

#### Testing the difference between the slopes

In the right panel, is the difference between the male and female slopes statistically significant? This is equivalent to asking, “Are the lines in the plot significantly different from parallel?” The p-value for the regression coefficient for the interaction tests the null hypothesis H0:βinter=0. Under the null hypothesis, the lines are parallel. Thus, to test the difference between the slopes, examine the interaction p-value.

From the output, the p-value for the test of interaction is **0.53**. In this case, since the interaction was between a continuous predictor and a binary predictor, the interaction is just one term, so the test can be obtained directly from the regression coefficient table. When the interaction involves a categorical predictor with more than two levels, we should perform `car::Anova(type=3)`.

Since p \> .05, we can make the following two inferences:

-   There is a non-significant difference in the app*\_*sd_temp effect on diast.bp between male and female (the lines are not significantly different from parallel).

-   There is a non significant difference in the sex effect on diast.bp between those with differing app*\_*sd_temp (the vertical distance between the lines does not depend on how large or small app*\_*sd_temp is).

-   Effect of app*\_*sd_temp on diast.bp does not depend on sex.

#### Estimating and testing the significance of the slope at each level of a moderator

```{r}
# Test slope for female
lm_models <- with(pmm_proj_multiimp, 
  lm(c66_46_avrdibp ~ y1_app_temp_sd_46 +
       c66_46_q1_ht + c66_46_c_bmi_cat +
       relevel(sex, ref = "Female") + 
       c66_46_ht_med + c66_46_edu +  
       c66_46_smok + c66_46_alc +
       c66_46_phys + c66_46_occup + 
       c66_46_diab + c66_46_hpcl + 
       c66_46_season + c66_46_urban_rural + day_mean +
       y1_app_temp_sd_46:relevel(sex, ref = "Female")))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2) 	
```

##### **Conclusion** 

The app*\_*sd_temp effects for female(0.2; p = .34) are NOT statistically significant.

#### Overall test of a predictor involved in an interaction

```{r}
# Get a list of all imputed datasets
imputed_datasets <- mice::complete(pmm_proj_multiimp, "all")

# Replace Y, predictors1, and predictors2 with your formulas
# Example:
compare_models <- function(data) {
  fit0 <- lm(c66_46_avrdibp ~ c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = data)
  fit1 <- lm(c66_46_avrdibp ~ y1_app_temp_sd_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean + y1_app_temp_sd_46:sex, data = data)
  anova(fit0, fit1)
}

# Apply comparison function to each imputed dataset
anova_results <- lapply(imputed_datasets, compare_models)

p_values = c()

# Inspect results
for (i in seq(1,100,1)) {
  pvalue = anova_results[[i]]$`Pr(>F)`[2]
  p_values = c(p_values, pvalue)
  
}

mean(p_values)  # Average p-value across imputations
sum(p_values < 0.05, na.rm = TRUE)

```

0/100 anova tests have p-value less than 0.05 =\> Thus, we conclude that the model with both app_sd_temp and app_sd_temp\* sex interaction is NOT significantly better than the model with just sex(p \>.05), or that app_sd_temp is NOT significantly associated with diast.bp. in a model which contains interaction.

### Average temperature

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrdibp ~ y1_avg_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean +
                    y1_avg_temp:sex))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)

#c66_46_q1_ht, c66_46_edu, c66_46_alc, , sex, c66_46_urban_rural 
```

-   **Intercept:** The estimated mean diastolic blood pressure among male with avg_temp = 0 is 28.78.

-   **Coefficient for avg_temp (“avg_temp main effect”):** *Among male* (the value at which the other term in the interaction is 0 or at its reference level), a 1 unit difference in avg_temp is associated with a -0.62 unit difference (decrease) in diastolic blood pressure

-   **Coefficient for Bmi_cat (“sex main effect”):** *Among those with* avg_temp *= 0 degrees* (the value at which the other term in the interaction is 0 or at its reference level), female people have an average diastolic blood pressure that is -2.07 units greater (since the coefficient is positive) than male.

-   **Coefficient for app_avg_temp× Bmi_cat(“interaction effect”):** This model allows the effect of each predictor in the interaction to differ depending on the level of the other. The interaction effect describes the rate at which that occurs. The interaction has two interpretations. First, the avg_temp effect on diast.bp. is -0.09 different (smaller i nabs. terms) for female than for male. Second, the sex difference in mean diast.bp changes by -0.09 bp units for each 1 degree difference in app_avg_temp. **The impact of BMI category on diastolic blood pressure depends on temperature** — as temperature increases, the difference in SBP between female and male people becomes bigger (by -0.09 mmHg per °C).

#### Visualizing an interaction

```{r}
# Apply the linear function to each imputed dataset
lm_model_no_inter <- lm(c66_46_avrdibp ~ y1_avg_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = bp_df_plot)

lm_model_inter <- lm(c66_46_avrdibp ~ y1_avg_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean + y1_avg_temp:sex, data = bp_df_plot)

# Effect of BMXWAIST at each level of RIAGENDR
# in the model with no interaction
plot_model(lm_model_no_inter,
           type = "eff",
           terms = c("y1_avg_temp", "sex"),
           title = "No Interaction",
           axis.title = c("Apparent average temperature (°C)", 
                          "diastolic blood pressure (mmHg)"))

# Effect of BMXWAIST at each level of RIAGENDR
# in the model with an interaction
plot_model(lm_model_inter,
           type = "eff",
           terms = c("y1_avg_temp", "sex"),
           title = "Interaction",
           axis.title = c("Apparent average temperature (°C)", 
                          "diastolic blood pressure (mmHg)"))
```

#### Testing the difference between the slopes

In the right panel, is the difference between the male and obeыe slopes statistically significant? This is equivalent to asking, “Are the lines in the plot significantly different from parallel?” The p-value for the regression coefficient for the interaction tests the null hypothesis H0:β3=0. Under the null hypothesis, the lines are parallel. Thus, to test the difference between the slopes, examine the interaction p-value.

From the output, the p-value for the test of interaction is **0.6**. In this case, since the interaction was between a continuous predictor and a binary predictor, the interaction is just one term, so the test can be obtained directly from the regression coefficient table. When the interaction involves a categorical predictor with more than two levels, we should perform `car::Anova(type=3)`.

Since p \> .05, we can make the following two inferences:

-   There is a non-significant difference in the avg_temp effect on diast.bp between male and female (the lines are not significantly different from parallel).

-   There is a non significant difference in the sex effect on diast.bp between those with differing avg_temp (the vertical distance between the lines does not depend on how large or small avg_temp is).

-   Effect of avg_temp on diast.bp does not depend on sex.

#### Estimating and testing the significance of the slope at each level of a moderator

```{r}
# Test slope for female
lm_models <- with(pmm_proj_multiimp, 
  lm(c66_46_avrdibp ~ y1_avg_temp +
       c66_46_q1_ht + c66_46_c_bmi_cat +
       relevel(sex, ref = "Female") + 
       c66_46_ht_med + c66_46_edu +  
       c66_46_smok + c66_46_alc +
       c66_46_phys + c66_46_occup + 
        + 
       c66_46_diab + c66_46_hpcl + 
       c66_46_season + c66_46_urban_rural + day_mean +
       y1_avg_temp:relevel(sex, ref = "Female")))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)
```

##### **Conclusion** 

The app*\_*avg_temp effects for female (Est = -0.64; p \< .01) are statistically significant.

#### Overall test of a predictor involved in an interaction

```{r}
# Get a list of all imputed datasets
imputed_datasets <- mice::complete(pmm_proj_multiimp, "all")

# Replace Y, predictors1, and predictors2 with your formulas
# Example:
compare_models <- function(data) {
  fit0 <- lm(c66_46_avrdibp ~ c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = data)
  fit1 <- lm(c66_46_avrdibp ~ y1_avg_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean + y1_avg_temp:sex, data = data)
  anova(fit0, fit1)
}

# Apply comparison function to each imputed dataset
anova_results <- lapply(imputed_datasets, compare_models)

p_values = c()

# Inspect results
for (i in seq(1,100,1)) {
  pvalue = anova_results[[i]]$`Pr(>F)`[2]
  p_values = c(p_values, pvalue)
  
}

mean(p_values)  # Average p-value across imputations
sum(p_values < 0.05, na.rm = TRUE)

```

100/100 anova tests have p-value less than 0.05 =\> Thus, we conclude that the model with both avg_temp and avg_temp\* sex interaction is significantly better than the model with just sex (p \<.05), or that avg_temp is significantly associated with diast.bp. in a model which contains interaction.

### Standard deviation of average temperature (-)

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrdibp ~ y1_sd_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean +
                    y1_sd_temp:sex))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)

#c66_46_q1_ht, c66_46_edu, c66_46_alc, , sex, c66_46_urban_rural 
```

-   **Intercept:** The estimated mean diastolic blood pressure among male with sd_temp = 0 is 61.89.

-   **Coefficient for app_avg_temp (“app_avg_temp main effect”):** *Among male* (the value at which the other term in the interaction is 0 or at its reference level), a 1 standard deviation difference (increase) in sd_temp is associated with a 0.22 unit difference (increase) in diastolic blood pressure

-   **Coefficient for Bmi_cat (“sex main effect”):** *Among those with* sd_temp *= 0 degrees* (the value at which the other term in the interaction is 0 or at its reference level), female people have ana average diastolic blood pressure that is -4.55 units greater (since the coefficient is positive) than male.

-   **Coefficient for sd_temp× Bmi_cat(“interaction effect”):** This model allows the effect of each predictor in the interaction to differ depending on the level of the other. The interaction effect describes the rate at which that occurs. The interaction has two interpretations. First, the app_avg_temp effect on diast.bp. is 0.22 different (smaller) for female than for male. Second, the bmi_cat difference in mean diast.bp changes by 0.22 bp units for each 1 degree difference in sd_temp. **The impact of BMI category on diastolic blood pressure depends on temperature** — as temperature increases, the difference in SBP between female and male people becomes smaller (by 0.22 mmHg per °C).

#### Visualizing an interaction

```{r}
# Apply the linear function to each imputed dataset
lm_model_no_inter <- lm(c66_46_avrdibp ~ y1_sd_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = bp_df_plot)

lm_model_inter <- lm(c66_46_avrdibp ~ y1_sd_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean + y1_sd_temp:sex, data = bp_df_plot)

# Effect of BMXWAIST at each level of RIAGENDR
# in the model with no interaction
plot_model(lm_model_no_inter,
           type = "eff",
           terms = c("y1_sd_temp", "sex"),
           title = "No Interaction",
           axis.title = c("Standard deviation of average temperature (°C)", 
                          "diastolic blood pressure (mmHg)"))

# Effect of BMXWAIST at each level of RIAGENDR
# in the model with an interaction
plot_model(lm_model_inter,
           type = "eff",
           terms = c("y1_sd_temp", "sex"),
           title = "Interaction",
           axis.title = c("Standard deviation of average temperature (°C)", 
                          "diastolic blood pressure (mmHg)"))
```

#### Testing the difference between the slopes

In the right panel, is the difference between the male and female slopes statistically significant? This is equivalent to asking, “Are the lines in the plot significantly different from parallel?” The p-value for the regression coefficient for the interaction tests the null hypothesis H0:βinter=0. Under the null hypothesis, the lines are parallel. Thus, to test the difference between the slopes, examine the interaction p-value.

From the output, the p-value for the test of interaction is **0.44**. In this case, since the interaction was between a continuous predictor and a binary predictor, the interaction is just one term, so the test can be obtained directly from the regression coefficient table. When the interaction involves a categorical predictor with more than two levels, we should perform `car::Anova(type=3)`.

Since p \> .05, we can make the following two inferences:

-   There is a non-significant difference in the sd_temp effect on diast.bp between male and female (the lines are not significantly different from parallel).

-   There is a non significant difference in the sex effect on diast.bp between those with differing sd_temp (the vertical distance between the lines does not depend on how large or small sd_temp is).

-   Effect of sd_temp on diast.bp does not depend on bmi_cat.

#### Estimating and testing the significance of the slope at each level of a moderator

```{r}
# Test slope for female
lm_models <- with(pmm_proj_multiimp, 
  lm(c66_46_avrdibp ~ y1_sd_temp +
       c66_46_q1_ht + c66_46_c_bmi_cat +
       relevel(sex, ref = "Female") + 
       c66_46_ht_med + c66_46_edu +  
       c66_46_smok + c66_46_alc +
       c66_46_phys + c66_46_occup + 
       c66_46_diab + c66_46_hpcl + 
       c66_46_season + c66_46_urban_rural + day_mean +
       y1_sd_temp:relevel(sex, ref = "Female")))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2) 	
```

##### **Conclusion** 

The sd_temp effects for female(Est = 0.23, p = .28) are NOT statistically significant.

#### Overall test of a predictor involved in an interaction

```{r}
# Get a list of all imputed datasets
imputed_datasets <- mice::complete(pmm_proj_multiimp, "all")

# Replace Y, predictors1, and predictors2 with your formulas
# Example:
compare_models <- function(data) {
  fit0 <- lm(c66_46_avrdibp ~ c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = data)
  fit1 <- lm(c66_46_avrdibp ~ y1_sd_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean + y1_sd_temp:sex, data = data)
  anova(fit0, fit1)
}

# Apply comparison function to each imputed dataset
anova_results <- lapply(imputed_datasets, compare_models)

p_values = c()

# Inspect results
for (i in seq(1,100,1)) {
  pvalue = anova_results[[i]]$`Pr(>F)`[2]
  p_values = c(p_values, pvalue)
  
}

mean(p_values)  # Average p-value across imputations
sum(p_values < 0.05, na.rm = TRUE)

```

25/100 anova tests have p-value less than 0.05 =\> Thus, we conclude that the model with both sd_temp and sd_temp sex interaction is significantly better than the model with just sex(p \>.05), or that sd_temp is significantly associated with diast.bp. in a model which contains interaction.

# Urban/rural

## Systolic blood pressure

### Apparent average temperature

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrsybp ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean +
                    y1_app_temp_avg_46:c66_46_urban_rural))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)

#c66_46_q1_ht, c66_46_edu, c66_46_alc, , sex, c66_46_urban_rural 
```

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrsybp ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean +
                    y1_app_temp_avg_46:c66_46_urban_rural + y1_app_temp_avg_46:c66_46_diab +
                    y1_app_temp_avg_46:c66_46_c_bmi_cat +
                    y1_app_temp_avg_46:c66_46_q1_ht + y1_app_temp_avg_46:sex))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)

#c66_46_q1_ht, c66_46_edu, c66_46_alc, , sex, c66_46_urban_rural 
```

#### Visualizing an interaction

```{r}
library(sjPlot)
library(ggplot2)

# Apply the linear function to each imputed dataset
lm_model_no_inter <- lm(c66_46_avrsybp ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = bp_df_plot)

lm_model_inter <- lm(c66_46_avrsybp ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean +
                    y1_app_temp_avg_46:c66_46_urban_rural + y1_app_temp_avg_46:c66_46_diab +
                    y1_app_temp_avg_46:c66_46_c_bmi_cat +
                    y1_app_temp_avg_46:c66_46_q1_ht + y1_app_temp_avg_46:sex, data = bp_df_plot)


plot_model(
  lm_model_no_inter,
  type = "eff",
  terms = c("y1_app_temp_avg_46", "c66_46_urban_rural"),
  title = "No Interaction term",
  axis.title = c("Annual apparent average temperature (°C)", 
                 "Systolic blood pressure (mmHg)"),
  pred.labels = c("Mean Apparent Temp (°C)", "Urbanicity"),
  legend.title = "Urbanicity",
  legend.labels = c("Rural", "Urban")
) +
  # Add a minimal theme
  theme_minimal() +
  # Customize some text sizing and alignment
  theme(
    plot.title = element_text(hjust = 0.5, size = 14),
    axis.title = element_text(size = 13),
    axis.text = element_text(size = 11),
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 11)
  )

plot_model(
  lm_model_inter,
  type = "eff",
  terms = c("y1_app_temp_avg_46", "c66_46_urban_rural"),
  title = "ApT and SBP under interaction with urbanicity",
  axis.title = c("Annual apparent average temperature (°C)", 
                 "Systolic blood pressure (mmHg)"),
  pred.labels = c("Mean Apparent Temp (°C)", "Urbanicity"),
  legend.title = "Urbanicity",
  legend.labels = c("Rural area", "Urban area")
) +
  # Add a minimal theme
  theme_minimal() +
  # Customize some text sizing and alignment
  theme(
    plot.title = element_text(hjust = 0.5, size = 14),
    axis.title = element_text(size = 13),
    axis.text = element_text(size = 12),
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 11)
  )

```

#### Testing the difference between the slopes

In the right panel, is the difference between the male and female slopes statistically significant? This is equivalent to asking, “Are the lines in the plot significantly different from parallel?” The p-value for the regression coefficient for the interaction tests the null hypothesis H0:β3=0. Under the null hypothesis, the lines are parallel. Thus, to test the difference between the slopes, examine the interaction p-value.

From the output, the p-value for the test of interaction is 0.3. In this case, since the interaction was between a continuous predictor and a binary predictor, the interaction is just one term, so the test can be obtained directly from the regression coefficient table. When the interaction involves a categorical predictor with more than two levels, we should perform `car::Anova(type=3)`.

Since p \> .05, we can make the following two inferences:

-   There is a non-significant difference in the app*\_*avg_temp effect on syst.bp between male and female (the lines are not significantly different from parallel).

-   There is a non significant difference in the sex effect on syst.bp between those with differing app*\_*avg_temp (the vertical distance between the lines does not depend on how large or small app*\_*avg_temp is).

-   Effect of app*\_*avg_temp on syst.bp does not depend on sex.

#### Estimating and testing the significance of the slope at each level of a moderator

```{r}
# Test slope for female
lm_models <- with(pmm_proj_multiimp, 
  lm(c66_46_avrsybp ~ y1_app_temp_avg_46 +
       c66_46_q1_ht + c66_46_c_bmi_cat +
       relevel(c66_46_urban_rural, ref = "Rural") + 
       c66_46_ht_med + c66_46_edu +  
       c66_46_smok + c66_46_alc +
       c66_46_phys + c66_46_occup + 
        sex + 
       c66_46_diab + c66_46_hpcl + 
       c66_46_season + day_mean +
       y1_app_temp_avg_46:relevel(c66_46_urban_rural, ref = "Rural") +
       y1_app_temp_avg_46:c66_46_diab +
       y1_app_temp_avg_46:c66_46_c_bmi_cat +
                    y1_app_temp_avg_46:c66_46_q1_ht + y1_app_temp_avg_46:sex
       
     )
  )

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)
```

##### **Conclusion** 

Female(Est = -0.71; 95% p = \<01) is statistically significant.

#### Overall test of a predictor involved in an interaction

```{r}
# Get a list of all imputed datasets
imputed_datasets <- mice::complete(pmm_proj_multiimp, "all")

# Replace Y, predictors1, and predictors2 with your formulas
# Example:
compare_models <- function(data) {
  fit0 <- lm(c66_46_avrsybp ~ c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = data)
  fit1 <- lm(c66_46_avrsybp ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean + y1_app_temp_avg_46:c66_46_urban_rural, data = data)
  anova(fit0, fit1)
}

# Apply comparison function to each imputed dataset
anova_results <- lapply(imputed_datasets, compare_models)

p_values = c()

# Inspect results
for (i in seq(1,100,1)) {
  pvalue = anova_results[[i]]$`Pr(>F)`[2]
  p_values = c(p_values, pvalue)
  
}

mean(p_values)  # Average p-value across imputations
sum(p_values < 0.05, na.rm = TRUE)

```

100/100 anova tests have p-value less than 0.05 =\> Thus, we conclude that the model with both app_avg_temp and app_avg_temp sex interaction is significantly better than the model with just bmi_cat(p \<.05), or that app_avg_temp is significantly associated with syst.bp. in a model which contains interaction.

### Apparent standard deviation of average temperature (-)

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrsybp ~ y1_app_temp_sd_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean +
                    y1_app_temp_sd_46:c66_46_urban_rural))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)

#c66_46_q1_ht, c66_46_edu, c66_46_alc, , sex, c66_46_urban_rural 
```



```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrsybp ~ y1_app_temp_sd_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean +
                    y1_app_temp_sd_46:c66_46_urban_rural + y1_app_temp_sd_46:c66_46_diab +
                    y1_app_temp_sd_46:c66_46_c_bmi_cat +
                    y1_app_temp_sd_46:c66_46_q1_ht + y1_app_temp_sd_46:sex))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)
```


#### Visualizing an interaction

```{r}
library(sjPlot)
library(ggplot2)

# Apply the linear function to each imputed dataset
lm_model_no_inter <- lm(c66_46_avrsybp ~ y1_app_temp_sd_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = bp_df_plot)

lm_model_inter <- lm(c66_46_avrsybp ~ y1_app_temp_sd_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean +
                    y1_app_temp_sd_46:c66_46_urban_rural + y1_app_temp_sd_46:c66_46_diab +
                    y1_app_temp_sd_46:c66_46_c_bmi_cat +
                    y1_app_temp_sd_46:c66_46_q1_ht + y1_app_temp_sd_46:sex, data = bp_df_plot)


plot_model(
  lm_model_no_inter,
  type = "eff",
  terms = c("y1_app_temp_sd_46", "c66_46_urban_rural"),
  title = "No Interaction term",
  axis.title = c("Standard deviation of apparent temperature", 
                 "Systolic blood pressure (mmHg)"),
  pred.labels = c("Mean Apparent Temp (°C)", "Urbanicity"),
  legend.title = "Urbanicity",
  legend.labels = c("Rural", "Urban")
) +
  # Add a minimal theme
  theme_minimal() +
  # Customize some text sizing and alignment
  theme(
    plot.title = element_text(hjust = 0.5, size = 14),
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 11),
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 11)
  )

plot_model(
  lm_model_inter,
  type = "eff",
  terms = c("y1_app_temp_sd_46", "c66_46_urban_rural"),
  title = "ATV and SBP under interaction with urbanicity",
  axis.title = c("Standard deviation of the apparent temperature", 
                 "Systolic blood pressure (mmHg)"),
  pred.labels = c("Mean Apparent Temp (°C)", "Urbanicity"),
  legend.title = "Urbanicity",
  legend.labels = c("Rural area", "Urban area")
) +
  # Add a minimal theme
  theme_minimal() +
  # Customize some text sizing and alignment
  theme(
    plot.title = element_text(hjust = 0.5, size = 14),
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 12),
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 11)
  )
```


#### Testing the difference between the slopes

In the right panel, is the difference between the male and female slopes statistically significant? This is equivalent to asking, “Are the lines in the plot significantly different from parallel?” The p-value for the regression coefficient for the interaction tests the null hypothesis H0:βinter=0. Under the null hypothesis, the lines are parallel. Thus, to test the difference between the slopes, examine the interaction p-value.

From the output, the p-value for the test of interaction is **0.52**. In this case, since the interaction was between a continuous predictor and a binary predictor, the interaction is just one term, so the test can be obtained directly from the regression coefficient table. When the interaction involves a categorical predictor with more than two levels, we should perform `car::Anova(type=3)`.

Since p \> .05, we can make the following two inferences:

-   There is a non-significant difference in the app*\_*sd_temp effect on syst.bp between male and female (the lines are not significantly different from parallel).

-   There is a non significant difference in the sex effect on syst.bp between those with differing app*\_*sd_temp (the vertical distance between the lines does not depend on how large or small app*\_*sd_temp is).

-   Effect of app*\_*sd_temp on syst.bp does not depend on sex.

#### Estimating and testing the significance of the slope at each level of a moderator

```{r}
# Test slope for female
lm_models <- with(pmm_proj_multiimp, 
  lm(c66_46_avrsybp ~ y1_app_temp_sd_46 +
       c66_46_q1_ht + c66_46_c_bmi_cat +
       relevel(c66_46_urban_rural, ref = "Rural") + 
       c66_46_ht_med + c66_46_edu +  
       c66_46_smok + c66_46_alc +
       c66_46_phys + c66_46_occup + 
       sex + 
       c66_46_diab + c66_46_hpcl + 
       c66_46_season + day_mean +
       y1_app_temp_sd_46:relevel(c66_46_urban_rural, ref = "Rural") +
       y1_app_temp_sd_46:c66_46_diab +
       y1_app_temp_sd_46:c66_46_c_bmi_cat +
                    y1_app_temp_sd_46:c66_46_q1_ht + y1_app_temp_sd_46:sex))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2) 	
```

##### **Conclusion** 

Female(Est = -0.13; p = .67) are NOT statistically significant.

#### Overall test of a predictor involved in an interaction

```{r}
# Get a list of all imputed datasets
imputed_datasets <- mice::complete(pmm_proj_multiimp, "all")

# Replace Y, predictors1, and predictors2 with your formulas
# Example:
compare_models <- function(data) {
  fit0 <- lm(c66_46_avrsybp ~ c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = data)
  fit1 <- lm(c66_46_avrsybp ~ y1_app_temp_sd_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean + y1_app_temp_sd_46:c66_46_urban_rural, data = data)
  anova(fit0, fit1)
}

# Apply comparison function to each imputed dataset
anova_results <- lapply(imputed_datasets, compare_models)

p_values = c()

# Inspect results
for (i in seq(1,100,1)) {
  pvalue = anova_results[[i]]$`Pr(>F)`[2]
  p_values = c(p_values, pvalue)
  
}

mean(p_values)  # Average p-value across imputations
sum(p_values < 0.05, na.rm = TRUE)

```

0/100 anova tests have p-value less than 0.05 =\> Thus, we conclude that the model with both app_sd_temp and app_sd_temp \*sex interaction is NOT significantly better than the model with just sex(p \>.05), or that app_sd_temp is NOT significantly associated with syst.bp. in a model which contains interaction.

### Average temperature

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrsybp ~ y1_avg_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean +
                    y1_avg_temp:c66_46_urban_rural))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)

#c66_46_q1_ht, c66_46_edu, c66_46_alc, , sex, c66_46_urban_rural 
```

#### Visualizing an interaction

```{r}
# Apply the linear function to each imputed dataset
lm_model_no_inter <- lm(c66_46_avrsybp ~ y1_avg_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = bp_df_plot)

lm_model_inter <- lm(c66_46_avrsybp ~ y1_avg_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean + y1_avg_temp:c66_46_urban_rural, data = bp_df_plot)

# Effect of BMXWAIST at each level of RIAGENDR
# in the model with no interaction
plot_model(lm_model_no_inter,
           type = "eff",
           terms = c("y1_avg_temp", "c66_46_urban_rural"),
           title = "No Interaction",
           axis.title = c("average temperature (°C)", 
                          "Systolic blood pressure (mmHg)"))

# Effect of BMXWAIST at each level of RIAGENDR
# in the model with an interaction
plot_model(lm_model_inter,
           type = "eff",
           terms = c("y1_avg_temp", "c66_46_urban_rural"),
           title = "Interaction",
           axis.title = c("average temperature (°C)", 
                          "Systolic blood pressure (mmHg)"))
```

#### Testing the difference between the slopes

From the output, the p-value for the test of interaction is **0.33**. In this case, since the interaction was between a continuous predictor and a binary predictor, the interaction is just one term, so the test can be obtained directly from the regression coefficient table. When the interaction involves a categorical predictor with more than two levels, we should perform `car::Anova(type=3)`.

#### Estimating and testing the significance of the slope at each level of a moderator

```{r}
# Test slope for female
lm_models <- with(pmm_proj_multiimp, 
  lm(c66_46_avrsybp ~ y1_avg_temp +
       c66_46_q1_ht + c66_46_c_bmi_cat +
       relevel(c66_46_urban_rural, ref = "Rural") + 
       c66_46_ht_med + c66_46_edu +  
       c66_46_smok + c66_46_alc +
       c66_46_phys + c66_46_occup + 
        + sex + 
       c66_46_diab + c66_46_hpcl + 
       c66_46_season + day_mean +
       y1_avg_temp:relevel(c66_46_urban_rural, ref = "Rural")))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)
```

##### **Conclusion** 

Female (Est = -0.73; p \< .00) is statistically significant.

#### Overall test of a predictor involved in an interaction

```{r}
# Get a list of all imputed datasets
imputed_datasets <- mice::complete(pmm_proj_multiimp, "all")

# Replace Y, predictors1, and predictors2 with your formulas
# Example:
compare_models <- function(data) {
  fit0 <- lm(c66_46_avrsybp ~ c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = data)
  fit1 <- lm(c66_46_avrsybp ~ y1_avg_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean + y1_avg_temp:c66_46_urban_rural, data = data)
  anova(fit0, fit1)
}

# Apply comparison function to each imputed dataset
anova_results <- lapply(imputed_datasets, compare_models)

p_values = c()

# Inspect results
for (i in seq(1,100,1)) {
  pvalue = anova_results[[i]]$`Pr(>F)`[2]
  p_values = c(p_values, pvalue)
  
}

mean(p_values)  # Average p-value across imputations
sum(p_values < 0.05, na.rm = TRUE)

```

100/100 anova tests have p-value less than 0.05 =\> Thus, we conclude that the model with both avg_temp and avg_temp\* sex interaction is significantly better than the model with just sex(p \<.05), or that avg_temp is significantly associated with syst.bp. in a model which contains interaction.

### Standard deviation of average temperature (-)

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrsybp ~ y1_sd_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean +
                    y1_sd_temp:c66_46_urban_rural))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)

#c66_46_q1_ht, c66_46_edu, c66_46_alc, , sex, c66_46_urban_rural 
```

#### Visualizing an interaction

```{r}
# Apply the linear function to each imputed dataset
lm_model_no_inter <- lm(c66_46_avrsybp ~ y1_sd_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = bp_df_plot)

lm_model_inter <- lm(c66_46_avrsybp ~ y1_sd_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean + y1_sd_temp:c66_46_urban_rural, data = bp_df_plot)

# Effect of BMXWAIST at each level of RIAGENDR
# in the model with no interaction
plot_model(lm_model_no_inter,
           type = "eff",
           terms = c("y1_sd_temp", "c66_46_urban_rural"),
           title = "No Interaction",
           axis.title = c("Standard deviation of apparent average temperature (°C)", 
                          "Systolic blood pressure (mmHg)"))

# Effect of BMXWAIST at each level of RIAGENDR
# in the model with an interaction
plot_model(lm_model_inter,
           type = "eff",
           terms = c("y1_sd_temp", "c66_46_urban_rural"),
           title = "Interaction",
           axis.title = c("Standard deviation of apparent average temperature (°C)", 
                          "Systolic blood pressure (mmHg)"))
```

#### Testing the difference between the slopes

From the output, the p-value for the test of interaction is **0.57**. In this case, since the interaction was between a continuous predictor and a binary predictor, the interaction is just one term, so the test can be obtained directly from the regression coefficient table. When the interaction involves a categorical predictor with more than two levels, we should perform `car::Anova(type=3)`.

#### Estimating and testing the significance of the slope at each level of a moderator

```{r}
# Test slope for female
lm_models <- with(pmm_proj_multiimp, 
  lm(c66_46_avrsybp ~ y1_sd_temp +
       c66_46_q1_ht + c66_46_c_bmi_cat +
       relevel(c66_46_urban_rural, ref = "Rural") + 
       c66_46_ht_med + c66_46_edu +  
       c66_46_smok + c66_46_alc +
       c66_46_phys + c66_46_occup + 
       sex + 
       c66_46_diab + c66_46_hpcl + 
       c66_46_season + day_mean +
       y1_sd_temp:relevel(c66_46_urban_rural, ref = "Rural")))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2) 	
```

##### **Conclusion** 

Female(Est = -0.18; p = .56) are NOT statistically significant.

#### Overall test of a predictor involved in an interaction

```{r}
# Get a list of all imputed datasets
imputed_datasets <- mice::complete(pmm_proj_multiimp, "all")

# Replace Y, predictors1, and predictors2 with your formulas
# Example:
compare_models <- function(data) {
  fit0 <- lm(c66_46_avrsybp ~ c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = data)
  fit1 <- lm(c66_46_avrsybp ~ y1_sd_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean + y1_sd_temp:c66_46_urban_rural, data = data)
  anova(fit0, fit1)
}

# Apply comparison function to each imputed dataset
anova_results <- lapply(imputed_datasets, compare_models)

p_values = c()

# Inspect results
for (i in seq(1,100,1)) {
  pvalue = anova_results[[i]]$`Pr(>F)`[2]
  p_values = c(p_values, pvalue)
  
}

mean(p_values)  # Average p-value across imputations
sum(p_values < 0.05, na.rm = TRUE)

```

0/100 anova tests have p-value less than 0.05 =\> Thus, we conclude that the model with both sd_temp and sd_temp \*sex interaction is NOT significantly better than the model with just sex (p \>.05), or that sd_temp is NOT significantly associated with syst.bp. in a model which contains interaction.

## Diastolic blood pressure

### Apparent average temperature

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrdibp ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean +
                    y1_app_temp_avg_46:c66_46_urban_rural))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)

#c66_46_q1_ht, c66_46_edu, c66_46_alc, , sex, c66_46_urban_rural 
```

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrdibp ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean +
                    y1_app_temp_avg_46:c66_46_urban_rural + y1_app_temp_avg_46:c66_46_diab +
                    y1_app_temp_avg_46:c66_46_c_bmi_cat +
                    y1_app_temp_avg_46:c66_46_q1_ht + y1_app_temp_avg_46:sex))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)

#c66_46_q1_ht, c66_46_edu, c66_46_alc, , sex, c66_46_urban_rural 
```


#### Visualizing an interaction


```{r}
library(sjPlot)
library(ggplot2)

# Apply the linear function to each imputed dataset
lm_model_no_inter <- lm(c66_46_avrdibp ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = bp_df_plot)

lm_model_inter <- lm(c66_46_avrdibp ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean +
                    y1_app_temp_avg_46:c66_46_urban_rural + y1_app_temp_avg_46:c66_46_diab +
                    y1_app_temp_avg_46:c66_46_c_bmi_cat +
                    y1_app_temp_avg_46:c66_46_q1_ht + y1_app_temp_avg_46:sex, data = bp_df_plot)


plot_model(
  lm_model_no_inter,
  type = "eff",
  terms = c("y1_app_temp_avg_46", "c66_46_urban_rural"),
  title = "No Interaction term",
  axis.title = c("Annual apparent average temperature (°C)", 
                 "Diastolic blood pressure (mmHg)"),
  pred.labels = c("Mean Apparent Temp (°C)", "Urbanicity"),
  legend.title = "Urbanicity",
  legend.labels = c("Rural", "Urban")
) +
  # Add a minimal theme
  theme_minimal() +
  # Customize some text sizing and alignment
  theme(
    plot.title = element_text(hjust = 0.5, size = 14),
    axis.title = element_text(size = 13),
    axis.text = element_text(size = 11),
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 11)
  )

plot_model(
  lm_model_inter,
  type = "eff",
  terms = c("y1_app_temp_avg_46", "c66_46_urban_rural"),
  title = "ApT and DBP under interaction with urbanicity",
  axis.title = c("Annual apparent average temperature (°C)", 
                 "Diastolic blood pressure (mmHg)"),
  pred.labels = c("Mean Apparent Temp (°C)", "Urbanicity"),
  legend.title = "Urbanicity",
  legend.labels = c("Rural area", "Urban area")
) +
  # Add a minimal theme
  theme_minimal() +
  # Customize some text sizing and alignment
  theme(
    plot.title = element_text(hjust = 0.5, size = 14),
    axis.title = element_text(size = 13),
    axis.text = element_text(size = 12),
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 11)
  )
```

#### Testing the difference between the slopes

From the output, the p-value for the test of interaction is **0.72**. In this case, since the interaction was between a continuous predictor and a binary predictor, the interaction is just one term, so the test can be obtained directly from the regression coefficient table. When the interaction involves a categorical predictor with more than two levels, we should perform `car::Anova(type=3)`.

#### Estimating and testing the significance of the slope at each level of a moderator

```{r}
# Test slope for female
lm_models <- with(pmm_proj_multiimp, 
  lm(c66_46_avrdibp ~ y1_app_temp_avg_46 +
       c66_46_q1_ht + c66_46_c_bmi_cat +
       relevel(c66_46_urban_rural, ref = "Rural") + 
       c66_46_ht_med + c66_46_edu +  
       c66_46_smok + c66_46_alc +
       c66_46_phys + c66_46_occup + 
        sex + 
       c66_46_diab + c66_46_hpcl + 
       c66_46_season + day_mean +
       y1_app_temp_avg_46:relevel(c66_46_urban_rural, ref = "Rural") +
       y1_app_temp_avg_46:c66_46_diab +
       y1_app_temp_avg_46:c66_46_c_bmi_cat +
                    y1_app_temp_avg_46:c66_46_q1_ht + y1_app_temp_avg_46:sex))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)
```

##### **Conclusion** 

The app*\_*avg_temp effects for female(Est = -0.6; 95% p \< 0.01) are statistically significant.

#### Overall test of a predictor involved in an interaction

```{r}
# Get a list of all imputed datasets
imputed_datasets <- mice::complete(pmm_proj_multiimp, "all")

# Replace Y, predictors1, and predictors2 with your formulas
# Example:
compare_models <- function(data) {
  fit0 <- lm(c66_46_avrdibp ~ c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = data)
  fit1 <- lm(c66_46_avrdibp ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean + y1_app_temp_avg_46:c66_46_urban_rural, data = data)
  anova(fit0, fit1)
}

# Apply comparison function to each imputed dataset
anova_results <- lapply(imputed_datasets, compare_models)

p_values = c()

# Inspect results
for (i in seq(1,100,1)) {
  pvalue = anova_results[[i]]$`Pr(>F)`[2]
  p_values = c(p_values, pvalue)
  
}

mean(p_values)  # Average p-value across imputations
sum(p_values < 0.05, na.rm = TRUE)

```

100/100 anova tests have p-value less than 0.05 =\> Thus, we conclude that the model with both app_avg_temp and app_avg_temp \*sex interaction is significantly better than the model with just sex(p \<.05), or that app_avg_temp is significantly associated with diast.bp. in a model which contains interaction.

### Apparent standard deviation of average temperature (-)

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrdibp ~ y1_app_temp_sd_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean +
                    y1_app_temp_sd_46:c66_46_urban_rural))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)

#c66_46_q1_ht, c66_46_edu, c66_46_alc, , sex, c66_46_urban_rural 
```
```{r}

```

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrdibp ~ y1_app_temp_sd_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean +
                    y1_app_temp_sd_46:c66_46_urban_rural + y1_app_temp_sd_46:c66_46_diab +
                    y1_app_temp_sd_46:c66_46_c_bmi_cat +
                    y1_app_temp_sd_46:c66_46_q1_ht + y1_app_temp_sd_46:sex))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)
```


#### Visualizing an interaction


```{r}
#500x400 and 510x400
library(sjPlot)
library(ggplot2)

# Apply the linear function to each imputed dataset
lm_model_no_inter <- lm(c66_46_avrdibp ~ y1_app_temp_sd_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = bp_df_plot)

lm_model_inter <- lm(c66_46_avrdibp ~ y1_app_temp_sd_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean +
                    y1_app_temp_sd_46:c66_46_urban_rural + y1_app_temp_sd_46:c66_46_diab +
                    y1_app_temp_sd_46:c66_46_c_bmi_cat +
                    y1_app_temp_sd_46:c66_46_q1_ht + y1_app_temp_sd_46:sex, data = bp_df_plot)


plot_model(
  lm_model_no_inter,
  type = "eff",
  terms = c("y1_app_temp_sd_46", "c66_46_urban_rural"),
  title = "No Interaction term",
  axis.title = c("Standard deviation of daily apparent temperature within a year", 
                 "Diastolic blood pressure (mmHg)"),
  pred.labels = c("Mean Apparent Temp (°C)", "Urbanicity"),
  legend.title = "Urbanicity",
  legend.labels = c("Rural", "Urban")
) +
  # Add a minimal theme
  theme_minimal() +
  # Customize some text sizing and alignment
  theme(
    plot.title = element_text(hjust = 0.5, size = 14),
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 11),
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 11)
  )

plot_model(
  lm_model_inter,
  type = "eff",
  terms = c("y1_app_temp_sd_46", "c66_46_urban_rural"),
  title = "ATV and DBP under interaction with urbanicity",
  axis.title = c("Standard deviation of the apparent temperature", 
                 "Diastolic blood pressure (mmHg)"),
  pred.labels = c("Mean Apparent Temp (°C)", "Urbanicity"),
  legend.title = "Urbanicity",
  legend.labels = c("Rural area", "Urban area")
) +
  # Add a minimal theme
  theme_minimal() +
  # Customize some text sizing and alignment
  theme(
    plot.title = element_text(hjust = 0.5, size = 14),
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 12),
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 11)
  )
```


#### Testing the difference between the slopes

From the output, the p-value for the test of interaction is **0.53**. In this case, since the interaction was between a continuous predictor and a binary predictor, the interaction is just one term, so the test can be obtained directly from the regression coefficient table. When the interaction involves a categorical predictor with more than two levels, we should perform `car::Anova(type=3)`.

#### Estimating and testing the significance of the slope at each level of a moderator

```{r}
# Test slope for female
lm_models <- with(pmm_proj_multiimp, 
  lm(c66_46_avrdibp ~ y1_app_temp_sd_46 +
       c66_46_q1_ht + c66_46_c_bmi_cat +
       relevel(c66_46_urban_rural, ref = "Rural") + 
       c66_46_ht_med + c66_46_edu +  
       c66_46_smok + c66_46_alc +
       c66_46_phys + c66_46_occup + 
       sex + 
       c66_46_diab + c66_46_hpcl + 
       c66_46_season + day_mean +
       y1_app_temp_sd_46:relevel(c66_46_urban_rural, ref = "Rural") +
       y1_app_temp_sd_46:c66_46_diab +
       y1_app_temp_sd_46:c66_46_c_bmi_cat +
                    y1_app_temp_sd_46:c66_46_q1_ht + y1_app_temp_sd_46:sex))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2) 	
```

#### Overall test of a predictor involved in an interaction

```{r}
# Get a list of all imputed datasets
imputed_datasets <- mice::complete(pmm_proj_multiimp, "all")

# Replace Y, predictors1, and predictors2 with your formulas
# Example:
compare_models <- function(data) {
  fit0 <- lm(c66_46_avrdibp ~ c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = data)
  fit1 <- lm(c66_46_avrdibp ~ y1_app_temp_sd_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean + y1_app_temp_sd_46:c66_46_urban_rural, data = data)
  anova(fit0, fit1)
}

# Apply comparison function to each imputed dataset
anova_results <- lapply(imputed_datasets, compare_models)

p_values = c()

# Inspect results
for (i in seq(1,100,1)) {
  pvalue = anova_results[[i]]$`Pr(>F)`[2]
  p_values = c(p_values, pvalue)
  
}

mean(p_values)  # Average p-value across imputations
sum(p_values < 0.05, na.rm = TRUE)

```

0/100 anova tests have p-value less than 0.05 =\> Thus, we conclude that the model with both app_sd_temp and app_sd_temp\* sex interaction is NOT significantly better than the model with just sex(p \>.05), or that app_sd_temp is NOT significantly associated with diast.bp. in a model which contains interaction.

### Average temperature

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrdibp ~ y1_avg_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean +
                    y1_avg_temp:c66_46_urban_rural))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)

#c66_46_q1_ht, c66_46_edu, c66_46_alc, , sex, c66_46_urban_rural 
```

#### Visualizing an interaction

```{r}
# Apply the linear function to each imputed dataset
lm_model_no_inter <- lm(c66_46_avrdibp ~ y1_avg_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = bp_df_plot)

lm_model_inter <- lm(c66_46_avrdibp ~ y1_avg_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean + y1_avg_temp:c66_46_urban_rural, data = bp_df_plot)

# Effect of BMXWAIST at each level of RIAGENDR
# in the model with no interaction
plot_model(lm_model_no_inter,
           type = "eff",
           terms = c("y1_avg_temp", "c66_46_urban_rural"),
           title = "No Interaction",
           axis.title = c("Apparent average temperature (°C)", 
                          "diastolic blood pressure (mmHg)"))

# Effect of BMXWAIST at each level of RIAGENDR
# in the model with an interaction
plot_model(lm_model_inter,
           type = "eff",
           terms = c("y1_avg_temp", "c66_46_urban_rural"),
           title = "Interaction",
           axis.title = c("Apparent average temperature (°C)", 
                          "diastolic blood pressure (mmHg)"))
```

#### Testing the difference between the slopes

From the output, the p-value for the test of interaction is **0.6**. In this case, since the interaction was between a continuous predictor and a binary predictor, the interaction is just one term, so the test can be obtained directly from the regression coefficient table. When the interaction involves a categorical predictor with more than two levels, we should perform `car::Anova(type=3)`.

#### Estimating and testing the significance of the slope at each level of a moderator

```{r}
# Test slope for female
lm_models <- with(pmm_proj_multiimp, 
  lm(c66_46_avrdibp ~ y1_avg_temp +
       c66_46_q1_ht + c66_46_c_bmi_cat +
       relevel(c66_46_urban_rural, ref = "Rural") + 
       c66_46_ht_med + c66_46_edu +  
       c66_46_smok + c66_46_alc +
       c66_46_phys + c66_46_occup + 
        + sex + 
       c66_46_diab + c66_46_hpcl + 
       c66_46_season + day_mean +
       y1_avg_temp:relevel(c66_46_urban_rural, ref = "Rural")))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)
```

##### **Conclusion** 

The app*\_*avg_temp effects for female (Est = -0.64; p \< .01) are statistically significant.

#### Overall test of a predictor involved in an interaction

```{r}
# Get a list of all imputed datasets
imputed_datasets <- mice::complete(pmm_proj_multiimp, "all")

# Replace Y, predictors1, and predictors2 with your formulas
# Example:
compare_models <- function(data) {
  fit0 <- lm(c66_46_avrdibp ~ c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = data)
  fit1 <- lm(c66_46_avrdibp ~ y1_avg_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean + y1_avg_temp:c66_46_urban_rural, data = data)
  anova(fit0, fit1)
}

# Apply comparison function to each imputed dataset
anova_results <- lapply(imputed_datasets, compare_models)

p_values = c()

# Inspect results
for (i in seq(1,100,1)) {
  pvalue = anova_results[[i]]$`Pr(>F)`[2]
  p_values = c(p_values, pvalue)
  
}

mean(p_values)  # Average p-value across imputations
sum(p_values < 0.05, na.rm = TRUE)

```

100/100 anova tests have p-value less than 0.05 =\> Thus, we conclude that the model with both avg_temp and avg_temp\* sex interaction is significantly better than the model with just sex (p \<.05), or that avg_temp is significantly associated with diast.bp. in a model which contains interaction.

### Standard deviation of average temperature (-)

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrdibp ~ y1_sd_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean +
                    y1_sd_temp:c66_46_urban_rural))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)

#c66_46_q1_ht, c66_46_edu, c66_46_alc, , sex, c66_46_urban_rural 
```

#### Visualizing an interaction

```{r}
# Apply the linear function to each imputed dataset
lm_model_no_inter <- lm(c66_46_avrdibp ~ y1_sd_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = bp_df_plot)

lm_model_inter <- lm(c66_46_avrdibp ~ y1_sd_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean + y1_sd_temp:c66_46_urban_rural, data = bp_df_plot)

# Effect of BMXWAIST at each level of RIAGENDR
# in the model with no interaction
plot_model(lm_model_no_inter,
           type = "eff",
           terms = c("y1_sd_temp", "c66_46_urban_rural"),
           title = "No Interaction",
           axis.title = c("Standard deviation of average temperature (°C)", 
                          "diastolic blood pressure (mmHg)"))

# Effect of BMXWAIST at each level of RIAGENDR
# in the model with an interaction
plot_model(lm_model_inter,
           type = "eff",
           terms = c("y1_sd_temp", "c66_46_urban_rural"),
           title = "Interaction",
           axis.title = c("Standard deviation of average temperature (°C)", 
                          "diastolic blood pressure (mmHg)"))
```

#### Testing the difference between the slopes

From the output, the p-value for the test of interaction is **0.44**. In this case, since the interaction was between a continuous predictor and a binary predictor, the interaction is just one term, so the test can be obtained directly from the regression coefficient table. When the interaction involves a categorical predictor with more than two levels, we should perform `car::Anova(type=3)`.

#### Estimating and testing the significance of the slope at each level of a moderator

```{r}
# Test slope for female
lm_models <- with(pmm_proj_multiimp, 
  lm(c66_46_avrdibp ~ y1_sd_temp +
       c66_46_q1_ht + c66_46_c_bmi_cat +
       relevel(c66_46_urban_rural, ref = "Rural") + 
       c66_46_ht_med + c66_46_edu +  
       c66_46_smok + c66_46_alc +
       c66_46_phys + c66_46_occup + 
       sex + 
       c66_46_diab + c66_46_hpcl + 
       c66_46_season + day_mean +
       y1_sd_temp:relevel(c66_46_urban_rural, ref = "Rural")))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2) 	
```

##### **Conclusion** 

The sd_temp effects for female(Est = 0.23, p = .28) are NOT statistically significant.

#### Overall test of a predictor involved in an interaction

```{r}
# Get a list of all imputed datasets
imputed_datasets <- mice::complete(pmm_proj_multiimp, "all")

# Replace Y, predictors1, and predictors2 with your formulas
# Example:
compare_models <- function(data) {
  fit0 <- lm(c66_46_avrdibp ~ c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = data)
  fit1 <- lm(c66_46_avrdibp ~ y1_sd_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean + y1_sd_temp:c66_46_urban_rural, data = data)
  anova(fit0, fit1)
}

# Apply comparison function to each imputed dataset
anova_results <- lapply(imputed_datasets, compare_models)

p_values = c()

# Inspect results
for (i in seq(1,100,1)) {
  pvalue = anova_results[[i]]$`Pr(>F)`[2]
  p_values = c(p_values, pvalue)
  
}

mean(p_values)  # Average p-value across imputations
sum(p_values < 0.05, na.rm = TRUE)

```

25/100 anova tests have p-value less than 0.05 =\> Thus, we conclude that the model with both sd_temp and sd_temp sex interaction is significantly better than the model with just sex(p \>.05), or that sd_temp is significantly associated with diast.bp. in a model which contains interaction.

# Hypertension

## Systolic blood pressure

### Apparent average temperature

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrsybp ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean +
                    y1_app_temp_avg_46:c66_46_q1_ht))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)

#c66_46_q1_ht, c66_46_edu, c66_46_alc, , sex, c66_46_urban_rural 
```

#### Visualizing an interaction

```{r}
# Apply the linear function to each imputed dataset
lm_model_no_inter <- lm(c66_46_avrsybp ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = bp_df_plot)

lm_model_inter <- lm(c66_46_avrsybp ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean + y1_app_temp_avg_46:c66_46_q1_ht, data = bp_df_plot)

# Effect of BMXWAIST at each level of RIAGENDR
# in the model with no interaction
plot_model(lm_model_no_inter,
           type = "eff",
           terms = c("y1_app_temp_avg_46", "c66_46_ht_med"),
           title = "No Interaction",
           axis.title = c("Apparent average temperature (°C)", 
                          "Systolic blood pressure (mmHg)"))

# Effect of BMXWAIST at each level of RIAGENDR
# in the model with an interaction
plot_model(lm_model_inter,
           type = "eff",
           terms = c("y1_app_temp_avg_46", "c66_46_ht_med"),
           title = "Interaction",
           axis.title = c("Apparent average temperature (°C)", 
                          "Systolic blood pressure (mmHg)"))
```

#### Testing the difference between the slopes

In the right panel, is the difference between the male and female slopes statistically significant? This is equivalent to asking, “Are the lines in the plot significantly different from parallel?” The p-value for the regression coefficient for the interaction tests the null hypothesis H0:β3=0. Under the null hypothesis, the lines are parallel. Thus, to test the difference between the slopes, examine the interaction p-value.

From the output, the p-value for the test of interaction is 0.3. In this case, since the interaction was between a continuous predictor and a binary predictor, the interaction is just one term, so the test can be obtained directly from the regression coefficient table. When the interaction involves a categorical predictor with more than two levels, we should perform `car::Anova(type=3)`.

Since p \> .05, we can make the following two inferences:

-   There is a non-significant difference in the app*\_*avg_temp effect on syst.bp between male and female (the lines are not significantly different from parallel).

-   There is a non significant difference in the sex effect on syst.bp between those with differing app*\_*avg_temp (the vertical distance between the lines does not depend on how large or small app*\_*avg_temp is).

-   Effect of app*\_*avg_temp on syst.bp does not depend on sex.

#### Estimating and testing the significance of the slope at each level of a moderator

```{r}
# Test slope for female
lm_models <- with(pmm_proj_multiimp, 
  lm(c66_46_avrsybp ~ y1_app_temp_avg_46 +
       c66_46_q1_ht + c66_46_c_bmi_cat +
       relevel(c66_46_ht_med, ref = "Yes") + 
       c66_46_urban_rural + c66_46_edu +  
       c66_46_smok + c66_46_alc +
       c66_46_phys + c66_46_occup + 
        + sex + 
       c66_46_diab + c66_46_hpcl + 
       c66_46_season + day_mean +
       y1_app_temp_avg_46:relevel(c66_46_ht_med, ref = "Yes")))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)
```

##### **Conclusion** 

Female(Est = -0.71; 95% p = \<01) is statistically significant.

#### Overall test of a predictor involved in an interaction

```{r}
# Get a list of all imputed datasets
imputed_datasets <- mice::complete(pmm_proj_multiimp, "all")

# Replace Y, predictors1, and predictors2 with your formulas
# Example:
compare_models <- function(data) {
  fit0 <- lm(c66_46_avrsybp ~ c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = data)
  fit1 <- lm(c66_46_avrsybp ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean + y1_app_temp_avg_46:c66_46_q1_ht, data = data)
  anova(fit0, fit1)
}

# Apply comparison function to each imputed dataset
anova_results <- lapply(imputed_datasets, compare_models)

p_values = c()

# Inspect results
for (i in seq(1,100,1)) {
  pvalue = anova_results[[i]]$`Pr(>F)`[2]
  p_values = c(p_values, pvalue)
  
}

mean(p_values)  # Average p-value across imputations
sum(p_values < 0.05, na.rm = TRUE)

```

100/100 anova tests have p-value less than 0.05 =\> Thus, we conclude that the model with both app_avg_temp and app_avg_temp sex interaction is significantly better than the model with just bmi_cat(p \<.05), or that app_avg_temp is significantly associated with syst.bp. in a model which contains interaction.

### Apparent standard deviation of average temperature (-)

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrsybp ~ y1_app_temp_sd_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean +
                    y1_app_temp_sd_46:c66_46_q1_ht))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)

#c66_46_q1_ht, c66_46_edu, c66_46_alc, , sex, c66_46_urban_rural 
```

#### Visualizing an interaction

```{r}
# Apply the linear function to each imputed dataset
lm_model_no_inter <- lm(c66_46_avrsybp ~ y1_app_temp_sd_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = bp_df_plot)

lm_model_inter <- lm(c66_46_avrsybp ~ y1_app_temp_sd_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean + y1_app_temp_sd_46:c66_46_q1_ht, data = bp_df_plot)

# Effect of BMXWAIST at each level of RIAGENDR
# in the model with no interaction
plot_model(lm_model_no_inter,
           type = "eff",
           terms = c("y1_app_temp_sd_46", "c66_46_ht_med"),
           title = "No Interaction",
           axis.title = c("Standard deviation of apparent average temperature (°C)", 
                          "Systolic blood pressure (mmHg)"))

# Effect of BMXWAIST at each level of RIAGENDR
# in the model with an interaction
plot_model(lm_model_inter,
           type = "eff",
           terms = c("y1_app_temp_sd_46", "c66_46_ht_med"),
           title = "Interaction",
           axis.title = c("Standard deviation of apparent average temperature (°C)", 
                          "Systolic blood pressure (mmHg)"))
```

#### Testing the difference between the slopes

In the right panel, is the difference between the male and female slopes statistically significant? This is equivalent to asking, “Are the lines in the plot significantly different from parallel?” The p-value for the regression coefficient for the interaction tests the null hypothesis H0:βinter=0. Under the null hypothesis, the lines are parallel. Thus, to test the difference between the slopes, examine the interaction p-value.

From the output, the p-value for the test of interaction is **0.52**. In this case, since the interaction was between a continuous predictor and a binary predictor, the interaction is just one term, so the test can be obtained directly from the regression coefficient table. When the interaction involves a categorical predictor with more than two levels, we should perform `car::Anova(type=3)`.

Since p \> .05, we can make the following two inferences:

-   There is a non-significant difference in the app*\_*sd_temp effect on syst.bp between male and female (the lines are not significantly different from parallel).

-   There is a non significant difference in the sex effect on syst.bp between those with differing app*\_*sd_temp (the vertical distance between the lines does not depend on how large or small app*\_*sd_temp is).

-   Effect of app*\_*sd_temp on syst.bp does not depend on sex.

#### Estimating and testing the significance of the slope at each level of a moderator

```{r}
# Test slope for female
lm_models <- with(pmm_proj_multiimp, 
  lm(c66_46_avrsybp ~ y1_app_temp_sd_46 +
       c66_46_q1_ht + c66_46_c_bmi_cat +
       relevel(c66_46_ht_med, ref = "Yes") + 
       c66_46_urban_rural + c66_46_edu +  
       c66_46_smok + c66_46_alc +
       c66_46_phys + c66_46_occup + 
       sex + 
       c66_46_diab + c66_46_hpcl + 
       c66_46_season + day_mean +
       y1_app_temp_sd_46:relevel(c66_46_ht_med, ref = "Yes")))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2) 	
```

##### **Conclusion** 

Female(Est = -0.13; p = .67) are NOT statistically significant.

#### Overall test of a predictor involved in an interaction

```{r}
# Get a list of all imputed datasets
imputed_datasets <- mice::complete(pmm_proj_multiimp, "all")

# Replace Y, predictors1, and predictors2 with your formulas
# Example:
compare_models <- function(data) {
  fit0 <- lm(c66_46_avrsybp ~ c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = data)
  fit1 <- lm(c66_46_avrsybp ~ y1_app_temp_sd_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean + y1_app_temp_sd_46:c66_46_q1_ht, data = data)
  anova(fit0, fit1)
}

# Apply comparison function to each imputed dataset
anova_results <- lapply(imputed_datasets, compare_models)

p_values = c()

# Inspect results
for (i in seq(1,100,1)) {
  pvalue = anova_results[[i]]$`Pr(>F)`[2]
  p_values = c(p_values, pvalue)
  
}

mean(p_values)  # Average p-value across imputations
sum(p_values < 0.05, na.rm = TRUE)

```

0/100 anova tests have p-value less than 0.05 =\> Thus, we conclude that the model with both app_sd_temp and app_sd_temp \*sex interaction is NOT significantly better than the model with just sex(p \>.05), or that app_sd_temp is NOT significantly associated with syst.bp. in a model which contains interaction.

### Average temperature

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrsybp ~ y1_avg_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean +
                    y1_avg_temp:c66_46_ht_med))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)

#c66_46_q1_ht, c66_46_edu, c66_46_alc, , sex, c66_46_urban_rural 
```

#### Visualizing an interaction

```{r}
# Apply the linear function to each imputed dataset
lm_model_no_inter <- lm(c66_46_avrsybp ~ y1_avg_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = bp_df_plot)

lm_model_inter <- lm(c66_46_avrsybp ~ y1_avg_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean + y1_avg_temp:c66_46_ht_med, data = bp_df_plot)

# Effect of BMXWAIST at each level of RIAGENDR
# in the model with no interaction
plot_model(lm_model_no_inter,
           type = "eff",
           terms = c("y1_avg_temp", "c66_46_ht_med"),
           title = "No Interaction",
           axis.title = c("average temperature (°C)", 
                          "Systolic blood pressure (mmHg)"))

# Effect of BMXWAIST at each level of RIAGENDR
# in the model with an interaction
plot_model(lm_model_inter,
           type = "eff",
           terms = c("y1_avg_temp", "c66_46_ht_med"),
           title = "Interaction",
           axis.title = c("average temperature (°C)", 
                          "Systolic blood pressure (mmHg)"))
```

#### Testing the difference between the slopes

From the output, the p-value for the test of interaction is **0.33**. In this case, since the interaction was between a continuous predictor and a binary predictor, the interaction is just one term, so the test can be obtained directly from the regression coefficient table. When the interaction involves a categorical predictor with more than two levels, we should perform `car::Anova(type=3)`.

#### Estimating and testing the significance of the slope at each level of a moderator

```{r}
# Test slope for female
lm_models <- with(pmm_proj_multiimp, 
  lm(c66_46_avrsybp ~ y1_avg_temp +
       c66_46_q1_ht + c66_46_c_bmi_cat +
       relevel(c66_46_ht_med, ref = "Yes") + 
       c66_46_urban_rural + c66_46_edu +  
       c66_46_smok + c66_46_alc +
       c66_46_phys + c66_46_occup + 
        + sex + 
       c66_46_diab + c66_46_hpcl + 
       c66_46_season + day_mean +
       y1_avg_temp:relevel(c66_46_ht_med, ref = "Yes")))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)
```

##### **Conclusion** 

Female (Est = -0.73; p \< .00) is statistically significant.

#### Overall test of a predictor involved in an interaction

```{r}
# Get a list of all imputed datasets
imputed_datasets <- mice::complete(pmm_proj_multiimp, "all")

# Replace Y, predictors1, and predictors2 with your formulas
# Example:
compare_models <- function(data) {
  fit0 <- lm(c66_46_avrsybp ~ c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = data)
  fit1 <- lm(c66_46_avrsybp ~ y1_avg_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean + y1_avg_temp:c66_46_ht_med, data = data)
  anova(fit0, fit1)
}

# Apply comparison function to each imputed dataset
anova_results <- lapply(imputed_datasets, compare_models)

p_values = c()

# Inspect results
for (i in seq(1,100,1)) {
  pvalue = anova_results[[i]]$`Pr(>F)`[2]
  p_values = c(p_values, pvalue)
  
}

mean(p_values)  # Average p-value across imputations
sum(p_values < 0.05, na.rm = TRUE)

```

100/100 anova tests have p-value less than 0.05 =\> Thus, we conclude that the model with both avg_temp and avg_temp\* sex interaction is significantly better than the model with just sex(p \<.05), or that avg_temp is significantly associated with syst.bp. in a model which contains interaction.

### Standard deviation of average temperature (-)

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrsybp ~ y1_sd_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean +
                    y1_sd_temp:c66_46_ht_med))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)

#c66_46_q1_ht, c66_46_edu, c66_46_alc, , sex, c66_46_urban_rural 
```

#### Visualizing an interaction

```{r}
# Apply the linear function to each imputed dataset
lm_model_no_inter <- lm(c66_46_avrsybp ~ y1_sd_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = bp_df_plot)

lm_model_inter <- lm(c66_46_avrsybp ~ y1_sd_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean + y1_sd_temp:c66_46_ht_med, data = bp_df_plot)

# Effect of BMXWAIST at each level of RIAGENDR
# in the model with no interaction
plot_model(lm_model_no_inter,
           type = "eff",
           terms = c("y1_sd_temp", "c66_46_ht_med"),
           title = "No Interaction",
           axis.title = c("Standard deviation of apparent average temperature (°C)", 
                          "Systolic blood pressure (mmHg)"))

# Effect of BMXWAIST at each level of RIAGENDR
# in the model with an interaction
plot_model(lm_model_inter,
           type = "eff",
           terms = c("y1_sd_temp", "c66_46_ht_med"),
           title = "Interaction",
           axis.title = c("Standard deviation of apparent average temperature (°C)", 
                          "Systolic blood pressure (mmHg)"))
```

#### Testing the difference between the slopes

From the output, the p-value for the test of interaction is **0.57**. In this case, since the interaction was between a continuous predictor and a binary predictor, the interaction is just one term, so the test can be obtained directly from the regression coefficient table. When the interaction involves a categorical predictor with more than two levels, we should perform `car::Anova(type=3)`.

#### Estimating and testing the significance of the slope at each level of a moderator

```{r}
# Test slope for female
lm_models <- with(pmm_proj_multiimp, 
  lm(c66_46_avrsybp ~ y1_sd_temp +
       c66_46_q1_ht + c66_46_c_bmi_cat +
       relevel(c66_46_ht_med, ref = "Yes") + 
       c66_46_urban_rural + c66_46_edu +  
       c66_46_smok + c66_46_alc +
       c66_46_phys + c66_46_occup + 
       sex + 
       c66_46_diab + c66_46_hpcl + 
       c66_46_season + day_mean +
       y1_sd_temp:relevel(c66_46_ht_med, ref = "Yes")))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2) 	
```

##### **Conclusion** 

Female(Est = -0.18; p = .56) are NOT statistically significant.

#### Overall test of a predictor involved in an interaction

```{r}
# Get a list of all imputed datasets
imputed_datasets <- mice::complete(pmm_proj_multiimp, "all")

# Replace Y, predictors1, and predictors2 with your formulas
# Example:
compare_models <- function(data) {
  fit0 <- lm(c66_46_avrsybp ~ c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = data)
  fit1 <- lm(c66_46_avrsybp ~ y1_sd_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean + y1_sd_temp:c66_46_ht_med, data = data)
  anova(fit0, fit1)
}

# Apply comparison function to each imputed dataset
anova_results <- lapply(imputed_datasets, compare_models)

p_values = c()

# Inspect results
for (i in seq(1,100,1)) {
  pvalue = anova_results[[i]]$`Pr(>F)`[2]
  p_values = c(p_values, pvalue)
  
}

mean(p_values)  # Average p-value across imputations
sum(p_values < 0.05, na.rm = TRUE)

```

0/100 anova tests have p-value less than 0.05 =\> Thus, we conclude that the model with both sd_temp and sd_temp \*sex interaction is NOT significantly better than the model with just sex (p \>.05), or that sd_temp is NOT significantly associated with syst.bp. in a model which contains interaction.

## Diastolic blood pressure

### Apparent average temperature

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrdibp ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean +
                    y1_app_temp_avg_46:c66_46_q1_ht))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)

#c66_46_q1_ht, c66_46_edu, c66_46_alc, , sex, c66_46_urban_rural 
```

#### Visualizing an interaction

```{r}
# Apply the linear function to each imputed dataset
lm_model_no_inter <- lm(c66_46_avrdibp ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = bp_df_plot)

lm_model_inter <- lm(c66_46_avrdibp ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean + y1_app_temp_avg_46:c66_46_q1_ht, data = bp_df_plot)

# Effect of BMXWAIST at each level of RIAGENDR
# in the model with no interaction
plot_model(lm_model_no_inter,
           type = "eff",
           terms = c("y1_app_temp_avg_46", "c66_46_ht_med"),
           title = "No Interaction",
           axis.title = c("Apparent average temperature (°C)", 
                          "diastolic blood pressure (mmHg)"))

# Effect of BMXWAIST at each level of RIAGENDR
# in the model with an interaction
plot_model(lm_model_inter,
           type = "eff",
           terms = c("y1_app_temp_avg_46", "c66_46_ht_med"),
           title = "Interaction",
           axis.title = c("Apparent average temperature (°C)", 
                          "diastolic blood pressure (mmHg)"))
```

#### Testing the difference between the slopes

From the output, the p-value for the test of interaction is **0.72**. In this case, since the interaction was between a continuous predictor and a binary predictor, the interaction is just one term, so the test can be obtained directly from the regression coefficient table. When the interaction involves a categorical predictor with more than two levels, we should perform `car::Anova(type=3)`.

#### Estimating and testing the significance of the slope at each level of a moderator

```{r}
# Test slope for female
lm_models <- with(pmm_proj_multiimp, 
  lm(c66_46_avrdibp ~ y1_app_temp_avg_46 +
       c66_46_q1_ht + c66_46_c_bmi_cat +
       relevel(c66_46_ht_med, ref = "Yes") + 
       c66_46_urban_rural + c66_46_edu +  
       c66_46_smok + c66_46_alc +
       c66_46_phys + c66_46_occup + 
        + sex + 
       c66_46_diab + c66_46_hpcl + 
       c66_46_season + day_mean +
       y1_app_temp_avg_46:relevel(c66_46_ht_med, ref = "Yes")))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)
```

##### **Conclusion** 

The app*\_*avg_temp effects for female(Est = -0.6; 95% p \< 0.01) are statistically significant.

#### Overall test of a predictor involved in an interaction

```{r}
# Get a list of all imputed datasets
imputed_datasets <- mice::complete(pmm_proj_multiimp, "all")

# Replace Y, predictors1, and predictors2 with your formulas
# Example:
compare_models <- function(data) {
  fit0 <- lm(c66_46_avrdibp ~ c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = data)
  fit1 <- lm(c66_46_avrdibp ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean + y1_app_temp_avg_46:c66_46_q1_ht, data = data)
  anova(fit0, fit1)
}

# Apply comparison function to each imputed dataset
anova_results <- lapply(imputed_datasets, compare_models)

p_values = c()

# Inspect results
for (i in seq(1,100,1)) {
  pvalue = anova_results[[i]]$`Pr(>F)`[2]
  p_values = c(p_values, pvalue)
  
}

mean(p_values)  # Average p-value across imputations
sum(p_values < 0.05, na.rm = TRUE)

```

100/100 anova tests have p-value less than 0.05 =\> Thus, we conclude that the model with both app_avg_temp and app_avg_temp \*sex interaction is significantly better than the model with just sex(p \<.05), or that app_avg_temp is significantly associated with diast.bp. in a model which contains interaction.

### Apparent standard deviation of average temperature (-)

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrdibp ~ y1_app_temp_sd_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean +
                    y1_app_temp_sd_46:c66_46_q1_ht))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)

#c66_46_q1_ht, c66_46_edu, c66_46_alc, , sex, c66_46_urban_rural 
```

#### Visualizing an interaction

```{r}
# Apply the linear function to each imputed dataset
lm_model_no_inter <- lm(c66_46_avrdibp ~ y1_app_temp_sd_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = bp_df_plot)

lm_model_inter <- lm(c66_46_avrdibp ~ y1_app_temp_sd_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean + y1_app_temp_sd_46:c66_46_q1_ht, data = bp_df_plot)

# Effect of BMXWAIST at each level of RIAGENDR
# in the model with no interaction
plot_model(lm_model_no_inter,
           type = "eff",
           terms = c("y1_app_temp_sd_46", "c66_46_ht_med"),
           title = "No Interaction",
           axis.title = c("Standard deviation of apparent average temperature (°C)", 
                          "diastolic blood pressure (mmHg)"))

# Effect of BMXWAIST at each level of RIAGENDR
# in the model with an interaction
plot_model(lm_model_inter,
           type = "eff",
           terms = c("y1_app_temp_sd_46", "c66_46_ht_med"),
           title = "Interaction",
           axis.title = c("Standard deviation of apparent average temperature (°C)", 
                          "diastolic blood pressure (mmHg)"))
```

#### Testing the difference between the slopes

From the output, the p-value for the test of interaction is **0.53**. In this case, since the interaction was between a continuous predictor and a binary predictor, the interaction is just one term, so the test can be obtained directly from the regression coefficient table. When the interaction involves a categorical predictor with more than two levels, we should perform `car::Anova(type=3)`.

#### Estimating and testing the significance of the slope at each level of a moderator

```{r}
# Test slope for female
lm_models <- with(pmm_proj_multiimp, 
  lm(c66_46_avrdibp ~ y1_app_temp_sd_46 +
       c66_46_q1_ht + c66_46_c_bmi_cat +
       relevel(c66_46_ht_med, ref = "Yes") + 
       c66_46_urban_rural + c66_46_edu +  
       c66_46_smok + c66_46_alc +
       c66_46_phys + c66_46_occup + 
       sex + 
       c66_46_diab + c66_46_hpcl + 
       c66_46_season + day_mean +
       y1_app_temp_sd_46:relevel(c66_46_ht_med, ref = "Yes")))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2) 	
```

#### Overall test of a predictor involved in an interaction

```{r}
# Get a list of all imputed datasets
imputed_datasets <- mice::complete(pmm_proj_multiimp, "all")

# Replace Y, predictors1, and predictors2 with your formulas
# Example:
compare_models <- function(data) {
  fit0 <- lm(c66_46_avrdibp ~ c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = data)
  fit1 <- lm(c66_46_avrdibp ~ y1_app_temp_sd_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean + y1_app_temp_sd_46:c66_46_q1_ht, data = data)
  anova(fit0, fit1)
}

# Apply comparison function to each imputed dataset
anova_results <- lapply(imputed_datasets, compare_models)

p_values = c()

# Inspect results
for (i in seq(1,100,1)) {
  pvalue = anova_results[[i]]$`Pr(>F)`[2]
  p_values = c(p_values, pvalue)
  
}

mean(p_values)  # Average p-value across imputations
sum(p_values < 0.05, na.rm = TRUE)

```

0/100 anova tests have p-value less than 0.05 =\> Thus, we conclude that the model with both app_sd_temp and app_sd_temp\* sex interaction is NOT significantly better than the model with just sex(p \>.05), or that app_sd_temp is NOT significantly associated with diast.bp. in a model which contains interaction.

### Average temperature

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrdibp ~ y1_avg_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean +
                    y1_avg_temp:c66_46_ht_med))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)

#c66_46_q1_ht, c66_46_edu, c66_46_alc, , sex, c66_46_urban_rural 
```

#### Visualizing an interaction

```{r}
# Apply the linear function to each imputed dataset
lm_model_no_inter <- lm(c66_46_avrdibp ~ y1_avg_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = bp_df_plot)

lm_model_inter <- lm(c66_46_avrdibp ~ y1_avg_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean + y1_avg_temp:c66_46_ht_med, data = bp_df_plot)

# Effect of BMXWAIST at each level of RIAGENDR
# in the model with no interaction
plot_model(lm_model_no_inter,
           type = "eff",
           terms = c("y1_avg_temp", "c66_46_ht_med"),
           title = "No Interaction",
           axis.title = c("Apparent average temperature (°C)", 
                          "diastolic blood pressure (mmHg)"))

# Effect of BMXWAIST at each level of RIAGENDR
# in the model with an interaction
plot_model(lm_model_inter,
           type = "eff",
           terms = c("y1_avg_temp", "c66_46_ht_med"),
           title = "Interaction",
           axis.title = c("Apparent average temperature (°C)", 
                          "diastolic blood pressure (mmHg)"))
```

#### Testing the difference between the slopes

From the output, the p-value for the test of interaction is **0.6**. In this case, since the interaction was between a continuous predictor and a binary predictor, the interaction is just one term, so the test can be obtained directly from the regression coefficient table. When the interaction involves a categorical predictor with more than two levels, we should perform `car::Anova(type=3)`.

#### Estimating and testing the significance of the slope at each level of a moderator

```{r}
# Test slope for female
lm_models <- with(pmm_proj_multiimp, 
  lm(c66_46_avrdibp ~ y1_avg_temp +
       c66_46_q1_ht + c66_46_c_bmi_cat +
       relevel(c66_46_ht_med, ref = "Yes") + 
       c66_46_urban_rural + c66_46_edu +  
       c66_46_smok + c66_46_alc +
       c66_46_phys + c66_46_occup + 
        + sex + 
       c66_46_diab + c66_46_hpcl + 
       c66_46_season + day_mean +
       y1_avg_temp:relevel(c66_46_ht_med, ref = "Yes")))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)
```

##### **Conclusion** 

The app*\_*avg_temp effects for female (Est = -0.64; p \< .01) are statistically significant.

#### Overall test of a predictor involved in an interaction

```{r}
# Get a list of all imputed datasets
imputed_datasets <- mice::complete(pmm_proj_multiimp, "all")

# Replace Y, predictors1, and predictors2 with your formulas
# Example:
compare_models <- function(data) {
  fit0 <- lm(c66_46_avrdibp ~ c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = data)
  fit1 <- lm(c66_46_avrdibp ~ y1_avg_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean + y1_avg_temp:c66_46_ht_med, data = data)
  anova(fit0, fit1)
}

# Apply comparison function to each imputed dataset
anova_results <- lapply(imputed_datasets, compare_models)

p_values = c()

# Inspect results
for (i in seq(1,100,1)) {
  pvalue = anova_results[[i]]$`Pr(>F)`[2]
  p_values = c(p_values, pvalue)
  
}

mean(p_values)  # Average p-value across imputations
sum(p_values < 0.05, na.rm = TRUE)

```

100/100 anova tests have p-value less than 0.05 =\> Thus, we conclude that the model with both avg_temp and avg_temp\* sex interaction is significantly better than the model with just sex (p \<.05), or that avg_temp is significantly associated with diast.bp. in a model which contains interaction.

### Standard deviation of average temperature (-)

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrdibp ~ y1_sd_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean +
                    y1_sd_temp:c66_46_ht_med))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)

#c66_46_q1_ht, c66_46_edu, c66_46_alc, , sex, c66_46_urban_rural 
```

#### Visualizing an interaction

```{r}
# Apply the linear function to each imputed dataset
lm_model_no_inter <- lm(c66_46_avrdibp ~ y1_sd_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = bp_df_plot)

lm_model_inter <- lm(c66_46_avrdibp ~ y1_sd_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean + y1_sd_temp:c66_46_ht_med, data = bp_df_plot)

# Effect of BMXWAIST at each level of RIAGENDR
# in the model with no interaction
plot_model(lm_model_no_inter,
           type = "eff",
           terms = c("y1_sd_temp", "c66_46_ht_med"),
           title = "No Interaction",
           axis.title = c("Standard deviation of average temperature (°C)", 
                          "diastolic blood pressure (mmHg)"))

# Effect of BMXWAIST at each level of RIAGENDR
# in the model with an interaction
plot_model(lm_model_inter,
           type = "eff",
           terms = c("y1_sd_temp", "c66_46_ht_med"),
           title = "Interaction",
           axis.title = c("Standard deviation of average temperature (°C)", 
                          "diastolic blood pressure (mmHg)"))
```

#### Testing the difference between the slopes

From the output, the p-value for the test of interaction is **0.44**. In this case, since the interaction was between a continuous predictor and a binary predictor, the interaction is just one term, so the test can be obtained directly from the regression coefficient table. When the interaction involves a categorical predictor with more than two levels, we should perform `car::Anova(type=3)`.

#### Estimating and testing the significance of the slope at each level of a moderator

```{r}
# Test slope for female
lm_models <- with(pmm_proj_multiimp, 
  lm(c66_46_avrdibp ~ y1_sd_temp +
       c66_46_q1_ht + c66_46_c_bmi_cat +
       relevel(c66_46_ht_med, ref = "Yes") + 
       c66_46_urban_rural + c66_46_edu +  
       c66_46_smok + c66_46_alc +
       c66_46_phys + c66_46_occup + 
       sex + 
       c66_46_diab + c66_46_hpcl + 
       c66_46_season + day_mean +
       y1_sd_temp:relevel(c66_46_ht_med, ref = "Yes")))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2) 	
```

##### **Conclusion** 

The sd_temp effects for female(Est = 0.23, p = .28) are NOT statistically significant.

#### Overall test of a predictor involved in an interaction

```{r}
# Get a list of all imputed datasets
imputed_datasets <- mice::complete(pmm_proj_multiimp, "all")

# Replace Y, predictors1, and predictors2 with your formulas
# Example:
compare_models <- function(data) {
  fit0 <- lm(c66_46_avrdibp ~ c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = data)
  fit1 <- lm(c66_46_avrdibp ~ y1_sd_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean + y1_sd_temp:c66_46_ht_med, data = data)
  anova(fit0, fit1)
}

# Apply comparison function to each imputed dataset
anova_results <- lapply(imputed_datasets, compare_models)

p_values = c()

# Inspect results
for (i in seq(1,100,1)) {
  pvalue = anova_results[[i]]$`Pr(>F)`[2]
  p_values = c(p_values, pvalue)
  
}

mean(p_values)  # Average p-value across imputations
sum(p_values < 0.05, na.rm = TRUE)

```

25/100 anova tests have p-value less than 0.05 =\> Thus, we conclude that the model with both sd_temp and sd_temp sex interaction is significantly better than the model with just sex(p \>.05), or that sd_temp is significantly associated with diast.bp. in a model which contains interaction.

#Diabetes

## Systolic blood pressure

### Apparent average temperature

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrsybp ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean +
                    y1_app_temp_avg_46:c66_46_urban_rural + y1_app_temp_avg_46:c66_46_diab +
                    y1_app_temp_avg_46:c66_46_c_bmi_cat +
                    y1_app_temp_avg_46:c66_46_q1_ht + y1_app_temp_avg_46:sex))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)

#c66_46_q1_ht, c66_46_edu, c66_46_alc, , sex, c66_46_urban_rural 
```

#### Visualizing an interaction

```{r}
# Apply the linear function to each imputed dataset
lm_model_no_inter <- lm(c66_46_avrsybp ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = bp_df_plot)

lm_model_inter <- lm(c66_46_avrsybp ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean + y1_app_temp_avg_46:c66_46_diab, data = bp_df_plot)

# Effect of BMXWAIST at each level of RIAGENDR
# in the model with no interaction
plot_model(lm_model_no_inter,
           type = "eff",
           terms = c("y1_app_temp_avg_46", "c66_46_diab"),
           title = "No Interaction",
           axis.title = c("Apparent average temperature (°C)", 
                          "Systolic blood pressure (mmHg)"))

# Effect of BMXWAIST at each level of RIAGENDR
# in the model with an interaction
plot_model(lm_model_inter,
           type = "eff",
           terms = c("y1_app_temp_avg_46", "c66_46_diab"),
           title = "Interaction",
           axis.title = c("Apparent average temperature (°C)", 
                          "Systolic blood pressure (mmHg)"))
```

```{r}
library(sjPlot)
library(ggplot2)

# Apply the linear function to each imputed dataset
lm_model_no_inter <- lm(c66_46_avrsybp ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = bp_df_plot)

lm_model_inter <- lm(c66_46_avrsybp ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean +
                    y1_app_temp_avg_46:c66_46_urban_rural + y1_app_temp_avg_46:c66_46_diab +
                    y1_app_temp_avg_46:c66_46_c_bmi_cat +
                    y1_app_temp_avg_46:c66_46_q1_ht + y1_app_temp_avg_46:sex, data = bp_df_plot)


plot_model(
  lm_model_no_inter,
  type = "eff",
  terms = c("y1_app_temp_avg_46", "c66_46_diab"),
  title = "No Interaction term",
  axis.title = c("Annual apparent average temperature (°C)", 
                 "Systolic blood pressure (mmHg)"),
  pred.labels = c("Mean Apparent Temp (°C)", "Diabetes"),
  legend.title = "Diabetes",
  legend.labels = c("Yes", "No")
) +
  # Add a minimal theme
  theme_minimal() +
  # Customize some text sizing and alignment
  theme(
    plot.title = element_text(hjust = 0.5, size = 14),
    axis.title = element_text(size = 13),
    axis.text = element_text(size = 11),
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 11)
  )

plot_model(
  lm_model_inter,
  type = "eff",
  terms = c("y1_app_temp_avg_46", "c66_46_diab"),
  title = "ApT and SBP under interaction with diabetes",
  axis.title = c("Annual apparent average temperature (°C)", 
                 "Systolic blood pressure (mmHg)"),
  pred.labels = c("Mean Apparent Temp (°C)", "Diabetes"),
  legend.title = "Diabetes",
  legend.labels = c("Yes", "No")
) +
  # Add a minimal theme
  theme_minimal() +
  # Customize some text sizing and alignment
  theme(
    plot.title = element_text(hjust = 0.5, size = 14),
    axis.title = element_text(size = 13),
    axis.text = element_text(size = 12),
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 11)
  )
```


#### Testing the difference between the slopes

In the right panel, is the difference between the male and female slopes statistically significant? This is equivalent to asking, “Are the lines in the plot significantly different from parallel?” The p-value for the regression coefficient for the interaction tests the null hypothesis H0:β3=0. Under the null hypothesis, the lines are parallel. Thus, to test the difference between the slopes, examine the interaction p-value.

From the output, the p-value for the test of interaction is 0.3. In this case, since the interaction was between a continuous predictor and a binary predictor, the interaction is just one term, so the test can be obtained directly from the regression coefficient table. When the interaction involves a categorical predictor with more than two levels, we should perform `car::Anova(type=3)`.

Since p \> .05, we can make the following two inferences:

-   There is a non-significant difference in the app*\_*avg_temp effect on syst.bp between male and female (the lines are not significantly different from parallel).

-   There is a non significant difference in the sex effect on syst.bp between those with differing app*\_*avg_temp (the vertical distance between the lines does not depend on how large or small app*\_*avg_temp is).

-   Effect of app*\_*avg_temp on syst.bp does not depend on sex.

#### Estimating and testing the significance of the slope at each level of a moderator

```{r}
source("Functions_rmph.R")
# Test slope for female
lm_models <- with(pmm_proj_multiimp, 
  lm(c66_46_avrsybp ~ y1_app_temp_avg_46 +
       c66_46_q1_ht + c66_46_c_bmi_cat +
       relevel(c66_46_diab, ref = "Yes") + 
       c66_46_urban_rural + c66_46_edu +  
       c66_46_smok + c66_46_alc +
       c66_46_phys + c66_46_occup + 
        sex + 
       c66_46_ht_med + c66_46_hpcl + 
       c66_46_season + day_mean +
       y1_app_temp_avg_46:relevel(c66_46_diab, ref = "Yes") +
     y1_app_temp_avg_46:c66_46_urban_rural + 
                    y1_app_temp_avg_46:c66_46_c_bmi_cat +
                    y1_app_temp_avg_46:c66_46_q1_ht + y1_app_temp_avg_46:sex))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)
```

##### **Conclusion** 

Female(Est = -0.71; 95% p = \<01) is statistically significant.

#### Overall test of a predictor involved in an interaction

```{r}
# Get a list of all imputed datasets
imputed_datasets <- mice::complete(pmm_proj_multiimp, "all")

# Replace Y, predictors1, and predictors2 with your formulas
# Example:
compare_models <- function(data) {
  fit0 <- lm(c66_46_avrsybp ~ c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = data)
  fit1 <- lm(c66_46_avrsybp ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean + y1_app_temp_avg_46:c66_46_diab, data = data)
  anova(fit0, fit1)
}

# Apply comparison function to each imputed dataset
anova_results <- lapply(imputed_datasets, compare_models)

p_values = c()

# Inspect results
for (i in seq(1,100,1)) {
  pvalue = anova_results[[i]]$`Pr(>F)`[2]
  p_values = c(p_values, pvalue)
  
}

mean(p_values)  # Average p-value across imputations
sum(p_values < 0.05, na.rm = TRUE)

```

100/100 anova tests have p-value less than 0.05 =\> Thus, we conclude that the model with both app_avg_temp and app_avg_temp sex interaction is significantly better than the model with just bmi_cat(p \<.05), or that app_avg_temp is significantly associated with syst.bp. in a model which contains interaction.

### Apparent standard deviation of average temperature (-)

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrsybp ~ y1_app_temp_sd_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean +
                    y1_app_temp_sd_46:c66_46_diab))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)

#c66_46_q1_ht, c66_46_edu, c66_46_alc, , sex, c66_46_urban_rural 
```

#### Visualizing an interaction

```{r}
# Apply the linear function to each imputed dataset
lm_model_no_inter <- lm(c66_46_avrsybp ~ y1_app_temp_sd_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = bp_df_plot)

lm_model_inter <- lm(c66_46_avrsybp ~ y1_app_temp_sd_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean +
                    y1_app_temp_sd_46:c66_46_urban_rural + y1_app_temp_sd_46:c66_46_diab +
                    y1_app_temp_sd_46:c66_46_c_bmi_cat +
                    y1_app_temp_sd_46:c66_46_q1_ht + y1_app_temp_sd_46:sex, data = bp_df_plot)

# Effect of BMXWAIST at each level of RIAGENDR
# in the model with no interaction
plot_model(lm_model_no_inter,
           type = "eff",
           terms = c("y1_app_temp_sd_46", "c66_46_diab"),
           title = "No Interaction",
           axis.title = c("Standard deviation of apparent average temperature (°C)", 
                          "Systolic blood pressure (mmHg)"))

# Effect of BMXWAIST at each level of RIAGENDR
# in the model with an interaction
plot_model(lm_model_inter,
           type = "eff",
           terms = c("y1_app_temp_sd_46", "c66_46_diab"),
           title = "Interaction",
           axis.title = c("Standard deviation of apparent average temperature (°C)", 
                          "Systolic blood pressure (mmHg)"))
```

#### Testing the difference between the slopes

In the right panel, is the difference between the male and female slopes statistically significant? This is equivalent to asking, “Are the lines in the plot significantly different from parallel?” The p-value for the regression coefficient for the interaction tests the null hypothesis H0:βinter=0. Under the null hypothesis, the lines are parallel. Thus, to test the difference between the slopes, examine the interaction p-value.

From the output, the p-value for the test of interaction is **0.52**. In this case, since the interaction was between a continuous predictor and a binary predictor, the interaction is just one term, so the test can be obtained directly from the regression coefficient table. When the interaction involves a categorical predictor with more than two levels, we should perform `car::Anova(type=3)`.

Since p \> .05, we can make the following two inferences:

-   There is a non-significant difference in the app*\_*sd_temp effect on syst.bp between male and female (the lines are not significantly different from parallel).

-   There is a non significant difference in the sex effect on syst.bp between those with differing app*\_*sd_temp (the vertical distance between the lines does not depend on how large or small app*\_*sd_temp is).

-   Effect of app*\_*sd_temp on syst.bp does not depend on sex.

#### Estimating and testing the significance of the slope at each level of a moderator

```{r}
# Test slope for female
lm_models <- with(pmm_proj_multiimp, 
  lm(c66_46_avrsybp ~ y1_app_temp_sd_46 +
       c66_46_q1_ht + c66_46_c_bmi_cat +
       relevel(c66_46_diab, ref = "Yes") + 
       c66_46_urban_rural + c66_46_edu +  
       c66_46_smok + c66_46_alc +
       c66_46_phys + c66_46_occup + 
       sex + 
       c66_46_ht_med + c66_46_hpcl + 
       c66_46_season + day_mean +
       y1_app_temp_sd_46:relevel(c66_46_diab, ref = "Yes")))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2) 	
```

##### **Conclusion** 

Female(Est = -0.13; p = .67) are NOT statistically significant.

#### Overall test of a predictor involved in an interaction

```{r}
# Get a list of all imputed datasets
imputed_datasets <- mice::complete(pmm_proj_multiimp, "all")

# Replace Y, predictors1, and predictors2 with your formulas
# Example:
compare_models <- function(data) {
  fit0 <- lm(c66_46_avrsybp ~ c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = data)
  fit1 <- lm(c66_46_avrsybp ~ y1_app_temp_sd_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean + y1_app_temp_sd_46:c66_46_diab, data = data)
  anova(fit0, fit1)
}

# Apply comparison function to each imputed dataset
anova_results <- lapply(imputed_datasets, compare_models)

p_values = c()

# Inspect results
for (i in seq(1,100,1)) {
  pvalue = anova_results[[i]]$`Pr(>F)`[2]
  p_values = c(p_values, pvalue)
  
}

mean(p_values)  # Average p-value across imputations
sum(p_values < 0.05, na.rm = TRUE)

```

0/100 anova tests have p-value less than 0.05 =\> Thus, we conclude that the model with both app_sd_temp and app_sd_temp \*sex interaction is NOT significantly better than the model with just sex(p \>.05), or that app_sd_temp is NOT significantly associated with syst.bp. in a model which contains interaction.

### Average temperature

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrsybp ~ y1_avg_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean +
                    y1_avg_temp:c66_46_diab))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)

#c66_46_q1_ht, c66_46_edu, c66_46_alc, , sex, c66_46_urban_rural 
```

#### Visualizing an interaction

```{r}
# Apply the linear function to each imputed dataset
lm_model_no_inter <- lm(c66_46_avrsybp ~ y1_avg_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = bp_df_plot)

lm_model_inter <- lm(c66_46_avrsybp ~ y1_avg_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean + y1_avg_temp:c66_46_diab, data = bp_df_plot)

# Effect of BMXWAIST at each level of RIAGENDR
# in the model with no interaction
plot_model(lm_model_no_inter,
           type = "eff",
           terms = c("y1_avg_temp", "c66_46_diab"),
           title = "No Interaction",
           axis.title = c("average temperature (°C)", 
                          "Systolic blood pressure (mmHg)"))

# Effect of BMXWAIST at each level of RIAGENDR
# in the model with an interaction
plot_model(lm_model_inter,
           type = "eff",
           terms = c("y1_avg_temp", "c66_46_diab"),
           title = "Interaction",
           axis.title = c("average temperature (°C)", 
                          "Systolic blood pressure (mmHg)"))
```

#### Testing the difference between the slopes

From the output, the p-value for the test of interaction is **0.33**. In this case, since the interaction was between a continuous predictor and a binary predictor, the interaction is just one term, so the test can be obtained directly from the regression coefficient table. When the interaction involves a categorical predictor with more than two levels, we should perform `car::Anova(type=3)`.

#### Estimating and testing the significance of the slope at each level of a moderator

```{r}
# Test slope for female
lm_models <- with(pmm_proj_multiimp, 
  lm(c66_46_avrsybp ~ y1_avg_temp +
       c66_46_q1_ht + c66_46_c_bmi_cat +
       relevel(c66_46_diab, ref = "Yes") + 
       c66_46_urban_rural + c66_46_edu +  
       c66_46_smok + c66_46_alc +
       c66_46_phys + c66_46_occup + 
        + sex + 
       c66_46_ht_med + c66_46_hpcl + 
       c66_46_season + day_mean +
       y1_avg_temp:relevel(c66_46_diab, ref = "Yes")))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)
```

##### **Conclusion** 

Female (Est = -0.73; p \< .00) is statistically significant.

#### Overall test of a predictor involved in an interaction

```{r}
# Get a list of all imputed datasets
imputed_datasets <- mice::complete(pmm_proj_multiimp, "all")

# Replace Y, predictors1, and predictors2 with your formulas
# Example:
compare_models <- function(data) {
  fit0 <- lm(c66_46_avrsybp ~ c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = data)
  fit1 <- lm(c66_46_avrsybp ~ y1_avg_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean + y1_avg_temp:c66_46_diab, data = data)
  anova(fit0, fit1)
}

# Apply comparison function to each imputed dataset
anova_results <- lapply(imputed_datasets, compare_models)

p_values = c()

# Inspect results
for (i in seq(1,100,1)) {
  pvalue = anova_results[[i]]$`Pr(>F)`[2]
  p_values = c(p_values, pvalue)
  
}

mean(p_values)  # Average p-value across imputations
sum(p_values < 0.05, na.rm = TRUE)

```

100/100 anova tests have p-value less than 0.05 =\> Thus, we conclude that the model with both avg_temp and avg_temp\* sex interaction is significantly better than the model with just sex(p \<.05), or that avg_temp is significantly associated with syst.bp. in a model which contains interaction.

### Standard deviation of average temperature (-)

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrsybp ~ y1_sd_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean +
                    y1_sd_temp:c66_46_diab))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)

#c66_46_q1_ht, c66_46_edu, c66_46_alc, , sex, c66_46_urban_rural 
```

#### Visualizing an interaction

```{r}
# Apply the linear function to each imputed dataset
lm_model_no_inter <- lm(c66_46_avrsybp ~ y1_sd_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = bp_df_plot)

lm_model_inter <- lm(c66_46_avrsybp ~ y1_sd_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean + y1_sd_temp:c66_46_diab, data = bp_df_plot)

# Effect of BMXWAIST at each level of RIAGENDR
# in the model with no interaction
plot_model(lm_model_no_inter,
           type = "eff",
           terms = c("y1_sd_temp", "c66_46_diab"),
           title = "No Interaction",
           axis.title = c("Standard deviation of apparent average temperature (°C)", 
                          "Systolic blood pressure (mmHg)"))

# Effect of BMXWAIST at each level of RIAGENDR
# in the model with an interaction
plot_model(lm_model_inter,
           type = "eff",
           terms = c("y1_sd_temp", "c66_46_diab"),
           title = "Interaction",
           axis.title = c("Standard deviation of apparent average temperature (°C)", 
                          "Systolic blood pressure (mmHg)"))
```

#### Testing the difference between the slopes

From the output, the p-value for the test of interaction is **0.57**. In this case, since the interaction was between a continuous predictor and a binary predictor, the interaction is just one term, so the test can be obtained directly from the regression coefficient table. When the interaction involves a categorical predictor with more than two levels, we should perform `car::Anova(type=3)`.

#### Estimating and testing the significance of the slope at each level of a moderator

```{r}
# Test slope for female
lm_models <- with(pmm_proj_multiimp, 
  lm(c66_46_avrsybp ~ y1_sd_temp +
       c66_46_q1_ht + c66_46_c_bmi_cat +
       relevel(c66_46_diab, ref = "Yes") + 
       c66_46_urban_rural + c66_46_edu +  
       c66_46_smok + c66_46_alc +
       c66_46_phys + c66_46_occup + 
       sex + 
       c66_46_ht_med + c66_46_hpcl + 
       c66_46_season + day_mean +
       y1_sd_temp:relevel(c66_46_diab, ref = "Yes")))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2) 	
```

##### **Conclusion** 

Female(Est = -0.18; p = .56) are NOT statistically significant.

#### Overall test of a predictor involved in an interaction

```{r}
# Get a list of all imputed datasets
imputed_datasets <- mice::complete(pmm_proj_multiimp, "all")

# Replace Y, predictors1, and predictors2 with your formulas
# Example:
compare_models <- function(data) {
  fit0 <- lm(c66_46_avrsybp ~ c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = data)
  fit1 <- lm(c66_46_avrsybp ~ y1_sd_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean + y1_sd_temp:c66_46_diab, data = data)
  anova(fit0, fit1)
}

# Apply comparison function to each imputed dataset
anova_results <- lapply(imputed_datasets, compare_models)

p_values = c()

# Inspect results
for (i in seq(1,100,1)) {
  pvalue = anova_results[[i]]$`Pr(>F)`[2]
  p_values = c(p_values, pvalue)
  
}

mean(p_values)  # Average p-value across imputations
sum(p_values < 0.05, na.rm = TRUE)

```

0/100 anova tests have p-value less than 0.05 =\> Thus, we conclude that the model with both sd_temp and sd_temp \*sex interaction is NOT significantly better than the model with just sex (p \>.05), or that sd_temp is NOT significantly associated with syst.bp. in a model which contains interaction.

## Diastolic blood pressure

### Apparent average temperature

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrdibp ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean +
                    y1_app_temp_avg_46:c66_46_diab))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)

#c66_46_q1_ht, c66_46_edu, c66_46_alc, , sex, c66_46_urban_rural 
```

#### Visualizing an interaction

```{r}
# Apply the linear function to each imputed dataset
lm_model_no_inter <- lm(c66_46_avrdibp ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = bp_df_plot)

lm_model_inter <- lm(c66_46_avrdibp ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean + y1_app_temp_avg_46:c66_46_diab, data = bp_df_plot)

# Effect of BMXWAIST at each level of RIAGENDR
# in the model with no interaction
plot_model(lm_model_no_inter,
           type = "eff",
           terms = c("y1_app_temp_avg_46", "c66_46_diab"),
           title = "No Interaction",
           axis.title = c("Apparent average temperature (°C)", 
                          "diastolic blood pressure (mmHg)"))

# Effect of BMXWAIST at each level of RIAGENDR
# in the model with an interaction
plot_model(lm_model_inter,
           type = "eff",
           terms = c("y1_app_temp_avg_46", "c66_46_diab"),
           title = "Interaction",
           axis.title = c("Apparent average temperature (°C)", 
                          "diastolic blood pressure (mmHg)"))
```

```{r}
library(sjPlot)
library(ggplot2)

# Apply the linear function to each imputed dataset
lm_model_no_inter <- lm(c66_46_avrdibp ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = bp_df_plot)

lm_model_inter <- lm(c66_46_avrdibp ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean +
                    y1_app_temp_avg_46:c66_46_urban_rural + y1_app_temp_avg_46:c66_46_diab +
                    y1_app_temp_avg_46:c66_46_c_bmi_cat +
                    y1_app_temp_avg_46:c66_46_q1_ht + y1_app_temp_avg_46:sex, data = bp_df_plot)


plot_model(
  lm_model_no_inter,
  type = "eff",
  terms = c("y1_app_temp_avg_46", "c66_46_diab"),
  title = "No Interaction model",
  axis.title = c("Annual apparent average temperature (°C)", 
                 "Diastolic blood pressure (mmHg)"),
  pred.labels = c("Mean Apparent Temp (°C)", "Diabetes"),
  legend.title = "Diabetes",
  legend.labels = c("Yes", "No")
) +
  # Add a minimal theme
  theme_minimal() +
  # Customize some text sizing and alignment
  theme(
    plot.title = element_text(hjust = 0.5, size = 14),
    axis.title = element_text(size = 13),
    axis.text = element_text(size = 11),
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 11)
  )

plot_model(
  lm_model_inter,
  type = "eff",
  terms = c("y1_app_temp_avg_46", "c66_46_diab"),
  title = "ApT and DBP under interaction with diabetes",
  axis.title = c("Annual apparent average temperature (°C)", 
                 "Diastolic blood pressure (mmHg)"),
  pred.labels = c("Mean Apparent Temp (°C)", "Diabetes"),
  legend.title = "Diabetes",
  legend.labels = c("Yes", "No")
) +
  # Add a minimal theme
  theme_minimal() +
  # Customize some text sizing and alignment
  theme(
    plot.title = element_text(hjust = 0.5, size = 14),
    axis.title = element_text(size = 13),
    axis.text = element_text(size = 12),
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 11)
  )
```


#### Testing the difference between the slopes

From the output, the p-value for the test of interaction is **0.72**. In this case, since the interaction was between a continuous predictor and a binary predictor, the interaction is just one term, so the test can be obtained directly from the regression coefficient table. When the interaction involves a categorical predictor with more than two levels, we should perform `car::Anova(type=3)`.

#### Estimating and testing the significance of the slope at each level of a moderator

```{r}
# Test slope for female
lm_models <- with(pmm_proj_multiimp, 
  lm(c66_46_avrdibp ~ y1_app_temp_avg_46 +
       c66_46_q1_ht + c66_46_c_bmi_cat +
       relevel(c66_46_diab, ref = "Yes") + 
       c66_46_urban_rural + c66_46_edu +  
       c66_46_smok + c66_46_alc +
       c66_46_phys + c66_46_occup + 
        sex + 
       c66_46_ht_med + c66_46_hpcl + 
       c66_46_season + day_mean +
       y1_app_temp_avg_46:relevel(c66_46_diab, ref = "Yes") +
     y1_app_temp_avg_46:c66_46_urban_rural + 
                    y1_app_temp_avg_46:c66_46_c_bmi_cat +
                    y1_app_temp_avg_46:c66_46_q1_ht + y1_app_temp_avg_46:sex))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)
```

##### **Conclusion** 

The app*\_*avg_temp effects for female(Est = -0.6; 95% p \< 0.01) are statistically significant.

#### Overall test of a predictor involved in an interaction

```{r}
# Get a list of all imputed datasets
imputed_datasets <- mice::complete(pmm_proj_multiimp, "all")

# Replace Y, predictors1, and predictors2 with your formulas
# Example:
compare_models <- function(data) {
  fit0 <- lm(c66_46_avrdibp ~ c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = data)
  fit1 <- lm(c66_46_avrdibp ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean + y1_app_temp_avg_46:c66_46_diab, data = data)
  anova(fit0, fit1)
}

# Apply comparison function to each imputed dataset
anova_results <- lapply(imputed_datasets, compare_models)

p_values = c()

# Inspect results
for (i in seq(1,100,1)) {
  pvalue = anova_results[[i]]$`Pr(>F)`[2]
  p_values = c(p_values, pvalue)
  
}

mean(p_values)  # Average p-value across imputations
sum(p_values < 0.05, na.rm = TRUE)

```

100/100 anova tests have p-value less than 0.05 =\> Thus, we conclude that the model with both app_avg_temp and app_avg_temp \*sex interaction is significantly better than the model with just sex(p \<.05), or that app_avg_temp is significantly associated with diast.bp. in a model which contains interaction.

### Apparent standard deviation of average temperature (-)

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrdibp ~ y1_app_temp_sd_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean +
                    y1_app_temp_sd_46:c66_46_diab))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)

#c66_46_q1_ht, c66_46_edu, c66_46_alc, , sex, c66_46_urban_rural 
```

#### Visualizing an interaction

```{r}
# Apply the linear function to each imputed dataset
lm_model_no_inter <- lm(c66_46_avrdibp ~ y1_app_temp_sd_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = bp_df_plot)

lm_model_inter <- lm(c66_46_avrdibp ~ y1_app_temp_sd_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean +
                    y1_app_temp_sd_46:c66_46_urban_rural + y1_app_temp_sd_46:c66_46_diab +
                    y1_app_temp_sd_46:c66_46_c_bmi_cat +
                    y1_app_temp_sd_46:c66_46_q1_ht + y1_app_temp_sd_46:sex, data = bp_df_plot)

# Effect of BMXWAIST at each level of RIAGENDR
# in the model with no interaction
plot_model(lm_model_no_inter,
           type = "eff",
           terms = c("y1_app_temp_sd_46", "c66_46_diab"),
           title = "No Interaction",
           axis.title = c("Standard deviation of apparent average temperature (°C)", 
                          "diastolic blood pressure (mmHg)"))

# Effect of BMXWAIST at each level of RIAGENDR
# in the model with an interaction
plot_model(lm_model_inter,
           type = "eff",
           terms = c("y1_app_temp_sd_46", "c66_46_diab"),
           title = "Interaction",
           axis.title = c("Standard deviation of apparent average temperature (°C)", 
                          "diastolic blood pressure (mmHg)"))
```

#### Testing the difference between the slopes

From the output, the p-value for the test of interaction is **0.53**. In this case, since the interaction was between a continuous predictor and a binary predictor, the interaction is just one term, so the test can be obtained directly from the regression coefficient table. When the interaction involves a categorical predictor with more than two levels, we should perform `car::Anova(type=3)`.

#### Estimating and testing the significance of the slope at each level of a moderator

```{r}
# Test slope for female
lm_models <- with(pmm_proj_multiimp, 
  lm(c66_46_avrdibp ~ y1_app_temp_sd_46 +
       c66_46_q1_ht + c66_46_c_bmi_cat +
       relevel(c66_46_diab, ref = "Yes") + 
       c66_46_urban_rural + c66_46_edu +  
       c66_46_smok + c66_46_alc +
       c66_46_phys + c66_46_occup + 
       sex + 
       c66_46_ht_med + c66_46_hpcl + 
       c66_46_season + day_mean +
       y1_app_temp_sd_46:relevel(c66_46_diab, ref = "Yes")))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2) 	
```

#### Overall test of a predictor involved in an interaction

```{r}
# Get a list of all imputed datasets
imputed_datasets <- mice::complete(pmm_proj_multiimp, "all")

# Replace Y, predictors1, and predictors2 with your formulas
# Example:
compare_models <- function(data) {
  fit0 <- lm(c66_46_avrdibp ~ c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = data)
  fit1 <- lm(c66_46_avrdibp ~ y1_app_temp_sd_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean + y1_app_temp_sd_46:c66_46_diab, data = data)
  anova(fit0, fit1)
}

# Apply comparison function to each imputed dataset
anova_results <- lapply(imputed_datasets, compare_models)

p_values = c()

# Inspect results
for (i in seq(1,100,1)) {
  pvalue = anova_results[[i]]$`Pr(>F)`[2]
  p_values = c(p_values, pvalue)
  
}

mean(p_values)  # Average p-value across imputations
sum(p_values < 0.05, na.rm = TRUE)

```

0/100 anova tests have p-value less than 0.05 =\> Thus, we conclude that the model with both app_sd_temp and app_sd_temp\* sex interaction is NOT significantly better than the model with just sex(p \>.05), or that app_sd_temp is NOT significantly associated with diast.bp. in a model which contains interaction.

### Average temperature

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrdibp ~ y1_avg_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean +
                    y1_avg_temp:c66_46_diab))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)

#c66_46_q1_ht, c66_46_edu, c66_46_alc, , sex, c66_46_urban_rural 
```

#### Visualizing an interaction

```{r}
# Apply the linear function to each imputed dataset
lm_model_no_inter <- lm(c66_46_avrdibp ~ y1_avg_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = bp_df_plot)

lm_model_inter <- lm(c66_46_avrdibp ~ y1_avg_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean + y1_avg_temp:c66_46_diab, data = bp_df_plot)

# Effect of BMXWAIST at each level of RIAGENDR
# in the model with no interaction
plot_model(lm_model_no_inter,
           type = "eff",
           terms = c("y1_avg_temp", "c66_46_diab"),
           title = "No Interaction",
           axis.title = c("Apparent average temperature (°C)", 
                          "diastolic blood pressure (mmHg)"))

# Effect of BMXWAIST at each level of RIAGENDR
# in the model with an interaction
plot_model(lm_model_inter,
           type = "eff",
           terms = c("y1_avg_temp", "c66_46_diab"),
           title = "Interaction",
           axis.title = c("Apparent average temperature (°C)", 
                          "diastolic blood pressure (mmHg)"))
```

#### Testing the difference between the slopes

From the output, the p-value for the test of interaction is **0.6**. In this case, since the interaction was between a continuous predictor and a binary predictor, the interaction is just one term, so the test can be obtained directly from the regression coefficient table. When the interaction involves a categorical predictor with more than two levels, we should perform `car::Anova(type=3)`.

#### Estimating and testing the significance of the slope at each level of a moderator

```{r}
# Test slope for female
lm_models <- with(pmm_proj_multiimp, 
  lm(c66_46_avrdibp ~ y1_avg_temp +
       c66_46_q1_ht + c66_46_c_bmi_cat +
       relevel(c66_46_diab, ref = "Yes") + 
       c66_46_urban_rural + c66_46_edu +  
       c66_46_smok + c66_46_alc +
       c66_46_phys + c66_46_occup + 
        + sex + 
       c66_46_ht_med + c66_46_hpcl + 
       c66_46_season + day_mean +
       y1_avg_temp:relevel(c66_46_diab, ref = "Yes")))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)
```

##### **Conclusion** 

The app*\_*avg_temp effects for female (Est = -0.64; p \< .01) are statistically significant.

#### Overall test of a predictor involved in an interaction

```{r}
# Get a list of all imputed datasets
imputed_datasets <- mice::complete(pmm_proj_multiimp, "all")

# Replace Y, predictors1, and predictors2 with your formulas
# Example:
compare_models <- function(data) {
  fit0 <- lm(c66_46_avrdibp ~ c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = data)
  fit1 <- lm(c66_46_avrdibp ~ y1_avg_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean + y1_avg_temp:c66_46_diab, data = data)
  anova(fit0, fit1)
}

# Apply comparison function to each imputed dataset
anova_results <- lapply(imputed_datasets, compare_models)

p_values = c()

# Inspect results
for (i in seq(1,100,1)) {
  pvalue = anova_results[[i]]$`Pr(>F)`[2]
  p_values = c(p_values, pvalue)
  
}

mean(p_values)  # Average p-value across imputations
sum(p_values < 0.05, na.rm = TRUE)

```

100/100 anova tests have p-value less than 0.05 =\> Thus, we conclude that the model with both avg_temp and avg_temp\* sex interaction is significantly better than the model with just sex (p \<.05), or that avg_temp is significantly associated with diast.bp. in a model which contains interaction.

### Standard deviation of average temperature (-)

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrdibp ~ y1_sd_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean +
                    y1_sd_temp:c66_46_diab))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)

#c66_46_q1_ht, c66_46_edu, c66_46_alc, , sex, c66_46_urban_rural 
```

#### Visualizing an interaction

```{r}
# Apply the linear function to each imputed dataset
lm_model_no_inter <- lm(c66_46_avrdibp ~ y1_sd_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = bp_df_plot)

lm_model_inter <- lm(c66_46_avrdibp ~ y1_sd_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean + y1_sd_temp:c66_46_diab, data = bp_df_plot)

# Effect of BMXWAIST at each level of RIAGENDR
# in the model with no interaction
plot_model(lm_model_no_inter,
           type = "eff",
           terms = c("y1_sd_temp", "c66_46_diab"),
           title = "No Interaction",
           axis.title = c("Standard deviation of average temperature (°C)", 
                          "diastolic blood pressure (mmHg)"))

# Effect of BMXWAIST at each level of RIAGENDR
# in the model with an interaction
plot_model(lm_model_inter,
           type = "eff",
           terms = c("y1_sd_temp", "c66_46_diab"),
           title = "Interaction",
           axis.title = c("Standard deviation of average temperature (°C)", 
                          "diastolic blood pressure (mmHg)"))
```

#### Testing the difference between the slopes

From the output, the p-value for the test of interaction is **0.44**. In this case, since the interaction was between a continuous predictor and a binary predictor, the interaction is just one term, so the test can be obtained directly from the regression coefficient table. When the interaction involves a categorical predictor with more than two levels, we should perform `car::Anova(type=3)`.

#### Estimating and testing the significance of the slope at each level of a moderator

```{r}
# Test slope for female
lm_models <- with(pmm_proj_multiimp, 
  lm(c66_46_avrdibp ~ y1_sd_temp +
       c66_46_q1_ht + c66_46_c_bmi_cat +
       relevel(c66_46_diab, ref = "Yes") + 
       c66_46_urban_rural + c66_46_edu +  
       c66_46_smok + c66_46_alc +
       c66_46_phys + c66_46_occup + 
       sex + 
       c66_46_ht_med + c66_46_hpcl + 
       c66_46_season + day_mean +
       y1_sd_temp:relevel(c66_46_diab, ref = "Yes")))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2) 	
```

##### **Conclusion** 

The sd_temp effects for female(Est = 0.23, p = .28) are NOT statistically significant.

#### Overall test of a predictor involved in an interaction

```{r}
# Get a list of all imputed datasets
imputed_datasets <- mice::complete(pmm_proj_multiimp, "all")

# Replace Y, predictors1, and predictors2 with your formulas
# Example:
compare_models <- function(data) {
  fit0 <- lm(c66_46_avrdibp ~ c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = data)
  fit1 <- lm(c66_46_avrdibp ~ y1_sd_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean + y1_sd_temp:c66_46_diab, data = data)
  anova(fit0, fit1)
}

# Apply comparison function to each imputed dataset
anova_results <- lapply(imputed_datasets, compare_models)

p_values = c()

# Inspect results
for (i in seq(1,100,1)) {
  pvalue = anova_results[[i]]$`Pr(>F)`[2]
  p_values = c(p_values, pvalue)
  
}

mean(p_values)  # Average p-value across imputations
sum(p_values < 0.05, na.rm = TRUE)

```

25/100 anova tests have p-value less than 0.05 =\> Thus, we conclude that the model with both sd_temp and sd_temp sex interaction is significantly better than the model with just sex(p \>.05), or that sd_temp is significantly associated with diast.bp. in a model which contains interaction.
