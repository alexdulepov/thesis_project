---
title: "Data_analysis"
author: "Aleksanr Dulepov"
format: pdf
editor: visual
---

```{r}
library(mgcv)
library(mice)
library(broom)
library(miceadds)
library(stargazer) #tables for regression output
library(purrr) # Flexible functional programming
library(stargazer)
library(table1)
library(htmlTable)
library(tidyverse)
library(olsrr) #best subset selection
library(statip) # for mfv1 = mode function
library(sandwich)
library(lmtest)
```

```{r}
#| eval: false
#| include: false
######extract summary in tibbles to backtransform the coefficients from log to original scale (broom package) for concurvity
glance(lm_pooled) # model performance
tidy(lm_pooled) # coefficients
augment(lm_pooled) #residuals,fitted values and other statistics
#################################################################
```

# Load mids objects

```{r}
file_path <- "//kaappi.oulu.fi/nfbc$/projects/P0511/Alex/2_pmm_proj_multiimp.rds"
file_path_2 <- "//kaappi.oulu.fi/nfbc$/projects/P0511/Alex/3_pmm_proj_multiimp_cimt.rds"
# Load
pmm_proj_multiimp = readRDS(file_path)
pmm_proj_multiimp_cimt = readRDS(file_path_2)
```

# Averaged dataset over all imputations for plotting

## Blood pressure dataset:

```{r}
# A mode function for factor columns
# mfv1 is from statip, or you could define your own:
# get_mode <- function(x) { ... }

bp_df_plot <- pmm_proj_multiimp %>%
  mice::complete("long") %>%
  filter(.imp != 0) %>%   # exclude the original data
  group_by(.id) %>%
  summarise(
    # 1) Retain the project_id for each group 
    #    (assuming one project_id per .id group)
    project_ID = first(project_ID),
    
    # 2) Summarize numeric columns with the mean
    across(
      .cols = where(is.numeric),
      .fns = ~ mean(.x, na.rm = TRUE)
    ),
    
    # 3) Summarize factor columns with the single most frequent value
    across(
      .cols = where(is.factor),
      .fns = ~ mfv1(.x)
    )
  )%>%
  ungroup()

```

## CIMT dataset:

```{r}
#| eval: false
#| include: false
# A mode function for factor columns
# mfv1 is from statip, or you could define your own:
# get_mode <- function(x) { ... }

cimt_df_plot <- pmm_proj_multiimp_cimt %>%
  mice::complete("long") %>%
  filter(.imp != 0) %>%   # exclude the original data
  group_by(.id) %>%
  summarise(
    # 1) Retain the project_id for each group 
    #    (assuming one project_id per .id group)
    project_ID = first(project_ID),
    
    # 2) Summarize numeric columns with the mean
    across(
      .cols = where(is.numeric),
      .fns = ~ mean(.x, na.rm = TRUE)
    ),
    
    # 3) Summarize factor columns with the single most frequent value
    across(
      .cols = where(is.factor),
      .fns = ~ mfv1(.x)
    )
  )%>%
  ungroup()
```

# Table 1

```{r}
source("Functions_rmph.R")

# Means and standard deviations
rbind(mi.mean.se.sd(pmm_proj_multiimp, "c66_46_avrsybp"),
      mi.mean.se.sd(pmm_proj_multiimp, "c66_46_avrdibp"),
      mi.mean.se.sd(pmm_proj_multiimp, "c66_46_c_bmi"),
      mi.mean.se.sd(pmm_proj_multiimp, "c66_46_phys"),
      mi.mean.se.sd(pmm_proj_multiimp, "c66_46_ap_age"),
      mi.mean.se.sd(pmm_proj_multiimp, "y1_avg_temp"),
      mi.mean.se.sd(pmm_proj_multiimp, "y1_sd_temp"),
      mi.mean.se.sd(pmm_proj_multiimp, "y1_app_temp_avg_46"),
      mi.mean.se.sd(pmm_proj_multiimp, "y1_app_temp_sd_46"),
      mi.mean.se.sd(pmm_proj_multiimp, "day_mean")
      )

# Frequencies and proportions
rbind(mi.n.p(pmm_proj_multiimp, "c66_46_c_bmi_cat"),
      mi.n.p(pmm_proj_multiimp, "c66_46_ht_med"),
      mi.n.p(pmm_proj_multiimp, "c66_46_edu"),
      mi.n.p(pmm_proj_multiimp, "c66_46_smok"),
      mi.n.p(pmm_proj_multiimp, "c66_46_alc"),
      mi.n.p(pmm_proj_multiimp, "c66_46_occup"),
      mi.n.p(pmm_proj_multiimp, "sex"),
      mi.n.p(pmm_proj_multiimp, "c66_46_diab"),
      mi.n.p(pmm_proj_multiimp, "c66_46_hpcl"),
      mi.n.p(pmm_proj_multiimp, "c66_46_season"),
      mi.n.p(pmm_proj_multiimp, "c66_46_urban_rural"),
      mi.n.p(pmm_proj_multiimp, "c66_46_q1_ht")
      )

rbind(mi.mean.se.sd.by(pmm_proj_multiimp, "y1_app_temp_avg_46", BY = "c66_46_urban_rural"),
      mi.mean.se.sd.by(pmm_proj_multiimp, "y1_app_temp_sd_46", BY = "c66_46_urban_rural")
      )
```

```{r}
# Using dplyr
library(dplyr)

bp_df_plot %>%
  group_by(c66_46_urban_rural) %>%
  summarise(
    count = n(),
    mean = mean(y1_app_temp_avg_46, na.rm = TRUE),
    sd = sd(y1_app_temp_avg_46, na.rm = TRUE),
    min = min(y1_app_temp_avg_46, na.rm = TRUE),
    max = max(y1_app_temp_avg_46, na.rm = TRUE),
    median = median(y1_app_temp_avg_46, na.rm = TRUE)
  )

```

```{r}


```

# Blood pressure dataset

First we start with plotting to see what type of relationship (linear or non-linear) we have. We are going to use now only 1 imputed dataset just for brief overview.

## 1. Generalized additive model

### 1.1 Systolic blood pressure

#### 1.1.1 Apparent average temperature

##### Unadjusted model (univariate)

First we examine univariate GAM model:

```{r}
imputed_dataset <- bp_df_plot

gam_model <- gam(c66_46_avrsybp ~ s(y1_app_temp_avg_46) 
                 ,method = "REML", 
                 data = imputed_dataset)

summary(gam_model)

plot(
  gam_model,
  select = 1,
  residuals = TRUE,
  all.terms = TRUE,
  shift = coef(gam_model)[1],
  seWithMean = TRUE,
  rug = TRUE,
  shade = TRUE,
  shade.col = "lightblue",
  pch = 1,
  cex = 0.001,
  col = "black",
  xlab= "Annual average apparent temperature",
  ylab= "Systolic blood pressure",
  main = "GAM model for SBP vs. ApT")

#Check the model performance
gam.check(gam_model)
```

EDF is 9.55 (p\<0.001) which means high non-linear relationship. However, based on the plot there is no clear evidence for non-linear relationship. Radj is only 0.03.

GAM diagnostics shows some explicit problems with heteroscedasticity.

##### Minimally adjusted model

Next we assess minimally adjusted model:

```{r}
gam_model <- gam(c66_46_avrsybp ~ s(y1_app_temp_avg_46, k =20) + c66_46_edu +
                  c66_46_occup + sex + c66_46_ht_med
                 ,method = "REML", 
                 data = imputed_dataset)

summary(gam_model)

plot(
  gam_model,
  select = 1,
  residuals = TRUE,
  all.terms = TRUE,
  shift = coef(gam_model)[1],
  seWithMean = TRUE,
  rug = TRUE,
  shade = TRUE,
  shade.col = "blue",
  pch = 1,
  cex = 0.001,
  col = "darkgray")

#Check the model performance
gam.check(gam_model)
```

EDF has decreased slightly to 8.8 (p\<0.001). Radj has increased to 0.147. Residual and response plots look better, but there is still some heteroscedasticity.

##### Fully adjusted

Next we asses fully adjusted GAM model:

```{r}

imputed_dataset <- complete(pmm_proj_multiimp, action = 33)
gam_model <- gam(c66_46_avrsybp ~ s(y1_app_temp_avg_46) + c66_46_q1_ht + 
                  c66_46_c_bmi_cat + 
                   c66_46_ht_med + c66_46_edu +  c66_46_smok + c66_46_alc +
                   s(c66_46_phys) + c66_46_occup +  
                   sex + c66_46_diab + c66_46_hpcl + c66_46_season + c66_46_urban_rural+ 
                    s(day_mean),method = "REML", 
                 data = imputed_dataset)

summary(gam_model)

plot(
  gam_model,
  select = 1,
  residuals = TRUE,
  all.terms = TRUE,
  shift = coef(gam_model)[1],
  seWithMean = TRUE,
  rug = TRUE,
  shade = TRUE,
  shade.col = "lightblue",
  pch = 1,
  cex = 0.01,
  col = "black",
  xlab= "Annual average apparent temperature",
  ylab= "Systolic blood pressure",
  main = "A",
  ylim = c(100, 180)
)


#Check the model performance
gam.check(gam_model)
```

```{r}
#Concurvity check
concurvity(gam_model, full = TRUE)
concurvity(gam_model, full = FALSE)
```

Overall concurvity is normal for all smooth parameters in the worst case scenario (less than 0.7), however becomes problematic for the day_mean (0.65).

GAM summary on 1 dataset demonstrates:

1\) Smooth term of app.temp. is significant and has edf 7.512. Radj = 0.26. Among other smooth terms, only bmi has significant EDF \> 1 value.

GAM diagnostics showed that:

1\) We should increase a number of basis functions for app.temp. and bmi (changed to k=20)

2\) Outcome variable shows some non-normality (qq-plot)

3\) Residuals vs. Linear Predictor diagnostic plot, is used to check for issues like heteroscedasticity (non-constant variance), non-linearity, and outliers. If the model is well-fitted, the residuals should be randomly scattered around zero with no clear pattern.

-   Heteroscedasticity (non-constant variance): At lower predicted values (\~110-120): Residuals are more tightly clustered, suggesting lower variance in this range. There may be more spread in the residuals for higher predicted values (right side of the plot). At mid and higher predicted values (\~120-150): The residual spread appears relatively consistent, meaning variance does not systematically increase further. This suggests that the primary issue is not general heteroscedasticity (increasing variance with increasing fitted values), but rather a lower variance at smaller fitted values

-   Outliers: There are some extreme residuals (above 60 and below -40), which might need investigation.

-   Non-linearity: If there were any visible patterns (curves, waves, trends), this would indicate that the model is missing some non-linear structure. Here, the spread seems relatively uniform, suggesting that the GAM is capturing most of the patterns.

4\) Response vs. Fitted Values plot is a standard diagnostic tool for evaluating how well the fitted values (predictions) align with the observed response values.

-   The points are loosely following a diagonal pattern (as fitted values increase, response values also increase), suggesting the model is capturing the main trend in the data. This confirms that the model is not severely misspecified in terms of directionality.

-   No severe curvature or systematic deviation: If we saw a curved trend, it might indicate missing nonlinear effects, but here, the spread looks fairly uniform.

Next, we apply fully adjusted GAM to all imputed datasets:

```{r}
# Apply the GAM function to each imputed dataset
models <- with(pmm_proj_multiimp, 
               gam(c66_46_avrsybp ~ s(y1_app_temp_avg_46) + c66_46_q1_ht + 
                   c66_46_c_bmi_cat + 
                   c66_46_ht_med + c66_46_edu +  c66_46_smok + c66_46_alc +
                   s(c66_46_phys) + c66_46_occup +  
                   sex + c66_46_diab + c66_46_hpcl + c66_46_season + c66_46_urban_rural+ 
                    s(day_mean),method = "REML"))

# Use `pool()` to combine results across imputations
pooled_results <- pool(models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)
pooled_results
```

Next we calculate the pooled values of BIC and Radj:

```{r}
#For Radj apply fisher transformation on rooted Radj first

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(models$analyses[[i]])$BIC,
         Radj = sqrt(glance(models$analyses[[i]])$adj.r.squared))
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = (fisher.trans(Radj))) %>%
  summarise(pooled_Radj = (fisher.backtrans(mean(Radj_trans)))^2,  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

The pooled Radj over all imputed datasets is 0.26 and BIC is 45995.

Pooled EDF and p-value:

```{r}
# Function to extract EDF and p-values from a GAM model
extract_edf_pvalue <- function(model) {
  gam_summary <- summary(model)
  tibble(term = rownames(gam_summary$s.table),  # Extract term names
         EDF = gam_summary$s.table[, "edf"],    # Extract EDF values
         p_value = gam_summary$s.table[, "p-value"])  # Extract p-values
}

# Number of imputations
num_imp <- length(models$analyses)

# Extract EDF and p-values for each imputed model
edf_pvalue_list <- map(models$analyses, extract_edf_pvalue)

# Combine results into a single dataframe
edf_pvalue_df <- bind_rows(edf_pvalue_list, .id = "imputation")

# Pool EDF and p-values (taking the mean)
pooled_edf_pvalue <- edf_pvalue_df %>%
  group_by(term) %>%
  summarise(pooled_EDF = mean(EDF),
            pooled_p_value = median(p_value),  # The Median P Rule
            .groups = "drop")

# View the pooled EDF and p-values
pooled_edf_pvalue
```

Pooled EDF for app.temp is 7.62 with p\<0.001 which again says about strong non-linear relationship, while plot does not support this idea.

#### 1.1.2 Apparent temperature standard deviation

##### Unadjusted model (univariate)

Now we will do the same wil apparent standard deviation:

```{r}
gam_model <- gam(c66_46_avrsybp ~ s(y1_app_temp_sd_46, k =20) 
                 ,method = "REML", 
                 data = imputed_dataset)

summary(gam_model)

plot(
  gam_model,
  select = 1,
  residuals = TRUE,
  all.terms = TRUE,
  shift = coef(gam_model)[1],
  seWithMean = TRUE,
  rug = TRUE,
  shade = TRUE,
  shade.col = "blue",
  pch = 1,
  cex = 0.001,
  col = "darkgray")

#Check the model performance
gam.check(gam_model)
```

EDF is 6.16 (p\<0.01), Radj = 0.004. Diagnostic plots look similar to app.avg.temp but have less clustering. Plot do no support non-linear relationship.

##### Minimally adjusted model

Next we assess minimally adjusted model:

```{r}
gam_model <- gam(c66_46_avrsybp ~ s(y1_app_temp_sd_46, k =40) + c66_46_edu +
                  c66_46_occup + sex + c66_46_ht_med
                 ,method = "REML", 
                 data = imputed_dataset)

summary(gam_model)

plot(
  gam_model,
  select = 1,
  residuals = TRUE,
  all.terms = TRUE,
  shift = coef(gam_model)[1],
  seWithMean = TRUE,
  rug = TRUE,
  shade = TRUE,
  shade.col = "blue",
  pch = 1,
  cex = 0.001,
  col = "darkgray")

#Check the model performance
gam.check(gam_model)
```

EDF is 5.48 (p=0.028), Radj = 0.13. Diagnostic plots show clusterness at some values. The variance of the outcome is not equal between exposure values.

##### Fully adjusted

Next we asses fully adjusted GAM model:

```{r}
imputed_dataset <- complete(pmm_proj_multiimp, action = 52)
gam_model <- gam(c66_46_avrsybp ~ s(y1_app_temp_sd_46) + c66_46_q1_ht + 
                   c66_46_c_bmi_cat + 
                   c66_46_ht_med + c66_46_edu +  c66_46_smok + c66_46_alc +
                   s(c66_46_phys) + c66_46_occup +
                   sex + c66_46_diab + c66_46_hpcl + c66_46_season + c66_46_urban_rural+ 
                    s(day_mean),method = "REML", 
                 data = imputed_dataset)

summary(gam_model)

plot(
  gam_model,
  select = 1,
  residuals = TRUE,
  all.terms = TRUE,
  shift = coef(gam_model)[1],
  seWithMean = TRUE,
  rug = TRUE,
  shade = TRUE,
  shade.col = "lightblue",
  pch = 1,
  cex = 0.01,
  col = "black",
  xlab= "Standard deviation of the apparent temperature",
  ylab= "Systolic blood pressure",
  main = "A",
  ylim = c(90, 180)
)


#Check the model performance
gam.check(gam_model)
```

```{r}
#Concurvity check
concurvity(gam_model, full = TRUE)
concurvity(gam_model, full = FALSE)
```

EDF is 6.1 (p\<0.001), Radj = 0.247. However, plot shows more linear relationship than non-linear. Among other smooth terms, only bmi has significant EDF \> 1 value. Plot does not provide evidence for non linear relationship.

GAM diagnostics showed that:

1\) The change of the basis functions numbers does not affect p-value of k-index.

2\) Outcome variable shows some non-normality (qq-plot)

3\) Residuals vs. Linear Predictor diagnostic plot:

-   Heteroscedasticity (non-constant variance): At lower and higher predicted values (\~110-120): Residuals are slightly more tightly clustered, suggesting lower variance in this range. Even heteroscedasticity present, it is likely small.

-   Outliers: There are some extreme residuals (above 60 and below -40), which might need investigation.

-   Non-linearity: No non-linearity patterns.

4\) Response vs. Fitted Values plot is a standard diagnostic tool for evaluating how well the fitted values (predictions) align with the observed response values.

-   The points are loosely following a diagonal pattern (as fitted values increase, response values also increase), suggesting the model is capturing the main trend in the data. This confirms that the model is not severely misspecified in terms of directionality.

-   No severe curvature or systematic deviation: If we saw a curved trend, it might indicate missing nonlinear effects, but here, the spread looks fairly uniform.

Next, we apply fully adjusted GAM to all imputed datasets:

```{r}
# Apply the GAM function to each imputed dataset
models <- with(pmm_proj_multiimp, 
               gam(c66_46_avrsybp ~ s(y1_app_temp_sd_46) + c66_46_q1_ht + 
                   c66_46_c_bmi_cat + 
                   c66_46_ht_med + c66_46_edu +  c66_46_smok + c66_46_alc +
                   s(c66_46_phys) + c66_46_occup +
                   sex + c66_46_diab + c66_46_hpcl + c66_46_season + c66_46_urban_rural+ 
                    s(day_mean),method = "REML"))

# Use `pool()` to combine results across imputations
pooled_results <- pool(models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)
pooled_results
```

Next we calculate the pooled values of BIC and Radj:

```{r}
#For Radj apply fisher transformation on rooted Radj first

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(models$analyses[[i]])$BIC,
         Radj = sqrt(glance(models$analyses[[i]])$adj.r.squared))
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = (fisher.trans(Radj))) %>%
  summarise(pooled_Radj = (fisher.backtrans(mean(Radj_trans)))^2,  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

The pooled Radj over all imputed datasets is 0.25 and BIC is 46 090.

Pooled EDF and p-value:

```{r}
# Function to extract EDF and p-values from a GAM model
extract_edf_pvalue <- function(model) {
  gam_summary <- summary(model)
  tibble(term = rownames(gam_summary$s.table),  # Extract term names
         EDF = gam_summary$s.table[, "edf"],    # Extract EDF values
         p_value = gam_summary$s.table[, "p-value"])  # Extract p-values
}

# Number of imputations
num_imp <- length(models$analyses)

# Extract EDF and p-values for each imputed model
edf_pvalue_list <- map(models$analyses, extract_edf_pvalue)

# Combine results into a single dataframe
edf_pvalue_df <- bind_rows(edf_pvalue_list, .id = "imputation")

# Pool EDF and p-values (taking the mean)
pooled_edf_pvalue <- edf_pvalue_df %>%
  group_by(term) %>%
  summarise(pooled_EDF = mean(EDF),
            pooled_p_value = median(p_value),  # The Median P Rule
            .groups = "drop")

# View the pooled EDF and p-values
pooled_edf_pvalue
```

Pooled EDF for app.temp is 5.05 with p\<0.001 which again says about strong non-linear relationship, while plot does not support this idea.

#### 1.1.3 Average temperature

##### Unadjusted model (univariate)

```{r}
gam_model <- gam(c66_46_avrsybp ~ s(y1_avg_temp, k =20) 
                 ,method = "REML", 
                 data = imputed_dataset)

summary(gam_model)

plot(
  gam_model,
  select = 1,
  residuals = TRUE,
  all.terms = TRUE,
  shift = coef(gam_model)[1],
  seWithMean = TRUE,
  rug = TRUE,
  shade = TRUE,
  shade.col = "blue",
  pch = 1,
  cex = 0.001,
  col = "darkgray")

#Check the model performance
gam.check(gam_model)
```

EDF = 9.22 (p\<0.001), Radj = 0.024. Plot shows some non linearity around 5-6.5 degress Celcius but overall it seems to be linear. Plot diagnostics have similar issues to app. temp.

##### Minimally adjusted model

Next we assess minimally adjusted model:

```{r}
gam_model <- gam(c66_46_avrsybp ~ s(y1_avg_temp, k =20) + c66_46_edu +
                  c66_46_occup + sex + c66_46_ht_med
                 ,method = "REML", 
                 data = imputed_dataset)

summary(gam_model)

plot(
  gam_model,
  select = 1,
  residuals = TRUE,
  all.terms = TRUE,
  shift = coef(gam_model)[1],
  seWithMean = TRUE,
  rug = TRUE,
  shade = TRUE,
  shade.col = "blue",
  pch = 1,
  cex = 0.001,
  col = "darkgray")

#Check the model performance
gam.check(gam_model)
```

EDF = 8.148 (p\<0.001), Radj = 0.142. Trends in diagnostic plots are similar to app.temp.

##### Fully adjusted

Next we asses fully adjusted GAM model:

```{r}
gam_model <- gam(c66_46_avrsybp ~ s(y1_avg_temp, k =20) + c66_46_q1_ht + 
                   s(c66_46_c_bmi, k = 20) + 
                   c66_46_ht_med + c66_46_edu +  c66_46_smok + c66_46_alc +
                   s(c66_46_phys) + c66_46_occup +  +
                   sex + c66_46_diab + c66_46_hpcl + c66_46_season + c66_46_urban_rural+ 
                    s(day_mean),method = "REML", 
                 data = imputed_dataset)

summary(gam_model)

p1 = plot(
  gam_model,
  select = 1,
  residuals = TRUE,
  all.terms = TRUE,
  shift = coef(gam_model)[1],
  seWithMean = TRUE,
  rug = TRUE,
  shade = TRUE,
  shade.col = "lightblue",
  pch = 1,
  cex = 0.001,
  col = "black",
  xlab= "Standard deviation of daily apparent temperature within a year",
  ylab= "Systolic blood pressure",
  main = "GAM fully adjusted model of ApT vs. SBP"
)

#Check the model performance
gam.check(gam_model)
```

```{r}
#Concurvity check
concurvity(gam_model, full = TRUE)
concurvity(gam_model, full = FALSE)
```

GAM summary on 1 dataset demonstrates:

1\) Smooth term of app.temp. is significant and has edf 6.694. Radj = 0.26. Among other smooth terms, only bmi has significant EDF \> 1 value.

**The more variables in the model the less exp-out relationship is non-linear.**

All diagnostics similar to app. temp.

Next, we apply fully adjusted GAM to all imputed datasets:

```{r}
# Apply the GAM function to each imputed dataset
models <- with(pmm_proj_multiimp, 
               gam(c66_46_avrsybp ~ s(y1_avg_temp, k =20) + c66_46_q1_ht + 
                   s(c66_46_c_bmi, k = 20) + 
                   c66_46_ht_med + c66_46_edu +  c66_46_smok + c66_46_alc +
                   s(c66_46_phys) + c66_46_occup +  +
                   sex + c66_46_diab + c66_46_hpcl + c66_46_season + c66_46_urban_rural+ 
                    s(day_mean),method = "REML"))

# Use `pool()` to combine results across imputations
pooled_results <- pool(models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)
pooled_results
```

Next we calculate the pooled values of BIC and Radj:

```{r}
#For Radj apply fisher transformation on rooted Radj first

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(models$analyses[[i]])$BIC,
         Radj = sqrt(glance(models$analyses[[i]])$adj.r.squared))
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = (fisher.trans(Radj))) %>%
  summarise(pooled_Radj = (fisher.backtrans(mean(Radj_trans)))^2,  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

The pooled Radj over all imputed datasets is 0.26 and BIC is 46 006.

Pooled EDF and p-value:

```{r}
# Function to extract EDF and p-values from a GAM model
extract_edf_pvalue <- function(model) {
  gam_summary <- summary(model)
  tibble(term = rownames(gam_summary$s.table),  # Extract term names
         EDF = gam_summary$s.table[, "edf"],    # Extract EDF values
         p_value = gam_summary$s.table[, "p-value"])  # Extract p-values
}

# Number of imputations
num_imp <- length(models$analyses)

# Extract EDF and p-values for each imputed model
edf_pvalue_list <- map(models$analyses, extract_edf_pvalue)

# Combine results into a single dataframe
edf_pvalue_df <- bind_rows(edf_pvalue_list, .id = "imputation")

# Pool EDF and p-values (taking the mean)
pooled_edf_pvalue <- edf_pvalue_df %>%
  group_by(term) %>%
  summarise(pooled_EDF = mean(EDF),
            pooled_p_value = median(p_value),  # The Median P Rule
            .groups = "drop")

# View the pooled EDF and p-values
pooled_edf_pvalue
```

Pooled EDF for app.temp is 6.83 with p\<0.001 which again says about strong non-linear relationship, while plot does not support this idea.

#### 1.1.4 Average temperature standard deviation

##### Unadjusted model (univariate)

Now we will examine avg.temp. standard deviation:

```{r}
gam_model <- gam(c66_46_avrsybp ~ s(y1_sd_temp, k =20) 
                 ,method = "REML", 
                 data = imputed_dataset)

summary(gam_model)

plot(
  gam_model,
  select = 1,
  residuals = TRUE,
  all.terms = TRUE,
  shift = coef(gam_model)[1],
  seWithMean = TRUE,
  rug = TRUE,
  shade = TRUE,
  shade.col = "blue",
  pch = 1,
  cex = 0.001,
  col = "darkgray")

#Check the model performance
gam.check(gam_model)
```

EDF is 7.45 (p\<0.01), Radj = 0.005. Diagnostic plots look similar to app.avg.temp but have less clustering. Plot do no support non-linear relationship.

##### Minimally adjusted model

Next we assess minimally adjusted model:

```{r}
gam_model <- gam(c66_46_avrsybp ~ s(y1_sd_temp, k =40) + c66_46_edu +
                  c66_46_occup + sex + c66_46_ht_med
                 ,method = "REML", 
                 data = imputed_dataset)

summary(gam_model)

plot(
  gam_model,
  select = 1,
  residuals = TRUE,
  all.terms = TRUE,
  shift = coef(gam_model)[1],
  seWithMean = TRUE,
  rug = TRUE,
  shade = TRUE,
  shade.col = "blue",
  pch = 1,
  cex = 0.001,
  col = "darkgray")

#Check the model performance
gam.check(gam_model)
```

EDF is 7.1 (p=0.028), Radj = 0.13. Diagnostic plots show clusterness at some values. The variance of the outcome is not equal between exposure values.

##### Fully adjusted

Next we asses fully adjusted GAM model:

```{r}
gam_model <- gam(c66_46_avrsybp ~ s(y1_sd_temp, k =20) + c66_46_q1_ht + 
                   s(c66_46_c_bmi, k = 20) + 
                   c66_46_ht_med + c66_46_edu +  c66_46_smok + c66_46_alc +
                   s(c66_46_phys) + c66_46_occup +  +
                   sex + c66_46_diab + c66_46_hpcl + c66_46_season + c66_46_urban_rural+ 
                    s(day_mean),method = "REML", 
                 data = imputed_dataset)

summary(gam_model)

png("gam_plot_app_sd.png", width = 1920, height = 1080, res = 300)
plot(
  gam_model,
  select = 1,
  residuals = TRUE,
  all.terms = TRUE,
  shift = coef(gam_model)[1],
  seWithMean = TRUE,
  rug = TRUE,
  shade = TRUE,
  shade.col = "blue",
  pch = 1,
  cex = 0.001,
  col = "darkgray"
)
dev.off()

plot(
  gam_model,
  select = 1,
  residuals = TRUE,
  all.terms = TRUE,
  shift = coef(gam_model)[1],
  seWithMean = TRUE,
  rug = TRUE,
  shade = TRUE,
  shade.col = "blue",
  pch = 1,
  cex = 0.001,
  col = "darkgray")

#Check the model performance
gam.check(gam_model)
```

```{r}
#Concurvity check
concurvity(gam_model, full = TRUE)
concurvity(gam_model, full = FALSE)
```

Relatively high overall concurvity of sd_temp, ap_age and day_mean. The highest concurvity is found between sd_temp and ap_age (0.53)

GAM summary on 1 dataset demonstrates:

1\) Smooth term of app.temp. is significant and has edf 5.3. Radj = 0.25. Among other smooth terms, only bmi has significant EDF \> 1 value.

**The more variables in the model the less exp-out relationship is non-linear.**

All diagnostics similar to app. temp.

Next, we apply fully adjusted GAM to all imputed datasets:

```{r}
# Apply the GAM function to each imputed dataset
models <- with(pmm_proj_multiimp, 
               gam(c66_46_avrsybp ~ s(y1_sd_temp, k =20) + c66_46_q1_ht + 
                   s(c66_46_c_bmi, k = 20) + 
                   c66_46_ht_med + c66_46_edu +  c66_46_smok + c66_46_alc +
                   s(c66_46_phys) + c66_46_occup + 
                   sex + c66_46_diab + c66_46_hpcl + c66_46_season + c66_46_urban_rural+ 
                    s(day_mean),method = "REML"))

# Use `pool()` to combine results across imputations
pooled_results <- pool(models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)
pooled_results
```

Next we calculate the pooled values of BIC and Radj:

```{r}
#For Radj apply fisher transformation on rooted Radj first

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(models$analyses[[i]])$BIC,
         Radj = sqrt(glance(models$analyses[[i]])$adj.r.squared))
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = (fisher.trans(Radj))) %>%
  summarise(pooled_Radj = (fisher.backtrans(mean(Radj_trans)))^2,  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

The pooled Radj over all imputed datasets is 0.25 and BIC is 46 093.

Pooled EDF and p-value:

```{r}
# Function to extract EDF and p-values from a GAM model
extract_edf_pvalue <- function(model) {
  gam_summary <- summary(model)
  tibble(term = rownames(gam_summary$s.table),  # Extract term names
         EDF = gam_summary$s.table[, "edf"],    # Extract EDF values
         p_value = gam_summary$s.table[, "p-value"])  # Extract p-values
}

# Number of imputations
num_imp <- length(models$analyses)

# Extract EDF and p-values for each imputed model
edf_pvalue_list <- map(models$analyses, extract_edf_pvalue)

# Combine results into a single dataframe
edf_pvalue_df <- bind_rows(edf_pvalue_list, .id = "imputation")

# Pool EDF and p-values (taking the mean)
pooled_edf_pvalue <- edf_pvalue_df %>%
  group_by(term) %>%
  summarise(pooled_EDF = mean(EDF),
            pooled_p_value = median(p_value),  # The Median P Rule
            .groups = "drop")

# View the pooled EDF and p-values
pooled_edf_pvalue
```

Pooled EDF for app.temp is 5.37 with p\<0.001 which again says about strong non-linear relationship, while plot does not support this idea.

### 1.2 Diastolic blood pressure

#### 1.2.1 Apparent average temperature

##### Unadjusted model (univariate)

```{r}
gam_model <- gam(c66_46_avrdibp ~ s(y1_app_temp_avg_46, k =20) 
                 ,method = "REML", 
                 data = imputed_dataset)

summary(gam_model)

plot(
  gam_model,
  select = 1,
  residuals = TRUE,
  all.terms = TRUE,
  shift = coef(gam_model)[1],
  seWithMean = TRUE,
  rug = TRUE,
  shade = TRUE,
  shade.col = "blue",
  pch = 1,
  cex = 0.001,
  col = "darkgray")

#Check the model performance
gam.check(gam_model)
```

EDF is 6.8 (p\<0.001) which means high non-linear relationship. However, based on the plot there is no clear evidence for non-linear relationship. Radj is only 0.02.

GAM diagnostics shows some explicit problems with heteroscedasticity.

##### Minimally adjusted model

Next we assess minimally adjusted model:

```{r}
gam_model <- gam(c66_46_avrdibp ~ s(y1_app_temp_avg_46, k =20) + c66_46_edu +
                  c66_46_occup + sex + c66_46_ht_med
                 ,method = "REML", 
                 data = imputed_dataset)

summary(gam_model)

plot(
  gam_model,
  select = 1,
  residuals = TRUE,
  all.terms = TRUE,
  shift = coef(gam_model)[1],
  seWithMean = TRUE,
  rug = TRUE,
  shade = TRUE,
  shade.col = "blue",
  pch = 1,
  cex = 0.001,
  col = "darkgray")

#Check the model performance
gam.check(gam_model)
```

EDF has decreased slightly to 5.9 (p\<0.001). Radj has increased to 0.08. Residual and response plots look better, but there is still some heteroscedasticity.

##### Fully adjusted

Next we asses fully adjusted GAM model:

```{r}

imputed_dataset <- complete(pmm_proj_multiimp, action = 70)
gam_model <- gam(c66_46_avrdibp ~ s(y1_app_temp_avg_46) + c66_46_q1_ht + 
                   c66_46_c_bmi_cat + 
                   c66_46_ht_med + c66_46_edu +  c66_46_smok + c66_46_alc +
                   s(c66_46_phys) + c66_46_occup +  
                   sex + c66_46_diab + c66_46_hpcl + c66_46_season + c66_46_urban_rural+ 
                    s(day_mean),method = "REML", 
                 data = imputed_dataset)

summary(gam_model)


plot(
  gam_model,
  select = 1,
  residuals = TRUE,
  all.terms = TRUE,
  shift = coef(gam_model)[1],
  seWithMean = TRUE,
  rug = TRUE,
  shade = TRUE,
  shade.col = "lightblue",
  pch = 1,
  cex = 0.01,
  col = "black",
  xlab= "Annual average apparent temperature",
  ylab= "Diastolic blood pressure",
  main = "B",
  ylim = c(60, 120)
)

#Check the model performance
gam.check(gam_model)
```

```{r}
#Concurvity check
concurvity(gam_model, full = TRUE)
concurvity(gam_model, full = FALSE)
```

GAM summary on 1 dataset demonstrates:

1\) Smooth term of app.temp. is significant and has edf 5. Radj = 0.27.

**The more variables in the model the less exp-out relationship is non-linear.**

All diagnostics similar to syst. app. temp.

Next, we apply fully adjusted GAM to all imputed datasets:

```{r}
# Apply the GAM function to each imputed dataset
models <- with(pmm_proj_multiimp, 
               gam(c66_46_avrdibp ~ s(y1_app_temp_avg_46) + c66_46_q1_ht +
                   c66_46_c_bmi_cat + 
                   c66_46_ht_med + c66_46_edu +  c66_46_smok + c66_46_alc +
                   s(c66_46_phys) + c66_46_occup +  
                   sex + c66_46_diab + c66_46_hpcl + c66_46_season +c66_46_urban_rural+ 
                    s(day_mean),method = "REML"))

# Use `pool()` to combine results across imputations
pooled_results <- pool(models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)
pooled_results
```

Next we calculate the pooled values of BIC and Radj:

```{r}
#For Radj apply fisher transformation on rooted Radj first

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(models$analyses[[i]])$BIC,
         Radj = sqrt(glance(models$analyses[[i]])$adj.r.squared))
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = (fisher.trans(Radj))) %>%
  summarise(pooled_Radj = (fisher.backtrans(mean(Radj_trans)))^2,  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

The pooled Radj over all imputed datasets is 0.27 and BIC is 41423.

Pooled EDF and p-value:

```{r}
# Function to extract EDF and p-values from a GAM model
extract_edf_pvalue <- function(model) {
  gam_summary <- summary(model)
  tibble(term = rownames(gam_summary$s.table),  # Extract term names
         EDF = gam_summary$s.table[, "edf"],    # Extract EDF values
         p_value = gam_summary$s.table[, "p-value"])  # Extract p-values
}

# Number of imputations
num_imp <- length(models$analyses)

# Extract EDF and p-values for each imputed model
edf_pvalue_list <- map(models$analyses, extract_edf_pvalue)

# Combine results into a single dataframe
edf_pvalue_df <- bind_rows(edf_pvalue_list, .id = "imputation")

# Pool EDF and p-values (taking the mean)
pooled_edf_pvalue <- edf_pvalue_df %>%
  group_by(term) %>%
  summarise(pooled_EDF = mean(EDF),
            pooled_p_value = median(p_value),  # The Median P Rule
            .groups = "drop")

# View the pooled EDF and p-values
pooled_edf_pvalue
```

Pooled EDF for app.temp is 4.9 with p\<0.001 which again says about strong non-linear relationship, while plot does not support this idea.

#### 1.2.2 Apparent temperature standard deviation

##### Unadjusted model (univariate)

Now we will do the same wil apparent standard deviation:

```{r}
gam_model <- gam(c66_46_avrdibp ~ s(y1_app_temp_sd_46, k =20) 
                 ,method = "REML", 
                 data = imputed_dataset)

summary(gam_model)

plot(
  gam_model,
  select = 1,
  residuals = TRUE,
  all.terms = TRUE,
  shift = coef(gam_model)[1],
  seWithMean = TRUE,
  rug = TRUE,
  shade = TRUE,
  shade.col = "blue",
  pch = 1,
  cex = 0.001,
  col = "darkgray")

#Check the model performance
gam.check(gam_model)
```

EDF is 6.7 (p\<0.01), Radj = 0.004. Diagnostic plots look similar to app.avg.temp but have less clustering. Plot do no support non-linear relationship.

##### Fully adjusted

Next we asses fully adjusted GAM model:

```{r}
imputed_dataset <- bp_df_plot
gam_model <- gam(c66_46_avrdibp ~ s(y1_app_temp_sd_46) + c66_46_q1_ht + 
                   c66_46_c_bmi_cat + 
                   c66_46_ht_med + c66_46_edu +  c66_46_smok + c66_46_alc +
                   s(c66_46_phys) + c66_46_occup +  
                   sex + c66_46_diab + c66_46_hpcl + c66_46_season + c66_46_urban_rural+ 
                    s(day_mean),method = "REML", 
                 data = imputed_dataset)

summary(gam_model)

plot(
  gam_model,
  select = 1,
  residuals = TRUE,
  all.terms = TRUE,
  shift = coef(gam_model)[1],
  seWithMean = TRUE,
  rug = TRUE,
  shade = TRUE,
  shade.col = "lightblue",
  pch = 1,
  cex = 0.01,
  col = "black",
  xlab= "Standard deviation of the apparent temperature",
  ylab= "Diastolic blood pressure",
  main = "B",
  ylim = c(60, 110)
)

#Check the model performance
gam.check(gam_model)
```

```{r}
#Concurvity check
concurvity(gam_model, full = TRUE)
concurvity(gam_model, full = FALSE)
```

**EDF=1 for app_sd_temp which means that gam consider the variable as linear!**

Next, we apply fully adjusted GAM to all imputed datasets:

```{r}
# Apply the GAM function to each imputed dataset
models <- with(pmm_proj_multiimp, 
               gam(c66_46_avrdibp ~ s(y1_app_temp_sd_46) + c66_46_q1_ht + 
                   c66_46_c_bmi_cat + 
                   c66_46_ht_med + c66_46_edu +  c66_46_smok + c66_46_alc +
                   s(c66_46_phys) + c66_46_occup +  
                   sex + c66_46_diab + c66_46_hpcl + c66_46_season + c66_46_urban_rural+ 
                    s(day_mean),method = "REML"))

# Use `pool()` to combine results across imputations
pooled_results <- pool(models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)
pooled_results
```

Next we calculate the pooled values of BIC and Radj:

```{r}
#For Radj apply fisher transformation on rooted Radj first

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(models$analyses[[i]])$BIC,
         Radj = sqrt(glance(models$analyses[[i]])$adj.r.squared))
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = (fisher.trans(Radj))) %>%
  summarise(pooled_Radj = (fisher.backtrans(mean(Radj_trans)))^2,  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

The pooled Radj over all imputed datasets is 0.26 and BIC is 41434.

Pooled EDF and p-value:

```{r}
# Function to extract EDF and p-values from a GAM model
extract_edf_pvalue <- function(model) {
  gam_summary <- summary(model)
  tibble(term = rownames(gam_summary$s.table),  # Extract term names
         EDF = gam_summary$s.table[, "edf"],    # Extract EDF values
         p_value = gam_summary$s.table[, "p-value"])  # Extract p-values
}

# Number of imputations
num_imp <- length(models$analyses)

# Extract EDF and p-values for each imputed model
edf_pvalue_list <- map(models$analyses, extract_edf_pvalue)

# Combine results into a single dataframe
edf_pvalue_df <- bind_rows(edf_pvalue_list, .id = "imputation")

# Pool EDF and p-values (taking the mean)
pooled_edf_pvalue <- edf_pvalue_df %>%
  group_by(term) %>%
  summarise(pooled_EDF = mean(EDF),
            pooled_p_value = median(p_value),  # The Median P Rule
            .groups = "drop")

# View the pooled EDF and p-values
pooled_edf_pvalue
```

Pooled EDF for app.sd is 1 which again says about linear relationship!

#### 1.2.3 Average temperature

##### Unadjusted model (univariate)

```{r}
gam_model <- gam(c66_46_avrdibp ~ s(y1_avg_temp, k =20) 
                 ,method = "REML", 
                 data = imputed_dataset)

summary(gam_model)

plot(
  gam_model,
  select = 1,
  residuals = TRUE,
  all.terms = TRUE,
  shift = coef(gam_model)[1],
  seWithMean = TRUE,
  rug = TRUE,
  shade = TRUE,
  shade.col = "blue",
  pch = 1,
  cex = 0.001,
  col = "darkgray")

#Check the model performance
gam.check(gam_model)
```

EDF = 6.7 (p\<0.001), Radj = 0.01. Trends in diagnostic plots are similar to app.temp.

##### Fully adjusted

Next we asses fully adjusted GAM model:

```{r}
gam_model <- gam(c66_46_avrdibp ~ s(y1_avg_temp, k =20) + c66_46_q1_ht + 
                   s(c66_46_c_bmi, k = 20) + 
                   c66_46_ht_med + c66_46_edu +  c66_46_smok + c66_46_alc +
                   s(c66_46_phys) + c66_46_occup +  +
                   sex + c66_46_diab + c66_46_hpcl + c66_46_season + c66_46_urban_rural+ 
                    s(day_mean),method = "REML", 
                 data = imputed_dataset)

summary(gam_model)

png("gam_plot_avg_t.png", width = 1920, height = 1080, res = 300)
plot(
  gam_model,
  select = 1,
  residuals = TRUE,
  all.terms = TRUE,
  shift = coef(gam_model)[1],
  seWithMean = TRUE,
  rug = TRUE,
  shade = TRUE,
  shade.col = "blue",
  pch = 1,
  cex = 0.001,
  col = "darkgray"
)
dev.off()

plot(
  gam_model,
  select = 1,
  residuals = TRUE,
  all.terms = TRUE,
  shift = coef(gam_model)[1],
  seWithMean = TRUE,
  rug = TRUE,
  shade = TRUE,
  shade.col = "blue",
  pch = 1,
  cex = 0.001,
  col = "darkgray")

#Check the model performance
gam.check(gam_model)
```

```{r}
#Concurvity check
concurvity(gam_model, full = TRUE)
concurvity(gam_model, full = FALSE)
```

Concurvity is okay.

GAM summary on 1 dataset demonstrates:

1\) Smooth term of app.temp. is significant and has edf 4.9. Radj = 0.27.

**The more variables in the model the less exp-out relationship is non-linear.**

All diagnostics similar to app. temp.

Next, we apply fully adjusted GAM to all imputed datasets:

```{r}
# Apply the GAM function to each imputed dataset
models <- with(pmm_proj_multiimp, 
               gam(c66_46_avrdibp ~ s(y1_avg_temp, k =20) + c66_46_q1_ht + 
                   s(c66_46_c_bmi, k = 20) + 
                   c66_46_ht_med + c66_46_edu +  c66_46_smok + c66_46_alc +
                   s(c66_46_phys) + c66_46_occup +  +
                   sex + c66_46_diab + c66_46_hpcl + c66_46_season + c66_46_urban_rural+ 
                    s(day_mean),method = "REML"))

# Use `pool()` to combine results across imputations
pooled_results <- pool(models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)
pooled_results
```

Next we calculate the pooled values of BIC and Radj:

```{r}
#For Radj apply fisher transformation on rooted Radj first

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(models$analyses[[i]])$BIC,
         Radj = sqrt(glance(models$analyses[[i]])$adj.r.squared))
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = (fisher.trans(Radj))) %>%
  summarise(pooled_Radj = (fisher.backtrans(mean(Radj_trans)))^2,  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

The pooled Radj over all imputed datasets is 0.26 and BIC is 41429.

Pooled EDF and p-value:

```{r}
# Function to extract EDF and p-values from a GAM model
extract_edf_pvalue <- function(model) {
  gam_summary <- summary(model)
  tibble(term = rownames(gam_summary$s.table),  # Extract term names
         EDF = gam_summary$s.table[, "edf"],    # Extract EDF values
         p_value = gam_summary$s.table[, "p-value"])  # Extract p-values
}

# Number of imputations
num_imp <- length(models$analyses)

# Extract EDF and p-values for each imputed model
edf_pvalue_list <- map(models$analyses, extract_edf_pvalue)

# Combine results into a single dataframe
edf_pvalue_df <- bind_rows(edf_pvalue_list, .id = "imputation")

# Pool EDF and p-values (taking the mean)
pooled_edf_pvalue <- edf_pvalue_df %>%
  group_by(term) %>%
  summarise(pooled_EDF = mean(EDF),
            pooled_p_value = median(p_value),  # The Median P Rule
            .groups = "drop")

# View the pooled EDF and p-values
pooled_edf_pvalue
```

Pooled EDF for app.temp is 4.9 with p\<0.001 which again says about strong non-linear relationship, while plot does not support this idea.

#### 1.2.4 Average temperature standard deviation

##### Unadjusted model (univariate)

Now we will examine avg.temp. standard deviation:

```{r}
gam_model <- gam(c66_46_avrdibp ~ s(y1_sd_temp, k =20) 
                 ,method = "REML", 
                 data = imputed_dataset)

summary(gam_model)

plot(
  gam_model,
  select = 1,
  residuals = TRUE,
  all.terms = TRUE,
  shift = coef(gam_model)[1],
  seWithMean = TRUE,
  rug = TRUE,
  shade = TRUE,
  shade.col = "blue",
  pch = 1,
  cex = 0.001,
  col = "darkgray")

#Check the model performance
gam.check(gam_model)
```

EDF is 7.1 (p\<0.01), Radj = 0.005. Diagnostic plots look similar to app.avg.temp but have less clustering. Plot do no support non-linear relationship.

##### Minimally adjusted model

Next we assess minimally adjusted model:

```{r}
gam_model <- gam(c66_46_avrdibp ~ s(y1_sd_temp, k =40) + c66_46_edu +
                  c66_46_occup + sex + c66_46_ht_med
                 ,method = "REML", 
                 data = imputed_dataset)

summary(gam_model)

plot(
  gam_model,
  select = 1,
  residuals = TRUE,
  all.terms = TRUE,
  shift = coef(gam_model)[1],
  seWithMean = TRUE,
  rug = TRUE,
  shade = TRUE,
  shade.col = "blue",
  pch = 1,
  cex = 0.001,
  col = "darkgray")

#Check the model performance
gam.check(gam_model)
```

EDF is 6.7 (p=0.02), Radj = 0.08. Diagnostic plots show clusterness at some values. The variance of the outcome is not equal between exposure values.

##### Fully adjusted

Next we asses fully adjusted GAM model:

```{r}
gam_model <- gam(c66_46_avrdibp ~ s(y1_sd_temp, k =20) + c66_46_q1_ht + 
                   s(c66_46_c_bmi, k = 20) + 
                   c66_46_ht_med + c66_46_edu +  c66_46_smok + c66_46_alc +
                   s(c66_46_phys) + c66_46_occup +  +
                   sex + c66_46_diab + c66_46_hpcl + c66_46_season + c66_46_urban_rural+ 
                    s(day_mean),method = "REML", 
                 data = imputed_dataset)

summary(gam_model)

png("gam_plot_app_sd.png", width = 1920, height = 1080, res = 300)
plot(
  gam_model,
  select = 1,
  residuals = TRUE,
  all.terms = TRUE,
  shift = coef(gam_model)[1],
  seWithMean = TRUE,
  rug = TRUE,
  shade = TRUE,
  shade.col = "blue",
  pch = 1,
  cex = 0.001,
  col = "darkgray"
)
dev.off()

plot(
  gam_model,
  select = 1,
  residuals = TRUE,
  all.terms = TRUE,
  shift = coef(gam_model)[1],
  seWithMean = TRUE,
  rug = TRUE,
  shade = TRUE,
  shade.col = "blue",
  pch = 1,
  cex = 0.001,
  col = "darkgray")

#Check the model performance
gam.check(gam_model)
```

```{r}
#Concurvity check
concurvity(gam_model, full = TRUE)
concurvity(gam_model, full = FALSE)
```

Relatively high overall concurvity of sd_temp, ap_age and day_mean. The highest concurvity is found between sd_temp and ap_age (0.53)

EDF=1 for sd_temp which means that gam consider the variable as linear!

**The more variables in the model the less exp-out relationship is non-linear.**

All diagnostics similar to syst. app. temp.

Next, we apply fully adjusted GAM to all imputed datasets:

```{r}
# Apply the GAM function to each imputed dataset
models <- with(pmm_proj_multiimp, 
               gam(c66_46_avrdibp ~ s(y1_sd_temp, k =20) + c66_46_q1_ht + 
                   s(c66_46_c_bmi, k = 20) + 
                   c66_46_ht_med + c66_46_edu +  c66_46_smok + c66_46_alc +
                   s(c66_46_phys) + c66_46_occup + 
                   sex + c66_46_diab + c66_46_hpcl + c66_46_season + c66_46_urban_rural+ 
                    s(day_mean),method = "REML"))

# Use `pool()` to combine results across imputations
pooled_results <- pool(models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)
pooled_results
```

Next we calculate the pooled values of BIC and Radj:

```{r}
#For Radj apply fisher transformation on rooted Radj first

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(models$analyses[[i]])$BIC,
         Radj = sqrt(glance(models$analyses[[i]])$adj.r.squared))
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = (fisher.trans(Radj))) %>%
  summarise(pooled_Radj = (fisher.backtrans(mean(Radj_trans)))^2,  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

The pooled Radj over all imputed datasets is 0.26 and BIC is 41454

Pooled EDF and p-value:

```{r}
# Function to extract EDF and p-values from a GAM model
extract_edf_pvalue <- function(model) {
  gam_summary <- summary(model)
  tibble(term = rownames(gam_summary$s.table),  # Extract term names
         EDF = gam_summary$s.table[, "edf"],    # Extract EDF values
         p_value = gam_summary$s.table[, "p-value"])  # Extract p-values
}

# Number of imputations
num_imp <- length(models$analyses)

# Extract EDF and p-values for each imputed model
edf_pvalue_list <- map(models$analyses, extract_edf_pvalue)

# Combine results into a single dataframe
edf_pvalue_df <- bind_rows(edf_pvalue_list, .id = "imputation")

# Pool EDF and p-values (taking the mean)
pooled_edf_pvalue <- edf_pvalue_df %>%
  group_by(term) %>%
  summarise(pooled_EDF = mean(EDF),
            pooled_p_value = median(p_value),  # The Median P Rule
            .groups = "drop")

# View the pooled EDF and p-values
pooled_edf_pvalue
```

Pooled EDF for app.temp is 3.2 with p\<0.001 which again says about strong non-linear relationship, while plot does not support this idea.

## 2 Linear Model

### Model Selection

#### Forward + backward stepwise selection

The idea behind optimized model is that to see the best possible model from the statistical point of view and compare with other models to determine the difference in estimates. This automatical model selection allows to exclude mutualy correlated variables and variables that do not improve the model by the BIC value. In other words, this method

exclude statistically "nonsignificant" variables.

We are going to apply `step` function that will generate a vector with the selected variables by utilizing forward + backward stepwise regression and BIC as a comparison metric between models. The final table shows how often each of the variables from the full model was selected.

```{r}
# Define outcomes and exposures
outcomes <- c("c66_46_avrdibp", "c66_46_avrsybp")
exposures <- c("y1_app_temp_avg_46", "y1_app_temp_sd_46", "y1_avg_temp", "y1_sd_temp")

# We will store the results in a list
results_list <- list()
counter <- 1

# Loop over each outcome and each exposure
for(out in outcomes){
  for(exp in exposures){
    
    # Define the scope with the specific exposure in the upper formula.
    scope <- list(
      upper = as.formula(
        paste("~", exp, "+ c66_46_q1_ht + c66_46_c_bmi + c66_46_ht_med +",
              "c66_46_edu + c66_46_smok + c66_46_alc + c66_46_phys +",
              "c66_46_occup +  sex + c66_46_diab +",
              "c66_46_hpcl + c66_46_season + day_mean + c66_46_urban_rural")
      ),
      lower = as.formula(paste("~", exp))
    )
    
    # Create an expression for the stepwise selection.
    # Substitute the outcome and the exposure variable names.
    expr_i <- substitute({
      f1 <- lm(OUTCOME ~ EXPOSURE)
      n <- nrow(model.frame(f1))
      f2 <- step(f1, scope = SCOPE, 
                 direction = "both", 
                 trace = 0, 
                 k = log(n))
    },
    list(OUTCOME = as.name(out),
         EXPOSURE = as.name(exp),
         SCOPE = scope))
    
    # Run the expression on each imputed dataset.
    # (Here pmm_proj_multiimp is assumed to be an object of class "mids" or similar.)
    fit <- with(pmm_proj_multiimp, eval(expr_i))
    
    # Extract the formulas, then the terms and then the predictor labels.
    formulas <- lapply(fit$analyses, formula)
    terms_list <- lapply(formulas, terms)
    votes <- unlist(lapply(terms_list, labels))
    
    # Save the outcome, exposure, and votes into the results list.
    results_list[[counter]] <- list(outcome = out, exposure = exp, votes = votes)
    counter <- counter + 1
  }
}

# Combine votes from all outcome/exposure combinations:
all_votes <- unlist(lapply(results_list, function(x) x$votes))

# Display a frequency table of how many times each predictor was selected: 
table(all_votes)
```

Out of 800 models (2 outcomes \* 4 exposures \* 100 imputed datasets) \``and`c66_46_phys`,`c66_46_edu`,`c66_46_smok\` were selected less than half times (372/800 and 307/800 respectively).

`c66_46_c_bmi`, `c66_46_ht_med`, `c66_46_q1_ht`, `sex` , `c66_46_urban_rural`were selected in all cases (800/800). So they will be included in the optimal model.

`day_mean` , `c66_46_alc` were selected in more than half cases but we will look how often they were selected in each out+exp combination separately:

```{r}
# (Assuming your results_list is already created as in your code.)
# Create a list to store frequency tables for each outcome/exposure combination
freq_tables <- list()

for(res in results_list){
  # Create a unique key for the combination (e.g., "c66_46_avrdibp | y1_app_temp_avg_46")
  key <- paste(res$outcome, res$exposure, sep = " | ")
  # Create a frequency table for the predictors selected ("votes")
  freq_tables[[key]] <- table(res$votes)
}

# Print the frequency tables for each combination:
for(key in names(freq_tables)){
  cat("Outcome and Exposure:", key, "\n")
  print(freq_tables[[key]])
  cat("\n")
}
```

\`\` was not selected in all cases with standard deviations as exposures. This can be explained by the fact that these variables do have relatively high correlation and affect on estimates and standard errors of each other. This variable *will not be included* in the optimal model of *standard deviations*

`c66_46_phys` was selected more than half times when the outcome was diastolic blood pressure and 0 times with systolic blood pressure. This might be due to the fact that physical activity does affect diastolic blood pressure more than systolic. This variable *will not be included* in the optimal model with *systolic blood pressure* as outcome.

`c66_46_edu` was selected more than half times when the outcome was systolic blood pressure and when the outcome was diastolic blood pressure with stand. deviations as exposures, but less than half when the outcome was diastolic blood pressure and exposures avg/app_avg temp. This variable *will not be included* in the optimal model with *diastolic blood pressure + avg/app_avg temp*.

`c66_46_smok` was selected more than half times when the outcome was systolic blood pressure (\>89 times), but less than half in case with diastolic. This variable *will not be included* in the optimal model with *diastolic blood pressure*.

`c66_46_urban_rural` was selected all the times (100/100) for all outcome and exposure combinations except diast.bp + avg/app_avg temp. This variable *will not be included* in the optimal model with *diastolic blood pressure + avg/app_avg temp.*

`day_mean` was selected more than half times for all outcome and exposure combinations except diast.bp + avg/app_avg temp. This variable *will not be included* in the optimal model with *diastolic blood pressure + avg/app_avg temp*.

`c66_46_alc` was selected100/100 times with diastolic bp. and more than half with systolic bp.

\``,`c66_46_phys`,`c66_46_edu`,`c66_46_smok`,`c66_46_urban_rural`,`day_mean\` were selected in more than half cases but they will be additionally verified in order to determine if them should be a part of the final model. To perform that we will run a multivariate Wald test and compare models by BIC and p-value.

SYS: APP_AVG: CHECK ALC(+), EDU(+), SMOK (+) APP_SD:CHECK ALC(+), EDU(+), SMOK (+) AVG: CHECK ALC(+), EDU(+), SMOK (+) SD:CHECK ALC(+), EDU(+), SMOK (+)

DIAS: APP_AVG: NO PHYS, NO SMOK, NO EDU. CHECK DAY_MEAN APP_SD: NO EDU, NO PHYS, NO SMOK, NO DAY_MEAN AVG: NO PHYS, NO SMOK, NO EDU. CHECK DAY_MEAN SD: NO EDU, NO PHYS, NO SMOK, NO DAY_MEAN

##### Education level

For app_avg + syst.bp:

```{r}

fit.var <- with(pmm_proj_multiimp, 
                  lm(c66_46_avrsybp ~ y1_app_temp_avg_46 +  
                       c66_46_alc + 
                       c66_46_c_bmi + c66_46_ht_med +
                       c66_46_q1_ht + c66_46_smok +
                       c66_46_urban_rural + day_mean +
                       sex + c66_46_edu))
fit.novar <- with(pmm_proj_multiimp, 
                    lm(c66_46_avrsybp ~ y1_app_temp_avg_46 +  
                       c66_46_alc + 
                       c66_46_c_bmi + c66_46_ht_med +
                       c66_46_q1_ht + c66_46_smok +
                       c66_46_urban_rural + day_mean +
                       sex))

#Wald test
D1(fit.var, fit.novar) #p=6.13*10^-5 => edu is recommended to include in the model

#BIC comparison
BIC.var <- fit.var$analyses %>%
  sapply(BIC) 

BIC.novar <- fit.novar$analyses %>%
  sapply(BIC)

sum(BIC.var > BIC.novar) #BIC with edu is bigger in 11/100 cases => recommended to include in the model
```

Education should be included in the app_avg + syst.bp model.

For app_sd+ syst.bp:

```{r}

fit.var <- with(pmm_proj_multiimp, 
                  lm(c66_46_avrsybp ~ y1_app_temp_sd_46 +  
                       c66_46_alc + c66_46_c_bmi +
                       c66_46_ht_med + c66_46_q1_ht +
                       c66_46_smok + c66_46_urban_rural +
                       day_mean + sex +
                       c66_46_edu))
fit.novar <- with(pmm_proj_multiimp, 
                    lm(c66_46_avrsybp ~ y1_app_temp_sd_46 +  
                       c66_46_alc + c66_46_c_bmi +
                       c66_46_ht_med + c66_46_q1_ht +
                       c66_46_smok + c66_46_urban_rural +
                       day_mean + sex))

#Wald test
D1(fit.var, fit.novar) #p=1.7*10^-7 => edu is recommended to include in the model

#BIC comparison
BIC.var <- fit.var$analyses %>%
  sapply(BIC) 

BIC.novar <- fit.novar$analyses %>%
  sapply(BIC)

sum(BIC.var > BIC.novar) #BIC with edu is bigger in 1/100 cases => recommended to include in the model
```

Education should be included in the app_sd + syst.bp model.

For avg+ syst.bp:

```{r}

fit.var <- with(pmm_proj_multiimp, 
                  lm(c66_46_avrsybp ~ y1_avg_temp +  
                       c66_46_alc + 
                       c66_46_c_bmi + c66_46_ht_med +
                       c66_46_q1_ht + c66_46_smok +
                       c66_46_urban_rural + day_mean +
                       sex + c66_46_edu))
fit.novar <- with(pmm_proj_multiimp, 
                    lm(c66_46_avrsybp ~ y1_avg_temp +  
                       c66_46_alc + 
                       c66_46_c_bmi + c66_46_ht_med +
                       c66_46_q1_ht + c66_46_smok +
                       c66_46_urban_rural + day_mean +
                       sex))

#Wald test
D1(fit.var, fit.novar) #p=5*10^-5 => edu is recommended to include in the model

#BIC comparison
BIC.var <- fit.var$analyses %>%
  sapply(BIC) 

BIC.novar <- fit.novar$analyses %>%
  sapply(BIC)

sum(BIC.var > BIC.novar) #BIC with edu is bigger in 11/100 cases => recommended to include in the model
```

Education should be included in the avg+ syst.bp model.

For sd+ syst.bp:

```{r}

fit.var <- with(pmm_proj_multiimp, 
                  lm(c66_46_avrsybp ~ y1_sd_temp +  
                       c66_46_alc + c66_46_c_bmi +
                       c66_46_ht_med + c66_46_q1_ht +
                       c66_46_smok + c66_46_urban_rural +
                       day_mean + sex +
                       c66_46_edu))
fit.novar <- with(pmm_proj_multiimp, 
                    lm(c66_46_avrsybp ~ y1_sd_temp +  
                       c66_46_alc + c66_46_c_bmi +
                       c66_46_ht_med + c66_46_q1_ht +
                       c66_46_smok + c66_46_urban_rural +
                       day_mean + sex))

#Wald test
D1(fit.var, fit.novar) #p=1.6*10^-6 => edu is recommended to include in the model

#BIC comparison
BIC.var <- fit.var$analyses %>%
  sapply(BIC) 

BIC.novar <- fit.novar$analyses %>%
  sapply(BIC)

sum(BIC.var > BIC.novar) #BIC with edu is bigger in 1/100 cases => recommended to include in the model
```

Education should be included in the sd+ syst.bp model.

##### Smoking

For app_avg + syst.bp:

```{r}

fit.var <- with(pmm_proj_multiimp, 
                  lm(c66_46_avrsybp ~ y1_app_temp_avg_46 +  
                       c66_46_alc + 
                       c66_46_c_bmi + c66_46_edu +
                       c66_46_ht_med + c66_46_q1_ht +
                       c66_46_urban_rural + day_mean +
                       sex + c66_46_smok))
fit.novar <- with(pmm_proj_multiimp, 
                    lm(c66_46_avrsybp ~ y1_app_temp_avg_46 +  
                       c66_46_alc + 
                       c66_46_c_bmi + c66_46_edu +
                       c66_46_ht_med + c66_46_q1_ht +
                       c66_46_urban_rural + day_mean +
                       sex))

#Wald test
D1(fit.var, fit.novar) #p=1.3*10^-8 => smok is recommended to include in the model

#BIC comparison
BIC.var <- fit.var$analyses %>%
  sapply(BIC) 

BIC.novar <- fit.novar$analyses %>%
  sapply(BIC)

sum(BIC.var > BIC.novar) #BIC with smok is bigger in 0/100 cases => recommended to include in the model
```

Smoking should be included in the app_avg + syst.bp model.

For app_sd + syst.bp:

```{r}

fit.var <- with(pmm_proj_multiimp, 
                  lm(c66_46_avrsybp ~ y1_app_temp_sd_46 +  
                       c66_46_alc + 
                       c66_46_c_bmi + c66_46_edu +
                       c66_46_ht_med + c66_46_q1_ht +
                       c66_46_urban_rural + day_mean +
                       sex + c66_46_smok))
fit.novar <- with(pmm_proj_multiimp, 
                    lm(c66_46_avrsybp ~ y1_app_temp_sd_46 +  
                       c66_46_alc + 
                       c66_46_c_bmi + c66_46_edu +
                       c66_46_ht_med + c66_46_q1_ht +
                       c66_46_urban_rural + day_mean +
                       sex))

#Wald test
D1(fit.var, fit.novar) #p=3.3*10^-8 => smok is recommended to include in the model

#BIC comparison
BIC.var <- fit.var$analyses %>%
  sapply(BIC) 

BIC.novar <- fit.novar$analyses %>%
  sapply(BIC)

sum(BIC.var > BIC.novar) #BIC with smok is bigger in 0/100 cases => recommended to include in the model
```

Smoking should be included in the app_sd + syst.bp model.

For avg + syst.bp:

```{r}

fit.var <- with(pmm_proj_multiimp, 
                  lm(c66_46_avrsybp ~ y1_avg_temp +  
                       c66_46_alc + 
                       c66_46_c_bmi + c66_46_edu +
                       c66_46_ht_med + c66_46_q1_ht +
                       c66_46_urban_rural + day_mean +
                       sex + c66_46_smok))
fit.novar <- with(pmm_proj_multiimp, 
                    lm(c66_46_avrsybp ~ y1_avg_temp +  
                       c66_46_alc + 
                       c66_46_c_bmi + c66_46_edu +
                       c66_46_ht_med + c66_46_q1_ht +
                       c66_46_urban_rural + day_mean +
                       sex))

#Wald test
D1(fit.var, fit.novar) #p=1.5*10^-8 => edu is recommended to include in the model

#BIC comparison
BIC.var <- fit.var$analyses %>%
  sapply(BIC) 

BIC.novar <- fit.novar$analyses %>%
  sapply(BIC)

sum(BIC.var > BIC.novar) #BIC with edu is bigger in 0/100 cases => recommended to include in the model
```

Smoking should be included in the avg + syst.bp model.

For sd + syst.bp:

```{r}

fit.var <- with(pmm_proj_multiimp, 
                  lm(c66_46_avrsybp ~ y1_sd_temp +  
                       c66_46_alc + 
                       c66_46_c_bmi + c66_46_edu +
                       c66_46_ht_med + c66_46_q1_ht +
                       c66_46_urban_rural + day_mean +
                       sex + c66_46_smok))
fit.novar <- with(pmm_proj_multiimp, 
                    lm(c66_46_avrsybp ~ y1_sd_temp +  
                       c66_46_alc + 
                       c66_46_c_bmi + c66_46_edu +
                       c66_46_ht_med + c66_46_q1_ht +
                       c66_46_urban_rural + day_mean +
                       sex))

#Wald test
D1(fit.var, fit.novar) #p=3.3*10^-8 => smok is recommended to include in the model

#BIC comparison
BIC.var <- fit.var$analyses %>%
  sapply(BIC) 

BIC.novar <- fit.novar$analyses %>%
  sapply(BIC)

sum(BIC.var > BIC.novar) #BIC with smok is bigger in 0/100 cases => recommended to include in the model
```

Smoking should be included in the sd + syst.bp model.

##### Alcohol

For app_avg + syst.bp:

```{r}
fit.var <- with(pmm_proj_multiimp, 
                  lm(c66_46_avrsybp ~ y1_app_temp_avg_46  +  
                       c66_46_smok + 
                       c66_46_c_bmi + c66_46_edu +
                       c66_46_ht_med + c66_46_q1_ht +
                       c66_46_urban_rural + day_mean +
                       sex + c66_46_alc))
fit.novar <- with(pmm_proj_multiimp, 
                    lm(c66_46_avrsybp ~ y1_app_temp_avg_46  +  
                       c66_46_smok + 
                       c66_46_c_bmi + c66_46_edu +
                       c66_46_ht_med + c66_46_q1_ht +
                       c66_46_urban_rural + day_mean +
                       sex))

#Wald test
D1(fit.var, fit.novar) #p=8*10^-8 => edu is recommended to include in the model

#BIC comparison
BIC.var <- fit.var$analyses %>%
  sapply(BIC) 

BIC.novar <- fit.novar$analyses %>%
  sapply(BIC)

sum(BIC.var > BIC.novar) #BIC with edu is bigger in 20/100 cases => recommended to include in the model
```

Alcohol should be included in the app_avg + syst.bp model.

For app_sd + syst.bp:

```{r}
fit.var <- with(pmm_proj_multiimp, 
                  lm(c66_46_avrsybp  ~ y1_app_temp_sd_46    +  
                       c66_46_c_bmi +
                       c66_46_edu + c66_46_ht_med +
                       c66_46_q1_ht + 
                       c66_46_smok + c66_46_urban_rural +
                       sex + day_mean + c66_46_alc))
fit.novar <- with(pmm_proj_multiimp, 
                    lm(c66_46_avrsybp  ~ y1_app_temp_sd_46    +  
                       c66_46_c_bmi +
                       c66_46_edu + c66_46_ht_med +
                       c66_46_q1_ht + 
                       c66_46_smok + c66_46_urban_rural +
                       sex + day_mean))

#Wald test
D1(fit.var, fit.novar) #p=5.6*10^-7 => day_mean is recommended to include in the model

#BIC comparison
BIC.var <- fit.var$analyses %>%
  sapply(BIC) 

BIC.novar <- fit.novar$analyses %>%
  sapply(BIC)

sum(BIC.var > BIC.novar) #BIC with day_mean is bigger in 38/100 cases => recommended to include in the model
```

Alcohol should be included in the app_sd + syst.bp model.

For avg + syst.bp:

```{r}
fit.var <- with(pmm_proj_multiimp, 
                  lm(c66_46_avrsybp ~ y1_avg_temp  +  
                       c66_46_smok + 
                       c66_46_c_bmi + c66_46_edu +
                       c66_46_ht_med + c66_46_q1_ht +
                       c66_46_urban_rural + day_mean +
                       sex + c66_46_alc))
fit.novar <- with(pmm_proj_multiimp, 
                    lm(c66_46_avrsybp ~ y1_avg_temp  +  
                       c66_46_smok + 
                       c66_46_c_bmi + c66_46_edu +
                       c66_46_ht_med + c66_46_q1_ht +
                       c66_46_urban_rural + day_mean +
                       sex))

#Wald test
D1(fit.var, fit.novar) #p=1^-7 => edu is recommended to include in the model

#BIC comparison
BIC.var <- fit.var$analyses %>%
  sapply(BIC) 

BIC.novar <- fit.novar$analyses %>%
  sapply(BIC)

sum(BIC.var > BIC.novar) #BIC with edu is bigger in 20/100 cases => recommended to include in the model
```

Alcohol should be included in the avg + syst.bp model.

For sd + syst.bp:

```{r}
fit.var <- with(pmm_proj_multiimp, 
                  lm(c66_46_avrsybp  ~ y1_sd_temp    +  
                       c66_46_c_bmi +
                       c66_46_edu + c66_46_ht_med +
                       c66_46_q1_ht + 
                       c66_46_smok + c66_46_urban_rural +
                       sex + day_mean + c66_46_alc))
fit.novar <- with(pmm_proj_multiimp, 
                    lm(c66_46_avrsybp  ~ y1_sd_temp    +  
                       c66_46_c_bmi +
                       c66_46_edu + c66_46_ht_med +
                       c66_46_q1_ht + 
                       c66_46_smok + c66_46_urban_rural +
                       sex + day_mean))

#Wald test
D1(fit.var, fit.novar) #p=5.7*10^-7 => day_mean is recommended to include in the model

#BIC comparison
BIC.var <- fit.var$analyses %>%
  sapply(BIC) 

BIC.novar <- fit.novar$analyses %>%
  sapply(BIC)

sum(BIC.var > BIC.novar) #BIC with day_mean is bigger in 37/100 cases => recommended to include in the model
```

Alcohol should be included in the sd + syst.bp model.

```{r}
fit.var <- with(pmm_proj_multiimp, 
                  lm(c66_46_avrdibp  ~ y1_avg_temp +  
                       c66_46_alc + c66_46_c_bmi +
                       c66_46_ht_med +
                       c66_46_q1_ht + 
                       c66_46_urban_rural + 
                       day_mean + sex))
fit.novar <- with(pmm_proj_multiimp, 
                    lm(c66_46_avrdibp  ~ y1_avg_temp +  
                       c66_46_alc + c66_46_c_bmi +
                       c66_46_ht_med +
                       c66_46_q1_ht + 
                       c66_46_urban_rural + 
                       sex))

#Wald test
D1(fit.var, fit.novar) #p=5.7*10^-7 => day_mean is recommended to include in the model

#BIC comparison
BIC.var <- fit.var$analyses %>%
  sapply(BIC) 

BIC.novar <- fit.novar$analyses %>%
  sapply(BIC)

sum(BIC.var > BIC.novar) #BIC with day_mean is bigger in 37/100 cases => recommended to include in the model


```

##### Conclusion about optimal models

SYS: APP_AVG: CHECK ALC(+), EDU(+), SMOK (+) APP_SD:CHECK ALC(+), EDU(+), SMOK (+) AVG: CHECK ALC(+), EDU(+), SMOK (+) SD:CHECK ALC(+), EDU(+), SMOK (+)

DIAS: APP_AVG: NO PHYS, NO SMOK, NO EDU. CHECK DAY_MEAN APP_SD: NO EDU, NO PHYS, NO SMOK, NO DAY_MEAN AVG: NO PHYS, NO SMOK, NO EDU. CHECK DAY_MEAN SD: NO EDU, NO PHYS, NO SMOK, NO DAY_MEA

In the end, we have the following optimal models:

Full model:

```{r}
c66_46_avrsybp ~y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean
```

###### **Systolic blood pressure**

1\) Systolic blood pressure + apparent average temp (no phys, no diab, no hpcl, no season, no occup):

```{r}
c66_46_avrsybp ~ y1_app_temp_avg_46  +  
                       c66_46_smok + 
                       c66_46_c_bmi + c66_46_edu +
                       c66_46_ht_med + c66_46_q1_ht +
                       c66_46_urban_rural + day_mean +
                       sex + c66_46_alc
```

2\) Systolic blood pressure + apparent standard deviation (no phys, no diab, no hpcl, no season, no occup, no ap_age):

```{r}
c66_46_avrsybp  ~ y1_app_temp_sd_46    +  
                       c66_46_c_bmi +
                       c66_46_edu + c66_46_ht_med +
                       c66_46_q1_ht + 
                       c66_46_smok + c66_46_urban_rural +
                       sex + day_mean + c66_46_alc
```

3\) Systolic blood pressure + average temperature (no phys, no diab, no, hpcl, no season, no occup):

```{r}
c66_46_avrsybp ~ y1_avg_temp  +  
                       c66_46_smok + 
                       c66_46_c_bmi + c66_46_edu +
                       c66_46_ht_med + c66_46_q1_ht +
                       c66_46_urban_rural + day_mean +
                       sex + c66_46_alc
```

4\) Systolic blood pressure + standard deviation (no phys, no diab, no hpcl, no season, no occup, no ap_age):

```{r}
c66_46_avrsybp  ~ y1_sd_temp  +  
                       c66_46_c_bmi +
                       c66_46_edu + c66_46_ht_med +
                       c66_46_q1_ht + 
                       c66_46_smok + c66_46_urban_rural +
                       sex + day_mean + c66_46_alc
```

###### **Diastolic blood pressure**

1\) Diastolic blood pressure + apparent average temp :

```{r}
c66_46_avrdibp  ~ y1_app_temp_avg_46 +  
                       c66_46_alc + c66_46_c_bmi +
                       c66_46_ht_med +
                       c66_46_q1_ht + 
                       c66_46_urban_rural + 
                       day_mean + sex  
```

2\) Diastolic blood pressure + apparent standard deviation :

```{r}
c66_46_avrdibp ~ y1_app_temp_sd_46  +  
                       c66_46_alc + c66_46_c_bmi +
                       c66_46_ht_med + 
                       c66_46_q1_ht +
                       c66_46_urban_rural +
                       sex 
```

3\) Diastolic blood pressure + average temperature:

```{r}
c66_46_avrdibp  ~ y1_avg_temp +  
                       c66_46_alc + c66_46_c_bmi +
                       c66_46_ht_med +
                       c66_46_q1_ht + 
                       c66_46_urban_rural + 
                       sex   
```

4\) Diastolic blood pressure + standard deviation :

```{r}
c66_46_avrdibp  ~ y1_sd_temp +  
                       c66_46_alc + c66_46_c_bmi +
                       c66_46_ht_med +
                       c66_46_q1_ht +
                       c66_46_urban_rural + 
                       sex 
```


### 2.1 Systolic blood pressure

#### 2.1.1 Apparent average temperature

##### 2.1.1.1 Unadjusted model (univariate)

Pool estimates over all datasets with 1 exposure variable:

```{r}
imputed_dataset = bp_df_plot

# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrsybp ~ y1_app_temp_avg_46))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)
pooled_results
```

Pooled b1 for app.temp is -1.05 (p\<0.001) \[-1.31 : -0.80\]. SE=0.13

###### Model diagnostics.

**Constant variance assumption**

We can diagnose the appropriateness of the constant variance assumption by examining a plot of residuals vs. fitted values (and, if we want to dig deeper, plots of residuals vs. each predictor) using car::residualPlots(). If the constant variance assumption is met, the spread of the points in the vertical direction should not vary much as you move in the horizontal direction.

```{r}
lm_model = lm(c66_46_avrsybp ~ y1_app_temp_avg_46, data = imputed_dataset)

#residual plot of predictors vs resid and fitted vs resid = constant variance
car::residualPlots(lm_model,
                   pch=20, col="gray",
                   fitted = T,
                   ask = F, layout = c(2,2),
                   tests = F, quadratic = F)

ols_test_breusch_pagan(lm_model)
```

We see constant variance almost everywhere except the range over 124-128 with multiple positive outliers. However, the violation is not serious in this case, so we can say that the assumption is satisfied.

**Linearity assumption**

The linearity assumption requires that the Y vs. X relationship be linear when holding all other predictors fixed. A residual plot that takes that into account and is effective at diagnosing non-linearity is the component-plus-residual plot (CR plot), also known as a partial-residual plot. We will create a CR plot for each continuous predictor using the car::crPlots().

If there were no linear association between Y and X after adjusting for all other predictors in the model, the dashed line would be a horizontal line at 0. The solid line is a smoother that relaxes the linearity assumption. In each panel, if the solid line falls exactly on top of the dashed line, then the linearity assumption is perfectly met for that predictor.

In each panel, the raw predictor values are plotted on the horizontal axis. The vertical axis plots the residuals plus the specific contribution of X , adjusted for all the other predictors in the model,\
( βjxij) added back in (the component). The dashed line is the linear regression of the component + residual vs. X . If there were no linear association between Y and X after adjusting for all other predictors in the model, the dashed line would be a horizontal line at 0. The solid line is a smoother that relaxes the linearity assumption. In each panel, if the solid line falls exactly on top of the dashed line, then the linearity assumption is perfectly met for that predictor. In this example, age is linear, but there seems to be a slight non-linearity for waist circumference.

```{r}
car::crPlots(lm_model, terms = ~ y1_app_temp_avg_46,
             pch=20, col="gray",
             smooth = list(smoother=car::gamLine), ask = F)
```

Non-linearity does happen because of the high leveraged values in the range of -5 - -3. In other, plot looks linear.

**Normality assumption**

The assumption is that the errors are normally distributed. We do not actually observe the errors, but we do observe the residuals and so we use those to diagnose normality (as well as other assumptions). There are formal tests of normality (e.g., the Shapiro-Wilk test); however, they are not needed in large samples since violation of the normality assumption will not impact inferences and they lack power in small samples. Instead, use a visual diagnosis.

```{r}
source("Functions_rmph.R")

check_normality(lm_model)
```

In the **left panel** (histogram of residuals) the solid line provides a smoothed estimate of the distribution of the observed residuals without making any assumption about the shape of the distribution. The dashed line, however, is the best fit normal curve. The closer these two curves are to each other, the better the normality assumption is met. We see that observed residuals almost perfectly normally distributed.

In the **right panel** (a QQ plot), the ordered values of the n residuals are plotted against the corresponding n quantiles of the standard normal distribution. The closer the points are to the dashed line, the better the normality assumption is met. In this example, there is a lack of normality in the tails of the distribution. Since the sample size is huge (n = 5661, obs \> predictors), non-normality unlikely impact the results.

**Effect of imputation on estimates (riv, lambda, fmi)**

λ (lambda)- the proportion of the variation attributable to the missing data. It is equal to zero if the missing data do not add extra variation to the sampling variance, an exceptional situation that can occur only if we can perfectly re-create the missing data. The maximum value is equal to 1, which occurs only if all variation is caused by the missing data. This is equally unlikely to occur in practice since it means that there is no information at all. If λ is high, say λ\>0.5 , the influence of the imputation model on the final result is larger than that of the complete-data model. This value can be interpreted as the proportion of variation in the parameter of interest due to the missing data.

riv - relative increase in variance due to nonresponse. This value can be interpreted as the proportional increase in the sampling variance of the parameter of interest that is due to the missing data.

γ (epsilon) - the fraction of information missing due to nonresponse. The interpretations of γ and λ are similar, but γ is adjusted for the finite number of imputations. The literature often confuses γ and λ , and erroneously labels λ as the fraction of missing information. The values of λ and γ are almost identical for large ν , but they could notably differ for low ν .

The quantities λ , riv and γ as well as their multivariate analogues ¯λ and ¯r are indicators of the severity of the missing data problem. Fractions of missing information up to 0.2 can be interpreted as “modest,” 0.3 as “moderately large” and 0.5 as “high”. High values indicate a difficult problem in which the final statistical inferences are highly dependent on the way in which the missing data were handled. Note that estimates of λ , riv and γ may be quite variable for low m.

```{r}
pooled_results
```

riv = 0, lambda = 0, fmi \~ 0 because of 1 variable only that does not have NA values.

###### Pooled Radj and BIC for linear model:

```{r}
pool.r.squared(lm_models, adjusted = T)

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(lm_models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(lm_models$analyses[[i]])$BIC,
         Radj = glance(lm_models$analyses[[i]])$adj.r.squared)
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = fisher.trans(Radj)) %>%
  summarise(pooled_Radj = fisher.backtrans(mean(Radj_trans)),  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

Pooled Radj is 0.01 which can be considered as very small (1% of variance are explained) and BIC 47373.

###### Visualizing the adjusted relationships

The relationship we would like to see is the relationship between the outcome and each predictor after removing the effect of all the other predictors. We can do this using an **added variable plot.** The vertical axis is labeled “outcome \| others” which corresponds to the outcome given all the predictors other than the one on the horizontal axis. The horizontal axis on each plot tells you to which predictor each added variable plot corresponds. For example, “predictor \| others” corresponds to predictor adjusted for all the other predictors. 

```{r}
car::avPlots(lm_model, terms = c66_46_avrsybp ~ y1_app_temp_avg_46, 
             col = "darkgray", 
             col.lines = "blue",
             pch = 16,
             id = F, ask = F)
```

##### 2.1.1.2 Minimally adjusted model

Pool estimates over all datasets with minimal set of variables:

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrsybp ~ y1_app_temp_avg_46 + c66_46_edu +
                c66_46_occup + sex + 
                c66_46_urban_rural + c66_46_season))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)
```

Pooled b1 for app.temp is -0.82 (p\<0.001) \[-1.06 : -0.57\]. The effect size decreased by 0.28.

**Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:** The type = 3 option specifies that we want a Type III test, which is a test of a predictor after adjusting for all other terms in the model

```{r}
mi.anova(pmm_proj_multiimp,
         "c66_46_avrsybp ~ y1_app_temp_avg_46 + c66_46_edu + 
         c66_46_occup + sex + c66_46_ht_med", type = 3)
```

R2=0.12. All categorical variables are stat. significant except occup status (p=0.12)

###### Model diagnostics.

Number of imputations

```{r}
howManyImputations::how_many_imputations(lm_models)
```

**Constant variance assumption**

```{r}
lm_model = lm(c66_46_avrsybp ~ y1_app_temp_avg_46 + c66_46_edu +
                c66_46_occup + sex + 
                c66_46_ht_med, data = imputed_dataset)

#residual plot of predictors vs resid and fitted vs resid = constant variance
car::residualPlots(lm_model,
                   pch=20, col="gray",
                   fitted = T,
                   ask = F, layout = c(2,2),
                   tests = F, quadratic = F)

ols_test_breusch_pagan(lm_model)
```

We see constant variance almost everywhere except the range over 115-123 vs 132-140 where there is some difference. However, the violation is not serious in this case, so we can say that the assumption is satisfied.

**Linearity assumption**

```{r}
car::crPlots(lm_model, terms = ~ y1_app_temp_avg_46 + c66_46_edu +
                c66_46_occup + sex + 
                c66_46_ht_med,
             pch=20, col="gray",
             smooth = list(smoother=car::gamLine), , ask = F)
```

Non-linearity does happen because of the high leveraged values in the range of -5 - -3. In other, plot looks linear.

**Normality assumption**

```{r}
source("Functions_rmph.R")

check_normality(lm_model)
```

In the **left panel** (histogram of residuals) we see that observed residuals almost perfectly normally distributed.

In the **right panel** (a QQ plot) there is a lack of normality in the tails of the distribution. Since the sample size is huge (n = 5661, obs \> predictors), non-normality unlikely impact the results.

**Effect of imputation on estimates (riv, lambda, fmi)**

```{r}
pooled_results
```

All variables have riv (relative increase in variance due to nonresponse), lambda (the proportion of the variation attributable to the missing data), fmi (the fraction of information missing due to nonresponse) less than 0.2 which can be interpreted as modest.

###### Pooled Radj and BIC for linear model:

```{r}
pool.r.squared(lm_models, adjusted = T)

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(lm_models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(lm_models$analyses[[i]])$BIC,
         Radj = glance(lm_models$analyses[[i]])$adj.r.squared)
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = fisher.trans(Radj)) %>%
  summarise(pooled_Radj = fisher.backtrans(mean(Radj_trans)),  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

Pooled Radj is 0.13, pooled BIC is 46693.

###### Visualizing the adjusted relationships

```{r}
car::avPlots(lm_model, terms = c66_46_avrsybp ~ y1_app_temp_avg_46 + 
               c66_46_edu + c66_46_occup + sex + 
                c66_46_ht_med, 
             col = "darkgray", 
             col.lines = "blue",
             pch = 16,
             id = F, ask = F)
```

Negative linear relationship.

##### 2.1.1.3 Optimized model

Pool estimates over all datasets with minimal set of variables:

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrsybp ~ y1_app_temp_avg_46  +  
                       c66_46_smok + 
                       c66_46_c_bmi + c66_46_edu +
                       c66_46_ht_med + c66_46_q1_ht +
                       c66_46_urban_rural + day_mean +
                       sex + c66_46_alc))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)
```

Pooled b1 for app.temp is -0.85 (p\<0.001) \[-1.1 : -0.6\]. The effect size decreased by 0.28.

**Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
mi.anova(pmm_proj_multiimp,
         "c66_46_avrsybp ~ y1_app_temp_avg_46 +  
                       c66_46_alc +  +
                       c66_46_c_bmi + c66_46_ht_med +
                       c66_46_q1_ht + c66_46_smok +
                       c66_46_urban_rural + day_mean +
                       sex + c66_46_edu", type = 3)
```

R2=0.19. All categorical variables are stat. significant.

###### Model diagnostics.

Number of imputations

```{r}
howManyImputations::how_many_imputations(lm_models)
```

6 imputations needed.

**Constant variance assumption**

```{r}
imputed_dataset = bp_df_plot
lm_model = lm(c66_46_avrsybp ~ y1_app_temp_avg_46 +  
                       c66_46_alc +  +
                       c66_46_c_bmi + c66_46_ht_med +
                       c66_46_q1_ht + c66_46_smok +
                       c66_46_urban_rural + day_mean +
                       sex + c66_46_edu, data = imputed_dataset)

#residual plot of predictors vs resid and fitted vs resid = constant variance
car::residualPlots(lm_model,
                   pch=20, col="gray",
                   fitted = T,
                   ask = F, layout = c(2,2),
                   tests = F, quadratic = F)

ols_test_breusch_pagan(lm_model)
```

We see decrease in variance from 140. However, the violation is not serious? in this case, so we can say that the assumption is satisfied.

**Linearity assumption**

```{r}
car::crPlots(lm_model, terms = ~ y1_app_temp_avg_46 +  
                       c66_46_alc +  +
                       c66_46_c_bmi + c66_46_ht_med +
                       c66_46_q1_ht + c66_46_smok +
                       c66_46_urban_rural + day_mean +
                       sex + c66_46_edu,
             pch=20, col="gray",
             smooth = list(smoother=car::gamLine), ask = F)
```

Non-linearity does happen because of the high leveraged values in the range of -5 - -3. In other, plot looks linear.

**Normality assumption**

```{r}
source("Functions_rmph.R")

check_normality(lm_model)
```

In the **left panel** (histogram of residuals) we see that observed residuals almost perfectly normally distributed.

In the **right panel** (a QQ plot) there is a lack of normality in the tails of the distribution. Since the sample size is huge (n = 5661, obs \> predictors), non-normality unlikely impact the results.

**Effect of imputation on estimates (riv, lambda, fmi)**

```{r}
pooled_results
```

All variables have riv (relative increase in variance due to nonresponse), lambda (the proportion of the variation attributable to the missing data), fmi (the fraction of information missing due to nonresponse) less than 0.2 which can be interpreted as modest.

###### Pooled Radj and BIC for linear model:

```{r}
pool.r.squared(lm_models, adjusted = T)

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(lm_models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(lm_models$analyses[[i]])$BIC,
         Radj = glance(lm_models$analyses[[i]])$adj.r.squared)
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = fisher.trans(Radj)) %>%
  summarise(pooled_Radj = fisher.backtrans(mean(Radj_trans)),  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

Pooled Radj is 0.25, pooled BIC is 45898.

###### Visualizing the adjusted relationships

```{r}
car::avPlots(lm_model, terms = c66_46_avrsybp ~ y1_app_temp_avg_46 +  
                       c66_46_alc +  +
                       c66_46_c_bmi + c66_46_ht_med +
                       c66_46_q1_ht + c66_46_smok +
                       c66_46_urban_rural + day_mean +
                       sex + c66_46_edu, 
             col = "darkgray", 
             col.lines = "blue",
             pch = 16,
             id = F, ask = F)
```

Negative linear relationship.

##### 2.1.1.4 Fully adjusted

Pool estimates over all datasets with minimal set of variables:

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrsybp ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)
```

Pooled b1 for app.temp is -0.88 (p\<0.001) \[-1.14 : -0.63\]. The effect size decreased by 0.17. c66_46_urban_rural, c66_46_edu, c66_46_q1_ht, sex in combination drop the effect size from 1.05 to -0.61. c66_46_alc and increase the effect size to -1.42. Standard error is 0.13 and did not change.

```{r}
#| eval: false
#| include: false
###########################
## 0) Load Required Packages
###########################
library(mice)
library(sandwich)

########################################
## 1) Define a Custom LM-Fitting Function
########################################
fit_lm_robust <- function(formula, ...) {
  # Fit a normal linear model:
  fit <- lm(formula, ...)
  
  # Compute robust variance-covariance matrix
  fit$vcov.robust <- vcovHC(fit, type = "HC3")
  
  # Assign a new class so we can define custom methods
  class(fit) <- c("myrobustlm", class(fit))
  return(fit)
}

##################################
## 2) Define Custom S3 Methods
##################################
# MICE's pool() calls coef() and vcov() on each fitted model. We override them so that
# the robust covariance is used, and so that coef() calls the standard 'lm' method
# (avoiding infinite recursion).

vcov.myrobustlm <- function(object, ...) {
  # This is safe because it just returns the stored matrix, no recursion
  object$vcov.robust
}

coef.myrobustlm <- function(object, ...) {
  # Temporarily remove the "myrobustlm" class so that
  # 'coef()' dispatches to the regular 'lm' method.
  old_class <- class(object)
  on.exit(class(object) <- old_class) # restore on exit
  
  # Drop "myrobustlm" from the class vector
  class(object) <- setdiff(old_class, "myrobustlm")
  
  # Now call the next appropriate method (for "lm")
  coef(object, ...)
}

##############################################
## 3) Use with() to Fit the Robust Model on 
##    Each Imputed Dataset in 'pmm_proj_multiimp'
##############################################
lm_models_robust <- with(
  data = pmm_proj_multiimp,
  expr = fit_lm_robust(
    c66_46_avrsybp ~ 
      y1_app_temp_avg_46 +
      c66_46_q1_ht + c66_46_c_bmi + 
      c66_46_ht_med + c66_46_edu +  
      c66_46_smok + c66_46_alc +
      c66_46_phys + c66_46_occup + 
       + sex + 
      c66_46_diab + c66_46_hpcl + 
      c66_46_season + c66_46_urban_rural +
      day_mean
  )
)

#####################################
## 4) Pool the Results with Rubin's Rules
#####################################
pooled_results <- pool(lm_models_robust)

# Look at the summary of pooled coefficients and robust standard errors:
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

```

**Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
mi.anova(pmm_proj_multiimp,
         "c66_46_avrsybp ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean", type = 3)
```

R2=0.18. All categorical variables are stat. significant except phys, occup, diab, hpcl, day_mean. An interesting finding is that the season is stat. significant, however the day_mean is not. At the same time season was not selected in any of the optimized model, but day_mean was.

###### Model diagnostics.

Number of imputations

```{r}
howManyImputations::how_many_imputations(lm_models)
```

7 imputations needed.

**Constant variance assumption**

```{r}
imputed_dataset = bp_df_plot
lm_model = lm(c66_46_avrsybp ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = imputed_dataset)

#residual plot of predictors vs resid and fitted vs resid = constant variance
car::residualPlots(lm_model,
                   pch=20, col="gray",
                   fitted = T,
                   ask = F, layout = c(2,2),
                   tests = F, quadratic = F)

ols_test_breusch_pagan(lm_model)

# Robust standard errors
x1 = coeftest(lm_model, vcov = vcovHC(lm_model, type = "HC3"))
coefci(lm_model, vcov. = vcovHC(lm_model, type = 'HC3'))

ols_test_breusch_pagan(lm_model)

plot(lm_model, which = 5) #high leverage + outliers = violation of constant variace (hetero)
```

We see decrease in variance from 140. However, the violation is not serious? in this case, so we can say that the assumption is satisfied.

**Linearity assumption**

```{r}
car::crPlots(lm_model, terms = ~ y1_app_temp_avg_46,
             pch=20, col="gray",
             smooth = list(smoother=car::gamLine), ask = F)
```

Non-linearity does happen because of the high leveraged values in the range of -5 - -3. In other, plot looks linear.

**Normality assumption**

```{r}
source("Functions_rmph.R")

check_normality(lm_model)
```

In the **left panel** (histogram of residuals) we see that observed residuals almost perfectly normally distributed.

In the **right panel** (a QQ plot) there is a lack of normality in the tails of the distribution. Since the sample size is huge (n = 5661, obs \> predictors), non-normality unlikely impact the results.

**Effect of imputation on estimates (riv, lambda, fmi)**

```{r}
pooled_results
```

All variables have riv (relative increase in variance due to nonresponse), lambda (the proportion of the variation attributable to the missing data), fmi (the fraction of information missing due to nonresponse) less than 0.2 which can be interpreted as modest.

###### Pooled Radj and BIC for linear model:

```{r}
pool.r.squared(lm_models, adjusted = T)

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(lm_models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(lm_models$analyses[[i]])$BIC,
         Radj = glance(lm_models$analyses[[i]])$adj.r.squared)
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = fisher.trans(Radj)) %>%
  summarise(pooled_Radj = fisher.backtrans(mean(Radj_trans)),  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

Pooled Radj is 0.25, pooled BIC is 45957.

###### Visualizing the adjusted relationships

```{r}
car::avPlots(lm_model, terms = c66_46_avrsybp ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, 
             col = "darkgray", 
             col.lines = "blue",
             pch = 16,
             id = F, ask = F)
```

Negative linear relationship.

#### 2.1.2 Apparent standard deviation of average temperature

##### 2.1.2.1 Unadjusted model (univariate)

Pool estimates over all datasets with 1 exposure variable:

```{r}
source("Functions_rmph.R")
imputed_dataset = bp_df_plot

# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrsybp ~ y1_app_temp_sd_46))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)
```

Pooled b1 for app_sd is -0.13 (p=0.61) -0.62, 0.36\]. The effect size is stat.insignificant and small. Also SE is quite high relatively to b1 (0.25).

###### Model diagnostics.

**Constant variance assumption**

```{r}
lm_model = lm(c66_46_avrsybp ~ y1_app_temp_sd_46, data = imputed_dataset)

#residual plot of predictors vs resid and fitted vs resid = constant variance
car::residualPlots(lm_model,
                   pch=20, col="gray",
                   fitted = T,
                   ask = F, layout = c(2,2),
                   tests = F, quadratic = F)

ols_test_breusch_pagan(lm_model)
```

Variance started decreasing from 125.4. The violation might affect the results. A violation of the constant variance assumption results in inaccurate confidence intervals and p-values, even in large samples, although regression coefficient estimates will still be unbiased.

**Linearity assumption**

```{r}
car::crPlots(lm_model, terms = ~ y1_app_temp_sd_46,
             pch=20, col="gray",
             smooth = list(smoother=car::gamLine), ask = F)
```

The relationship is linear.

**Normality assumption**

```{r}
source("Functions_rmph.R")

check_normality(lm_model)
```

In the **left panel** (histogram of residuals) we see that observed residuals almost perfectly normally distributed.

In the **right panel** (a QQ plot) there is a lack of normality in the tails of the distribution. Since the sample size is huge (n = 5661, obs \> predictors), non-normality unlikely impact the results.

**Effect of imputation on estimates (riv, lambda, fmi)**

```{r}
pooled_results
```

riv = 0, lambda = 0, fmi \~ 0 because of 1 variable only that does not have NA values.

###### Pooled Radj and BIC for linear model:

```{r}
pool.r.squared(lm_models, adjusted = F)

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(lm_models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(lm_models$analyses[[i]])$BIC,
         Radj = glance(lm_models$analyses[[i]])$r.squared)
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = fisher.trans(Radj)) %>%
  summarise(pooled_Radj = fisher.backtrans(mean(Radj_trans)),  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

Pooled R=0.0005 and Radj is-0.00013 which can be considered as almost zero and BIC 47437.

###### Visualizing the adjusted relationships

```{r}
car::avPlots(lm_model, terms = c66_46_avrsybp ~ y1_app_temp_sd_46, 
             col = "darkgray", 
             col.lines = "blue",
             pch = 16,
             id = F, ask = F)
```

Horizontal line = no effect.

##### 2.1.2.2 Minimally adjusted model

Pool estimates over all datasets with minimal set of variables:

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrsybp ~ y1_app_temp_sd_46 + c66_46_edu +
                c66_46_occup + sex + 
                c66_46_urban_rural + c66_46_season))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2) 
```

Pooled b1 for app.temp is -0.26 (p=0.27) \[-0.72, 0.20\]. The effect size increased by 0.13 and p-value decreased.

**Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
mi.anova(pmm_proj_multiimp,
         "c66_46_avrsybp ~ y1_app_temp_sd_46 + c66_46_edu + 
         c66_46_occup + sex + c66_46_ht_med", type = 3)
```

R2=0.12. All categorical variables are stat. significant except occup status (p=0.09)

###### Model diagnostics.

Number of imputations

```{r}
howManyImputations::how_many_imputations(lm_models)
```

**Constant variance assumption**

```{r}
lm_model = lm(c66_46_avrsybp ~ y1_app_temp_sd_46 + c66_46_edu +
                c66_46_occup + sex + 
                c66_46_ht_med, data = imputed_dataset)

#residual plot of predictors vs resid and fitted vs resid = constant variance
car::residualPlots(lm_model,
                   pch=20, col="gray",
                   fitted = T,
                   ask = F, layout = c(2,2),
                   tests = F, quadratic = F)

ols_test_breusch_pagan(lm_model)
```

**Linearity assumption**

```{r}
car::crPlots(lm_model, terms = ~ y1_app_temp_sd_46 + c66_46_edu +
                c66_46_occup + sex + 
                c66_46_ht_med,
             pch=20, col="gray",
             smooth = list(smoother=car::gamLine), , ask = F)
```

Linear relationship.

**Normality assumption**

```{r}
source("Functions_rmph.R")

check_normality(lm_model)
```

In the **left panel** (histogram of residuals) we see that observed residuals almost perfectly normally distributed.

In the **right panel** (a QQ plot) there is a lack of normality in the tails of the distribution. Since the sample size is huge (n = 5661, obs \> predictors), non-normality unlikely impact the results.

**Effect of imputation on estimates (riv, lambda, fmi)**

```{r}
pooled_results
```

All variables have riv (relative increase in variance due to nonresponse), lambda (the proportion of the variation attributable to the missing data), fmi (the fraction of information missing due to nonresponse) less than 0.2 which can be interpreted as modest.

###### Pooled Radj and BIC for linear model:

```{r}
pool.r.squared(lm_models, adjusted = T)

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(lm_models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(lm_models$analyses[[i]])$BIC,
         Radj = glance(lm_models$analyses[[i]])$adj.r.squared)
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = fisher.trans(Radj)) %>%
  summarise(pooled_Radj = fisher.backtrans(mean(Radj_trans)),  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

Pooled Radj is 0.12, pooled BIC is 46735.

###### Visualizing the adjusted relationships

```{r}
car::avPlots(lm_model, terms = c66_46_avrsybp ~ y1_app_temp_sd_46 + 
               c66_46_edu + c66_46_occup + sex + 
                c66_46_ht_med, 
             col = "darkgray", 
             col.lines = "blue",
             pch = 16,
             id = F, ask = F)
```

Horizontal line.

##### 2.1.2.3 Optimized model

Pool estimates over all datasets with minimal set of variables:

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrsybp  ~ y1_app_temp_sd_46    +  
                       c66_46_c_bmi +
                       c66_46_edu + c66_46_ht_med +
                       c66_46_q1_ht + 
                       c66_46_smok + c66_46_urban_rural +
                       sex + day_mean + c66_46_alc))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)
```

Pooled b1 for app_sd is 0.02 (p=0.93) \[-0.41 : 0.45\]. The effect size became positive.

**Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
mi.anova(pmm_proj_multiimp,
         "c66_46_avrsybp ~ y1_app_temp_sd_46    +  
                       c66_46_c_bmi +
                       c66_46_edu + c66_46_ht_med +
                       c66_46_q1_ht + 
                       c66_46_smok + c66_46_urban_rural +
                       sex + day_mean + c66_46_alc", type = 3)
```

R2=0.19. All categorical variables are stat. significant, except app_sd.

###### Model diagnostics.

Number of imputations

```{r}
howManyImputations::how_many_imputations(lm_models)
```

6 imputations needed. **Constant variance assumption**

```{r}
imputed_dataset = bp_df_plot
lm_model = lm(c66_46_avrsybp ~ y1_app_temp_sd_46    +  
                       c66_46_c_bmi +
                       c66_46_edu + c66_46_ht_med +
                       c66_46_q1_ht + 
                       c66_46_smok + c66_46_urban_rural +
                       sex + day_mean + c66_46_alc, data = imputed_dataset)

#residual plot of predictors vs resid and fitted vs resid = constant variance
car::residualPlots(lm_model,
                   pch=20, col="gray",
                   fitted = T,
                   ask = F, layout = c(2,2),
                   tests = F, quadratic = F)

ols_test_breusch_pagan(lm_model)
```

We see decrease in variance from 140. The violation might affect the results. A violation of the constant variance assumption results in inaccurate confidence intervals and p-values, even in large samples, although regression coefficient estimates will still be unbiased.

**Linearity assumption**

```{r}
car::crPlots(lm_model, terms = ~ y1_app_temp_sd_46    +  
                       c66_46_c_bmi +
                       c66_46_edu + c66_46_ht_med +
                       c66_46_q1_ht + 
                       c66_46_smok + c66_46_urban_rural +
                       sex + day_mean + c66_46_alc,
             pch=20, col="gray",
             smooth = list(smoother=car::gamLine), ask = F)
```

Linear

**Normality assumption**

```{r}
source("Functions_rmph.R")

check_normality(lm_model)
```

In the **left panel** (histogram of residuals) we see that observed residuals almost perfectly normally distributed.

In the **right panel** (a QQ plot) there is a lack of normality in the tails of the distribution. Since the sample size is huge (n = 5661, obs \> predictors), non-normality unlikely impact the results.

**Effect of imputation on estimates (riv, lambda, fmi)**

```{r}
pooled_results
```

All variables have riv (relative increase in variance due to nonresponse), lambda (the proportion of the variation attributable to the missing data), fmi (the fraction of information missing due to nonresponse) less than 0.2 which can be interpreted as modest.

###### Pooled Radj and BIC for linear model:

```{r}
pool.r.squared(lm_models, adjusted = T)

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(lm_models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(lm_models$analyses[[i]])$BIC,
         Radj = glance(lm_models$analyses[[i]])$adj.r.squared)
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = fisher.trans(Radj)) %>%
  summarise(pooled_Radj = fisher.backtrans(mean(Radj_trans)),  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

Pooled Radj is 0.25, pooled BIC is 45934.

###### Visualizing the adjusted relationships

```{r}
car::avPlots(lm_model, terms = c66_46_avrsybp ~ y1_app_temp_sd_46    +  
                       c66_46_c_bmi +
                       c66_46_edu + c66_46_ht_med +
                       c66_46_q1_ht + 
                       c66_46_smok + c66_46_urban_rural +
                       sex + day_mean + c66_46_alc, 
             col = "darkgray", 
             col.lines = "blue",
             pch = 16,
             id = F, ask = F)
```

Horizontal line.

##### 2.1.2.4 Fully adjusted(-)

Pool estimates over all datasets with minimal set of variables:

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrsybp ~ y1_app_temp_sd_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)
```

Pooled b1 for app.temp is 0.8 (p=0.01) \[0.16, 1.44\]. The SE increased from 0.25 to 0.33 but now it is smaller than estimate. This might be due to the fact that important confounders were controlled for and now we have a estimate closed to the true value and we "unmask" true relationship. This change happens due to ap_age variable

**Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
mi.anova(pmm_proj_multiimp,
         "c66_46_avrsybp ~ y1_app_temp_sd_46 +
                    c66_46_q1_ht + c66_46_c_bmi + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean", type = 3)
```

R2=0.18. All categorical variables are stat. significant except phys, occup, diab, hpcl, day_mean. An interesting finding is that the season is stat. significant, however the day_mean is not. At the same time season was not selected in any of the optimized model, but day_mean was.

###### Model diagnostics.

Number of imputations

```{r}
howManyImputations::how_many_imputations(lm_models)
```

6 imputations needed.

**Constant variance assumption**

```{r}
imputed_dataset = bp_df_plot
lm_model = lm(c66_46_avrsybp ~ y1_app_temp_sd_46 +
                    c66_46_q1_ht + c66_46_c_bmi + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = imputed_dataset)

#residual plot of predictors vs resid and fitted vs resid = constant variance
car::residualPlots(lm_model,
                   pch=20, col="gray",
                   fitted = T,
                   ask = F, layout = c(2,2),
                   tests = F, quadratic = F)

ols_test_breusch_pagan(lm_model)
```

We see decrease in variance from 140. However, the violation is not serious? in this case, so we can say that the assumption is satisfied.

**Linearity assumption**

```{r}
car::crPlots(lm_model, terms = ~ y1_app_temp_sd_46 +
                    c66_46_q1_ht + c66_46_c_bmi + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean,
             pch=20, col="gray",
             smooth = list(smoother=car::gamLine), ask = F)
```

Linear relationship.

**Normality assumption**

```{r}
source("Functions_rmph.R")

check_normality(lm_model)
```

In the **left panel** (histogram of residuals) we see that observed residuals almost perfectly normally distributed.

In the **right panel** (a QQ plot) there is a lack of normality in the tails of the distribution. Since the sample size is huge (n = 5661, obs \> predictors), non-normality unlikely impact the results.

**Effect of imputation on estimates (riv, lambda, fmi)**

```{r}
pooled_results
```

All variables have riv (relative increase in variance due to nonresponse), lambda (the proportion of the variation attributable to the missing data), fmi (the fraction of information missing due to nonresponse) less than 0.2 which can be interpreted as modest.

###### Pooled Radj and BIC for linear model:

```{r}
pool.r.squared(lm_models, adjusted = T)

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(lm_models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(lm_models$analyses[[i]])$BIC,
         Radj = glance(lm_models$analyses[[i]])$adj.r.squared)
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = fisher.trans(Radj)) %>%
  summarise(pooled_Radj = fisher.backtrans(mean(Radj_trans)),  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

Pooled Radj is 0.25, pooled BIC is 45998.

###### Visualizing the adjusted relationships

```{r}
car::avPlots(lm_model, terms = c66_46_avrsybp ~ y1_app_temp_sd_46 +
                    c66_46_q1_ht + c66_46_c_bmi + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, 
             col = "darkgray", 
             col.lines = "blue",
             pch = 16,
             id = F, ask = F)
```

Positive linear relationship.

#### 2.1.3 Average temperature

##### 2.1.3.1 Unadjusted model (univariate)

Pool estimates over all datasets with 1 exposure variable:

```{r}
source("Functions_rmph.R")
imputed_dataset = bp_df_plot

# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrsybp ~ y1_avg_temp))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2) 
```

Pooled b1 for app_sd is -1.06 (p\<0.001) \[-1.34, -0.78\]. The effect size is stat.insignificant and small. SE = 0.14.

###### Model diagnostics.

**Constant variance assumption**

```{r}
lm_model = lm(c66_46_avrsybp ~ y1_avg_temp, data = imputed_dataset)

#residual plot of predictors vs resid and fitted vs resid = constant variance
car::residualPlots(lm_model,
                   pch=20, col="gray",
                   fitted = T,
                   ask = F, layout = c(2,2),
                   tests = F, quadratic = F)

ols_test_breusch_pagan(lm_model)
```

Variance started decreasing from 125.4. The violation might affect the results. A violation of the constant variance assumption results in inaccurate confidence intervals and p-values, even in large samples, although regression coefficient estimates will still be unbiased.

**Linearity assumption**

```{r}
car::crPlots(lm_model, terms = ~ y1_avg_temp,
             pch=20, col="gray",
             smooth = list(smoother=car::gamLine), ask = F)
```

The relationship is definitely linear.

**Normality assumption**

```{r}
source("Functions_rmph.R")

check_normality(lm_model)
```

In the **left panel** (histogram of residuals) we see that observed residuals almost perfectly normally distributed.

In the **right panel** (a QQ plot) there is a lack of normality in the tails of the distribution. Since the sample size is huge (n = 5661, obs \> predictors), non-normality unlikely impact the results.

**Effect of imputation on estimates (riv, lambda, fmi)**

```{r}
pooled_results
```

riv = 0, lambda = 0, fmi \~ 0 because of 1 variable only that does not have NA values.

###### Pooled Radj and BIC for linear model:

```{r}
pool.r.squared(lm_models, adjusted = F)

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(lm_models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(lm_models$analyses[[i]])$BIC,
         Radj = glance(lm_models$analyses[[i]])$adj.r.squared)
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = fisher.trans(Radj)) %>%
  summarise(pooled_Radj = fisher.backtrans(mean(Radj_trans)),  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

Pooled Radj is-0.01 which can be considered as almost zero and BIC 47381.

###### Visualizing the adjusted relationships

```{r}
car::avPlots(lm_model, terms = c66_46_avrsybp ~ y1_avg_temp, 
             col = "darkgray", 
             col.lines = "blue",
             pch = 16,
             id = F, ask = F)
```

Negative linear relationship.

##### 2.1.3.2 Minimally adjusted model

Pool estimates over all datasets with minimal set of variables:

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrsybp ~ y1_avg_temp + c66_46_edu +
                c66_46_occup + sex + 
                c66_46_urban_rural + c66_46_season))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2) 
```

Pooled b1 for app.temp is -0.81 (p\<0.01) \[-1.07, -0.55\]. The effect size decreased by 0.25 and p-value decreased.

**Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
mi.anova(pmm_proj_multiimp,
         "c66_46_avrsybp ~ y1_avg_temp + c66_46_edu + 
         c66_46_occup + sex + c66_46_ht_med", type = 3)
```

R2=0.12. All categorical variables are stat. significant except occup status (p=0.12)

###### Model diagnostics.

Number of imputations

```{r}
howManyImputations::how_many_imputations(lm_models)
```

**Constant variance assumption**

```{r}
lm_model = lm(c66_46_avrsybp ~ y1_avg_temp + c66_46_edu +
                c66_46_occup + sex + 
                c66_46_ht_med, data = imputed_dataset)

#residual plot of predictors vs resid and fitted vs resid = constant variance
car::residualPlots(lm_model,
                   pch=20, col="gray",
                   fitted = T,
                   ask = F, layout = c(2,2),
                   tests = F, quadratic = F)

ols_test_breusch_pagan(lm_model)
```

Mostly constant variance with small deviation.

**Linearity assumption**

```{r}
car::crPlots(lm_model, terms = ~ y1_avg_temp + c66_46_edu +
                c66_46_occup + sex + 
                c66_46_ht_med,
             pch=20, col="gray",
             smooth = list(smoother=car::gamLine), , ask = F)
```

Linear relationship.

**Normality assumption**

```{r}
source("Functions_rmph.R")

check_normality(lm_model)
```

In the **left panel** (histogram of residuals) we see that observed residuals almost perfectly normally distributed.

In the **right panel** (a QQ plot) there is a lack of normality in the tails of the distribution. Since the sample size is huge (n = 5661, obs \> predictors), non-normality unlikely impact the results.

**Effect of imputation on estimates (riv, lambda, fmi)**

```{r}
pooled_results
```

All variables have riv (relative increase in variance due to nonresponse), lambda (the proportion of the variation attributable to the missing data), fmi (the fraction of information missing due to nonresponse) less than 0.2 which can be interpreted as modest.

###### Pooled Radj and BIC for linear model:

```{r}
pool.r.squared(lm_models, adjusted = T)

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(lm_models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(lm_models$analyses[[i]])$BIC,
         Radj = glance(lm_models$analyses[[i]])$adj.r.squared)
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = fisher.trans(Radj)) %>%
  summarise(pooled_Radj = fisher.backtrans(mean(Radj_trans)),  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

Pooled Radj is 0.13, pooled BIC is 46699.

###### Visualizing the adjusted relationships

```{r}
car::avPlots(lm_model, terms = c66_46_avrsybp ~ y1_avg_temp + 
               c66_46_edu + c66_46_occup + sex + 
                c66_46_ht_med, 
             col = "darkgray", 
             col.lines = "blue",
             pch = 16,
             id = F, ask = F)
```

Negative slope..

##### 2.1.3.3 Optimized model

Pool estimates over all datasets with minimal set of variables:

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrsybp ~ y1_avg_temp  +  
                       c66_46_smok + 
                       c66_46_c_bmi + c66_46_edu +
                       c66_46_ht_med + c66_46_q1_ht +
                       c66_46_urban_rural + day_mean +
                       sex + c66_46_alc))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2) 
```

Pooled b1 for avg is -0.86 (p\<0.01) \[-1.14, -0.59\].

**Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
mi.anova(pmm_proj_multiimp,
         "c66_46_avrsybp ~ y1_avg_temp   +  
                       c66_46_smok +  +
                       c66_46_c_bmi + c66_46_edu +
                       c66_46_ht_med + c66_46_q1_ht +
                       c66_46_urban_rural + day_mean +
                       sex + c66_46_alc", type = 3)
```

R2=0.19. All categorical variables are stat. significant.

###### Model diagnostics.

Number of imputations

```{r}
howManyImputations::how_many_imputations(lm_models)
```

6 imputations needed.

**Constant variance assumption**

```{r}
imputed_dataset = bp_df_plot
lm_model = lm(c66_46_avrsybp ~ y1_avg_temp   +  
                       c66_46_smok +  +
                       c66_46_c_bmi + c66_46_edu +
                       c66_46_ht_med + c66_46_q1_ht +
                       c66_46_urban_rural + day_mean +
                       sex + c66_46_alc, data = imputed_dataset)

#residual plot of predictors vs resid and fitted vs resid = constant variance
car::residualPlots(lm_model,
                   pch=20, col="gray",
                   fitted = T,
                   ask = F, layout = c(2,2),
                   tests = F, quadratic = F)

ols_test_breusch_pagan(lm_model)
```

We see decrease in variance from 140. The violation might affect the results. A violation of the constant variance assumption results in inaccurate confidence intervals and p-values, even in large samples, although regression coefficient estimates will still be unbiased.

**Linearity assumption**

```{r}
car::crPlots(lm_model, terms = ~ y1_avg_temp   +  
                       c66_46_smok +  +
                       c66_46_c_bmi + c66_46_edu +
                       c66_46_ht_med + c66_46_q1_ht +
                       c66_46_urban_rural + day_mean +
                       sex + c66_46_alc,
             pch=20, col="gray",
             smooth = list(smoother=car::gamLine), ask = F)
```

Linear

**Normality assumption**

```{r}
source("Functions_rmph.R")

check_normality(lm_model)
```

In the **left panel** (histogram of residuals) we see that observed residuals almost perfectly normally distributed.

In the **right panel** (a QQ plot) there is a lack of normality in the tails of the distribution. Since the sample size is huge (n = 5661, obs \> predictors), non-normality unlikely impact the results.

**Effect of imputation on estimates (riv, lambda, fmi)**

```{r}
pooled_results
```

All variables have riv (relative increase in variance due to nonresponse), lambda (the proportion of the variation attributable to the missing data), fmi (the fraction of information missing due to nonresponse) less than 0.2 which can be interpreted as modest.

###### Pooled Radj and BIC for linear model:

```{r}
pool.r.squared(lm_models, adjusted = T)

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(lm_models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(lm_models$analyses[[i]])$BIC,
         Radj = glance(lm_models$analyses[[i]])$adj.r.squared)
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = fisher.trans(Radj)) %>%
  summarise(pooled_Radj = fisher.backtrans(mean(Radj_trans)),  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

Pooled Radj is 0.25, pooled BIC is 45904..

###### Visualizing the adjusted relationships

```{r}
car::avPlots(lm_model, terms = c66_46_avrsybp ~ y1_avg_temp   +  
                       c66_46_smok +  +
                       c66_46_c_bmi + c66_46_edu +
                       c66_46_ht_med + c66_46_q1_ht +
                       c66_46_urban_rural + day_mean +
                       sex + c66_46_alc, 
             col = "darkgray", 
             col.lines = "blue",
             pch = 16,
             id = F, ask = F)
```

Negative slope.

##### 2.1.3.4 Fully adjusted

Pool estimates over all datasets with minimal set of variables:

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrsybp ~ y1_avg_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)
```

Pooled b1 for avg.temp is -0.91 (p\<0.001) \[1.19, -0.63\]. The SE the same.

**Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
mi.anova(pmm_proj_multiimp,
         "c66_46_avrsybp ~ y1_avg_temp +
                    c66_46_q1_ht + c66_46_c_bmi + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean", type = 3)
```

R2=0.18. All categorical variables are stat. significant except phys, occup, diab, hpcl, day_mean. An interesting finding is that the season is stat. significant, however the day_mean is not. At the same time season was not selected in any of the optimized model, but day_mean was.

###### Model diagnostics.

Number of imputations

```{r}
howManyImputations::how_many_imputations(lm_models)
```

6 imputations needed.

**Constant variance assumption**

```{r}
imputed_dataset = bp_df_plot
lm_model = lm(c66_46_avrsybp ~ y1_avg_temp +
                    c66_46_q1_ht + c66_46_c_bmi + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = imputed_dataset)

#residual plot of predictors vs resid and fitted vs resid = constant variance
car::residualPlots(lm_model,
                   pch=20, col="gray",
                   fitted = T,
                   ask = F, layout = c(2,2),
                   tests = F, quadratic = F)

ols_test_breusch_pagan(lm_model)
```

We see decrease in variance from 140. However, the violation is not serious? in this case, so we can say that the assumption is satisfied.

**Linearity assumption**

```{r}
car::crPlots(lm_model, terms = ~ y1_avg_temp +
                    c66_46_q1_ht + c66_46_c_bmi + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean,
             pch=20, col="gray",
             smooth = list(smoother=car::gamLine), ask = F)
```

Linear relationship.

**Normality assumption**

```{r}
source("Functions_rmph.R")

check_normality(lm_model)
```

In the **left panel** (histogram of residuals) we see that observed residuals almost perfectly normally distributed.

In the **right panel** (a QQ plot) there is a lack of normality in the tails of the distribution. Since the sample size is huge (n = 5661, obs \> predictors), non-normality unlikely impact the results.

**Effect of imputation on estimates (riv, lambda, fmi)**

```{r}
pooled_results
```

All variables have riv (relative increase in variance due to nonresponse), lambda (the proportion of the variation attributable to the missing data), fmi (the fraction of information missing due to nonresponse) less than 0.2 which can be interpreted as modest.

###### Pooled Radj and BIC for linear model:

```{r}
pool.r.squared(lm_models, adjusted = T)

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(lm_models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(lm_models$analyses[[i]])$BIC,
         Radj = glance(lm_models$analyses[[i]])$adj.r.squared)
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = fisher.trans(Radj)) %>%
  summarise(pooled_Radj = fisher.backtrans(mean(Radj_trans)),  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

Pooled Radj is 0.25, pooled BIC is 45963.

###### Visualizing the adjusted relationships

```{r}
car::avPlots(lm_model, terms = c66_46_avrsybp ~ y1_avg_temp +
                    c66_46_q1_ht + c66_46_c_bmi + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, 
             col = "darkgray", 
             col.lines = "blue",
             pch = 16,
             id = F, ask = F)
```

Negative slope.

#### 2.1.4 Standard Deviation of average temperature

##### 2.1.4.1 Unadjusted model (univariate)

Pool estimates over all datasets with 1 exposure variable:

```{r}
source("Functions_rmph.R")
imputed_dataset = bp_df_plot

# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrsybp ~ y1_sd_temp))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2) 
```

Pooled b1 for avd_sd is -0.09 (p=0.73) \[-0.57, 0.40\]. SE = 0.25 which is greater than effect size.

###### Model diagnostics.

**Constant variance assumption**

```{r}
lm_model = lm(c66_46_avrsybp ~ y1_sd_temp, data = imputed_dataset)

#residual plot of predictors vs resid and fitted vs resid = constant variance
car::residualPlots(lm_model,
                   pch=20, col="gray",
                   fitted = T,
                   ask = F, layout = c(2,2),
                   tests = F, quadratic = F)

ols_test_breusch_pagan(lm_model)
```

Variance started decreasing from 125.4. The violation might affect the results. A violation of the constant variance assumption results in inaccurate confidence intervals and p-values, even in large samples, although regression coefficient estimates will still be unbiased.

**Linearity assumption**

```{r}
car::crPlots(lm_model, terms = ~ y1_sd_temp,
             pch=20, col="gray",
             smooth = list(smoother=car::gamLine), ask = F)
```

The relationship is definitely linear.

**Normality assumption**

```{r}
source("Functions_rmph.R")

check_normality(lm_model)
```

In the **left panel** (histogram of residuals) we see that observed residuals almost perfectly normally distributed.

In the **right panel** (a QQ plot) there is a lack of normality in the tails of the distribution. Since the sample size is huge (n = 5661, obs \> predictors), non-normality unlikely impact the results.

**Effect of imputation on estimates (riv, lambda, fmi)**

```{r}
pooled_results
```

riv = 0, lambda = 0, fmi \~ 0 because of 1 variable only that does not have NA values.

###### Pooled Radj and BIC for linear model:

```{r}
pool.r.squared(lm_models, adjusted = F)

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(lm_models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(lm_models$analyses[[i]])$BIC,
         Radj = glance(lm_models$analyses[[i]])$adj.r.squared)
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = fisher.trans(Radj)) %>%
  summarise(pooled_Radj = fisher.backtrans(mean(Radj_trans)),  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

Pooled Radj is-0.00015 which can be considered as almost zero and BIC 47438.

###### Visualizing the adjusted relationships

```{r}
car::avPlots(lm_model, terms = c66_46_avrsybp ~ y1_sd_temp, 
             col = "darkgray", 
             col.lines = "blue",
             pch = 16,
             id = F, ask = F)
```

Horizontal line.

##### 2.1.4.2 Minimally adjusted model

Pool estimates over all datasets with minimal set of variables:

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrsybp ~ y1_sd_temp + c66_46_edu +
                c66_46_occup + sex + 
                c66_46_urban_rural + c66_46_season))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2) 
```

Pooled b1 for app.temp is -0.24 (p=0.31) \[-0.69, 0.22\].

**Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
mi.anova(pmm_proj_multiimp,
         "c66_46_avrsybp ~ y1_sd_temp + c66_46_edu + 
         c66_46_occup + sex + c66_46_ht_med", type = 3)
```

R2=0.11. All categorical variables are stat. significant except occup status (p=0.12) and avg_sd

###### Model diagnostics.

Number of imputations

```{r}
howManyImputations::how_many_imputations(lm_models)
```

**Constant variance assumption**

```{r}
lm_model = lm(c66_46_avrsybp ~ y1_sd_temp + c66_46_edu +
                c66_46_occup + sex + 
                c66_46_ht_med, data = imputed_dataset)

#residual plot of predictors vs resid and fitted vs resid = constant variance
car::residualPlots(lm_model,
                   pch=20, col="gray",
                   fitted = T,
                   ask = F, layout = c(2,2),
                   tests = F, quadratic = F)

ols_test_breusch_pagan(lm_model)
```

This clustering migh be due to the fact that there is only 1 contin.variable and 4 categorical

**Linearity assumption**

```{r}
car::crPlots(lm_model, terms = ~ y1_sd_temp + c66_46_edu +
                c66_46_occup + sex + 
                c66_46_ht_med,
             pch=20, col="gray",
             smooth = list(smoother=car::gamLine), , ask = F)
```

Linear relationship.

**Normality assumption**

```{r}
source("Functions_rmph.R")

check_normality(lm_model)
```

In the **left panel** (histogram of residuals) we see that observed residuals almost perfectly normally distributed.

In the **right panel** (a QQ plot) there is a lack of normality in the tails of the distribution. Since the sample size is huge (n = 5661, obs \> predictors), non-normality unlikely impact the results.

**Effect of imputation on estimates (riv, lambda, fmi)**

```{r}
pooled_results
```

All variables have riv (relative increase in variance due to nonresponse), lambda (the proportion of the variation attributable to the missing data), fmi (the fraction of information missing due to nonresponse) less than 0.2 which can be interpreted as modest.

###### Pooled Radj and BIC for linear model:

```{r}
pool.r.squared(lm_models, adjusted = T)

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(lm_models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(lm_models$analyses[[i]])$BIC,
         Radj = glance(lm_models$analyses[[i]])$adj.r.squared)
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = fisher.trans(Radj)) %>%
  summarise(pooled_Radj = fisher.backtrans(mean(Radj_trans)),  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

Pooled Radj is 0.12, pooled BIC is 46735.

###### Visualizing the adjusted relationships

```{r}
car::avPlots(lm_model, terms = c66_46_avrsybp ~ y1_sd_temp + 
               c66_46_edu + c66_46_occup + sex + 
                c66_46_ht_med, 
             col = "darkgray", 
             col.lines = "blue",
             pch = 16,
             id = F, ask = F)
```

Horizontal line.

##### 2.1.4.3 Optimized model

Pool estimates over all datasets with minimal set of variables:

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrsybp  ~ y1_sd_temp  +  
                       c66_46_c_bmi +
                       c66_46_edu + c66_46_ht_med +
                       c66_46_q1_ht + 
                       c66_46_smok + c66_46_urban_rural +
                       sex + day_mean + c66_46_alc))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2) 
```

Pooled b1 for avg is -0.01 (p=0.97) \[-0.44, 0.42\].

**Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
mi.anova(pmm_proj_multiimp,
         "c66_46_avrsybp ~ y1_sd_temp + c66_46_alc +  
                       c66_46_c_bmi +
                       c66_46_edu + c66_46_ht_med +
                       c66_46_q1_ht + 
                       c66_46_smok + c66_46_urban_rural +
                       sex + day_mean", type = 3)
```

R2=0.19. All categorical variables are stat. significant except sd_temp.

###### Model diagnostics.

Number of imputations

```{r}
howManyImputations::how_many_imputations(lm_models)
```

6 imputations needed. **Constant variance assumption**

```{r}
imputed_dataset = bp_df_plot
lm_model = lm(c66_46_avrsybp ~ y1_sd_temp + c66_46_alc +  
                       c66_46_c_bmi +
                       c66_46_edu + c66_46_ht_med +
                       c66_46_q1_ht + 
                       c66_46_smok + c66_46_urban_rural +
                       sex + day_mean, data = imputed_dataset)

#residual plot of predictors vs resid and fitted vs resid = constant variance
car::residualPlots(lm_model,
                   pch=20, col="gray",
                   fitted = T,
                   ask = F, layout = c(2,2),
                   tests = F, quadratic = F)

ols_test_breusch_pagan(lm_model)
```

We see decrease in variance from 140. The violation might affect the results. A violation of the constant variance assumption results in inaccurate confidence intervals and p-values, even in large samples, although regression coefficient estimates will still be unbiased.

**Linearity assumption**

```{r}
car::crPlots(lm_model, terms = ~ y1_sd_temp + c66_46_alc +  
                       c66_46_c_bmi +
                       c66_46_edu + c66_46_ht_med +
                       c66_46_q1_ht + 
                       c66_46_smok + c66_46_urban_rural +
                       sex + day_mean,
             pch=20, col="gray",
             smooth = list(smoother=car::gamLine), ask = F)
```

Linear

**Normality assumption**

```{r}
source("Functions_rmph.R")

check_normality(lm_model)
```

In the **left panel** (histogram of residuals) we see that observed residuals almost perfectly normally distributed.

In the **right panel** (a QQ plot) there is a lack of normality in the tails of the distribution. Since the sample size is huge (n = 5661, obs \> predictors), non-normality unlikely impact the results.

**Effect of imputation on estimates (riv, lambda, fmi)**

```{r}
pooled_results
```

All variables have riv (relative increase in variance due to nonresponse), lambda (the proportion of the variation attributable to the missing data), fmi (the fraction of information missing due to nonresponse) less than 0.2 which can be interpreted as modest.

###### Pooled Radj and BIC for linear model:

```{r}
pool.r.squared(lm_models, adjusted = T)

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(lm_models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(lm_models$analyses[[i]])$BIC,
         Radj = glance(lm_models$analyses[[i]])$adj.r.squared)
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = fisher.trans(Radj)) %>%
  summarise(pooled_Radj = fisher.backtrans(mean(Radj_trans)),  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

Pooled Radj is 0.25, pooled BIC is 45934..

###### Visualizing the adjusted relationships

```{r}
car::avPlots(lm_model, terms = c66_46_avrsybp ~ y1_sd_temp + c66_46_alc +  
                       c66_46_c_bmi +
                       c66_46_edu + c66_46_ht_med +
                       c66_46_q1_ht + 
                       c66_46_smok + c66_46_urban_rural +
                       sex + day_mean, 
             col = "darkgray", 
             col.lines = "blue",
             pch = 16,
             id = F, ask = F)
```

Horizontal line.

##### 2.1.4.4 Fully adjusted(-)

Pool estimates over all datasets with minimal set of variables:

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrsybp ~ y1_sd_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)
```

Pooled b1 for avg.temp is 0.7 (p=0.03) \[0.06, 1.33\]. The SE increaed to 0.32.

**Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
mi.anova(pmm_proj_multiimp,
         "c66_46_avrsybp ~ y1_sd_temp +
                    c66_46_q1_ht + c66_46_c_bmi + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean", type = 3)
```

R2=0.18. All categorical variables are stat. significant except phys, occup, diab, hpcl, day_mean. An interesting finding is that the season is stat. significant, however the day_mean is not. At the same time season was not selected in any of the optimized model, but day_mean was.

###### Model diagnostics.

Number of imputations

```{r}
howManyImputations::how_many_imputations(lm_models)
```

6 imputations needed.

**Constant variance assumption**

```{r}
imputed_dataset = bp_df_plot
lm_model = lm(c66_46_avrsybp ~ y1_sd_temp +
                    c66_46_q1_ht + c66_46_c_bmi + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = imputed_dataset)

#residual plot of predictors vs resid and fitted vs resid = constant variance
car::residualPlots(lm_model,
                   pch=20, col="gray",
                   fitted = T,
                   ask = F, layout = c(2,2),
                   tests = F, quadratic = F)

ols_test_breusch_pagan(lm_model)
```

We see decrease in variance from 140. However, the violation is not serious? in this case, so we can say that the assumption is satisfied.

**Linearity assumption**

```{r}
car::crPlots(lm_model, terms = ~ y1_sd_temp +
                    c66_46_q1_ht + c66_46_c_bmi + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean,
             pch=20, col="gray",
             smooth = list(smoother=car::gamLine), ask = F)
```

Linear relationship.

**Normality assumption**

```{r}
source("Functions_rmph.R")

check_normality(lm_model)
```

In the **left panel** (histogram of residuals) we see that observed residuals almost perfectly normally distributed.

In the **right panel** (a QQ plot) there is a lack of normality in the tails of the distribution. Since the sample size is huge (n = 5661, obs \> predictors), non-normality unlikely impact the results.

**Effect of imputation on estimates (riv, lambda, fmi)**

```{r}
pooled_results
```

All variables have riv (relative increase in variance due to nonresponse), lambda (the proportion of the variation attributable to the missing data), fmi (the fraction of information missing due to nonresponse) less than 0.2 which can be interpreted as modest.

###### Pooled Radj and BIC for linear model:

```{r}
pool.r.squared(lm_models, adjusted = T)

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(lm_models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(lm_models$analyses[[i]])$BIC,
         Radj = glance(lm_models$analyses[[i]])$adj.r.squared)
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = fisher.trans(Radj)) %>%
  summarise(pooled_Radj = fisher.backtrans(mean(Radj_trans)),  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

Pooled Radj is 0.25, pooled BIC is 46000.

###### Visualizing the adjusted relationships

```{r}
car::avPlots(lm_model, terms = c66_46_avrsybp ~ y1_sd_temp +
                    c66_46_q1_ht + c66_46_c_bmi + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, 
             col = "darkgray", 
             col.lines = "blue",
             pch = 16,
             id = F, ask = F)
```

Positive slope.

### 2.2 Diastolic blood pressure

#### 2.2.1 Apparent average temperature

##### 2.2.1.1 Unadjusted model (univariate)

Pool estimates over all datasets with 1 exposure variable:

```{r}
imputed_dataset = bp_df_plot

# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrdibp ~ y1_app_temp_avg_46))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)
pooled_results
```

Pooled b1 for app.temp is -0.68 (p\<0.001) \[-0.85, -0.51\]. SE=0.09.

###### Model diagnostics.

**Constant variance assumption**

We can diagnose the appropriateness of the constant variance assumption by examining a plot of residuals vs. fitted values (and, if we want to dig deeper, plots of residuals vs. each predictor) using car::residualPlots(). If the constant variance assumption is met, the spread of the points in the vertical direction should not vary much as you move in the horizontal direction.

```{r}
lm_model = lm(c66_46_avrdibp ~ y1_app_temp_avg_46, data = imputed_dataset)

#residual plot of predictors vs resid and fitted vs resid = constant variance
car::residualPlots(lm_model,
                   pch=20, col="gray",
                   fitted = T,
                   ask = F, layout = c(2,2),
                   tests = F, quadratic = F)

ols_test_breusch_pagan(lm_model)
```

Constant variance assumption is satisfied.

**Linearity assumption**

The linearity assumption requires that the Y vs. X relationship be linear when holding all other predictors fixed. A residual plot that takes that into account and is effective at diagnosing non-linearity is the component-plus-residual plot (CR plot), also known as a partial-residual plot. We will create a CR plot for each continuous predictor using the car::crPlots().

If there were no linear association between Y and X after adjusting for all other predictors in the model, the dashed line would be a horizontal line at 0. The solid line is a smoother that relaxes the linearity assumption. In each panel, if the solid line falls exactly on top of the dashed line, then the linearity assumption is perfectly met for that predictor.

In each panel, the raw predictor values are plotted on the horizontal axis. The vertical axis plots the residuals plus the specific contribution of X , adjusted for all the other predictors in the model,\
( βjxij) added back in (the component). The dashed line is the linear regression of the component + residual vs. X . If there were no linear association between Y and X after adjusting for all other predictors in the model, the dashed line would be a horizontal line at 0. The solid line is a smoother that relaxes the linearity assumption. In each panel, if the solid line falls exactly on top of the dashed line, then the linearity assumption is perfectly met for that predictor. In this example, age is linear, but there seems to be a slight non-linearity for waist circumference.

```{r}
car::crPlots(lm_model, terms = ~ y1_app_temp_avg_46,
             pch=20, col="gray",
             smooth = list(smoother=car::gamLine), ask = F)
```

Linear relationship.

**Normality assumption**

The assumption is that the errors are normally distributed. We do not actually observe the errors, but we do observe the residuals and so we use those to diagnose normality (as well as other assumptions). There are formal tests of normality (e.g., the Shapiro-Wilk test); however, they are not needed in large samples since violation of the normality assumption will not impact inferences and they lack power in small samples. Instead, use a visual diagnosis.

```{r}
source("Functions_rmph.R")

check_normality(lm_model)
```

In the **left panel** (histogram of residuals) the solid line provides a smoothed estimate of the distribution of the observed residuals without making any assumption about the shape of the distribution. The dashed line, however, is the best fit normal curve. The closer these two curves are to each other, the better the normality assumption is met. We see that observed residuals almost perfectly normally distributed.

In the **right panel** (a QQ plot), the ordered values of the n residuals are plotted against the corresponding n quantiles of the standard normal distribution. The closer the points are to the dashed line, the better the normality assumption is met. In this example, there is a lack of normality in the tails of the distribution. Since the sample size is huge (n = 5661, obs \> predictors), non-normality unlikely impact the results.

**Effect of imputation on estimates (riv, lambda, fmi)**

λ (lambda)- the proportion of the variation attributable to the missing data. It is equal to zero if the missing data do not add extra variation to the sampling variance, an exceptional situation that can occur only if we can perfectly re-create the missing data. The maximum value is equal to 1, which occurs only if all variation is caused by the missing data. This is equally unlikely to occur in practice since it means that there is no information at all. If λ is high, say λ\>0.5 , the influence of the imputation model on the final result is larger than that of the complete-data model.

riv - relative increase in variance due to nonresponse.

γ (epsilon) - the fraction of information missing due to nonresponse. The interpretations of γ and λ are similar, but γ is adjusted for the finite number of imputations. The literature often confuses γ and λ , and erroneously labels λ as the fraction of missing information. The values of λ and γ are almost identical for large ν , but they could notably differ for low ν .

The quantities λ , riv and γ as well as their multivariate analogues ¯λ and ¯r are indicators of the severity of the missing data problem. Fractions of missing information up to 0.2 can be interpreted as “modest,” 0.3 as “moderately large” and 0.5 as “high”. High values indicate a difficult problem in which the final statistical inferences are highly dependent on the way in which the missing data were handled. Note that estimates of λ , riv and γ may be quite variable for low m.

```{r}
pooled_results
```

riv = 0, lambda = 0, fmi \~ 0 because of 1 variable only that does not have NA values.

###### Pooled Radj and BIC for linear model:

```{r}
pool.r.squared(lm_models, adjusted = T)

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(lm_models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(lm_models$analyses[[i]])$BIC,
         Radj = glance(lm_models$analyses[[i]])$adj.r.squared)
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = fisher.trans(Radj)) %>%
  summarise(pooled_Radj = fisher.backtrans(mean(Radj_trans)),  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

Pooled Radj is 0.01 which can be considered as very small (1% of variance are explained) and BIC 47876.

###### Visualizing the adjusted relationships

The relationship we would like to see is the relationship between the outcome and each predictor after removing the effect of all the other predictors. We can do this using an **added variable plot.** The vertical axis is labeled “outcome \| others” which corresponds to the outcome given all the predictors other than the one on the horizontal axis. The horizontal axis on each plot tells you to which predictor each added variable plot corresponds. For example, “predictor \| others” corresponds to predictor adjusted for all the other predictors. 

```{r}
car::avPlots(lm_model, terms = c66_46_avrdibp ~ y1_app_temp_avg_46, 
             col = "darkgray", 
             col.lines = "blue",
             pch = 16,
             id = F, ask = F)
```

Negative slope.

##### 2.2.1.2 Minimally adjusted model

Pool estimates over all datasets with minimal set of variables:

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrdibp ~ y1_app_temp_avg_46 + c66_46_edu +
                c66_46_occup + sex + 
                c66_46_urban_rural + c66_46_season))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2) 
```

Pooled b1 for app.temp is -0.54 (p\<0.001) \[-0.71, -0.37\]. The effect size decreased by 0.14.

**Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:** The type = 3 option specifies that we want a Type III test, which is a test of a predictor after adjusting for all other terms in the model

```{r}
mi.anova(pmm_proj_multiimp,
         "c66_46_avrdibp ~ y1_app_temp_avg_46 + c66_46_edu + 
         c66_46_occup + sex + c66_46_ht_med", type = 3)
```

R2=0.12. All categorical variables are stat. significant except occup status (p=0.16)

###### Model diagnostics.

Number of imputations

```{r}
howManyImputations::how_many_imputations(lm_models)
```

**Constant variance assumption**

```{r}
lm_model = lm(c66_46_avrdibp ~ y1_app_temp_avg_46 + c66_46_edu +
                c66_46_occup + sex + 
                c66_46_ht_med, data = imputed_dataset)

#residual plot of predictors vs resid and fitted vs resid = constant variance
car::residualPlots(lm_model,
                   pch=20, col="gray",
                   fitted = T,
                   ask = F, layout = c(2,2),
                   tests = F, quadratic = F)

ols_test_breusch_pagan(lm_model)
```

Constant variance is violated right after 89. However, the violation is not serious in this case, so we can say that the assumption is satisfied.

**Linearity assumption**

```{r}
car::crPlots(lm_model, terms = ~ y1_app_temp_avg_46 + c66_46_edu +
                c66_46_occup + sex + 
                c66_46_ht_med,
             pch=20, col="gray",
             smooth = list(smoother=car::gamLine), , ask = F)
```

Linear.

**Normality assumption**

```{r}
source("Functions_rmph.R")

check_normality(lm_model)
```

In the **left panel** (histogram of residuals) we see that observed residuals almost perfectly normally distributed.

In the **right panel** (a QQ plot) there is a lack of normality in the tails of the distribution. Since the sample size is huge (n = 5661, obs \> predictors), non-normality unlikely impact the results.

**Effect of imputation on estimates (riv, lambda, fmi)**

```{r}
pooled_results
```

All variables have riv (relative increase in variance due to nonresponse), lambda (the proportion of the variation attributable to the missing data), fmi (the fraction of information missing due to nonresponse) less than 0.2 which can be interpreted as modest.

###### Pooled Radj and BIC for linear model:

```{r}
pool.r.squared(lm_models, adjusted = T)

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(lm_models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(lm_models$analyses[[i]])$BIC,
         Radj = glance(lm_models$analyses[[i]])$adj.r.squared)
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = fisher.trans(Radj)) %>%
  summarise(pooled_Radj = fisher.backtrans(mean(Radj_trans)),  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

Pooled Radj is 0.08, pooled BIC is 42496.

###### Visualizing the adjusted relationships

```{r}
car::avPlots(lm_model, terms = c66_46_avrdibp ~ y1_app_temp_avg_46 + 
               c66_46_edu + c66_46_occup + sex + 
                c66_46_ht_med, 
             col = "darkgray", 
             col.lines = "blue",
             pch = 16,
             id = F, ask = F)
```

Positive relationship.

##### 2.2.1.3 Optimized model

Pool estimates over all datasets with minimal set of variables:

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrdibp  ~ y1_app_temp_avg_46 +  
                       c66_46_alc + c66_46_c_bmi +
                       c66_46_ht_med +
                       c66_46_q1_ht + 
                       c66_46_urban_rural + 
                       day_mean + sex))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)
```

Pooled b1 for app.temp is -0.75 (p\<0.001) \[-0.91, -0.59\].

**Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
mi.anova(pmm_proj_multiimp,
         "c66_46_avrdibp ~ y1_app_temp_avg_46 +  
                       c66_46_alc +  +
                       c66_46_c_bmi + 
                       c66_46_ht_med + c66_46_q1_ht +
                       sex + c66_46_phys", type = 3)
```

R2=0.19. All categorical variables are stat. significant.

###### Model diagnostics.

Number of imputations

```{r}
howManyImputations::how_many_imputations(lm_models)
```

6 imputations needed. **Constant variance assumption**

```{r}
imputed_dataset = bp_df_plot
lm_model = lm(c66_46_avrdibp ~ y1_app_temp_avg_46 +  
                       c66_46_alc +  +
                       c66_46_c_bmi + 
                       c66_46_ht_med + c66_46_q1_ht +
                       sex + c66_46_phys, data = imputed_dataset)

#residual plot of predictors vs resid and fitted vs resid = constant variance
car::residualPlots(lm_model,
                   pch=20, col="gray",
                   fitted = T,
                   ask = F, layout = c(2,2),
                   tests = F, quadratic = F)

ols_test_breusch_pagan(lm_model)
```

We see decrease in variance from 95. However, the violation is not serious in this case, so we can say that the assumption is satisfied.

**Linearity assumption**

```{r}
car::crPlots(lm_model, terms = ~ y1_app_temp_avg_46 +  
                       c66_46_alc +  +
                       c66_46_c_bmi + 
                       c66_46_ht_med + c66_46_q1_ht +
                       sex + c66_46_phys,
             pch=20, col="gray",
             smooth = list(smoother=car::gamLine), ask = F)
```

Linear.

**Normality assumption**

```{r}
source("Functions_rmph.R")

check_normality(lm_model)
```

In the **left panel** (histogram of residuals) we see that observed residuals almost perfectly normally distributed.

In the **right panel** (a QQ plot) there is a lack of normality in the tails of the distribution. Since the sample size is huge (n = 5661, obs \> predictors), non-normality unlikely impact the results.

**Effect of imputation on estimates (riv, lambda, fmi)**

```{r}
pooled_results
```

All variables have riv (relative increase in variance due to nonresponse), lambda (the proportion of the variation attributable to the missing data), fmi (the fraction of information missing due to nonresponse) less than 0.2 which can be interpreted as modest.

###### Pooled Radj and BIC for linear model:

```{r}
pool.r.squared(lm_models, adjusted = T)

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(lm_models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(lm_models$analyses[[i]])$BIC,
         Radj = glance(lm_models$analyses[[i]])$adj.r.squared)
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = fisher.trans(Radj)) %>%
  summarise(pooled_Radj = fisher.backtrans(mean(Radj_trans)),  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

Pooled Radj is 0.26, pooled BIC is 41336.

###### Visualizing the adjusted relationships

```{r}
car::avPlots(lm_model, terms = c66_46_avrdibp ~ y1_app_temp_avg_46 +  
                       c66_46_alc +  +
                       c66_46_c_bmi + 
                       c66_46_ht_med + c66_46_q1_ht +
                       sex + c66_46_phys, 
             col = "darkgray", 
             col.lines = "blue",
             pch = 16,
             id = F, ask = F)
```

Positive linear relationship.

##### 2.2.1.4 Fully adjusted

Pool estimates over all datasets with minimal set of variables:

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrdibp ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)
```

Pooled b1 for app.temp is -0.63 (p\<0.001) \[-0.80 -0.46\]. c66_46_urban_rural, c66_46_edu, c66_46_q1_ht, sex in combination drop the effect size from 1.05 to -0.61. c66_46_alc and increase the effect size. Standard error is 0.09 and did not change.

**Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
mi.anova(pmm_proj_multiimp,
         "c66_46_avrdibp ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean", type = 3)
```

R2=0.18. All categorical variables are stat. significant except phys, occup, diab, season, day_mean. Both day_mean and season are not included in the optimized model.

###### Model diagnostics.

Number of imputations

```{r}
howManyImputations::how_many_imputations(lm_models)
```

7 imputations needed. **Constant variance assumption**

```{r}
imputed_dataset = bp_df_plot
lm_model = lm(c66_46_avrdibp ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = imputed_dataset)

#residual plot of predictors vs resid and fitted vs resid = constant variance
car::residualPlots(lm_model,
                   pch=20, col="gray",
                   fitted = T,
                   ask = F, layout = c(2,2),
                   tests = F, quadratic = F)

ols_test_breusch_pagan(lm_model)
```

We see decrease in variance from 95. However, the violation is not serious? in this case, so we can say that the assumption is satisfied.

**Linearity assumption**

```{r}
car::crPlots(lm_model, terms = ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean,
             pch=20, col="gray",
             smooth = list(smoother=car::gamLine), ask = F)
```

Linear.

**Normality assumption**

```{r}
source("Functions_rmph.R")

check_normality(lm_model)
```

In the **left panel** (histogram of residuals) we see that observed residuals almost perfectly normally distributed.

In the **right panel** (a QQ plot) there is a lack of normality in the tails of the distribution. Since the sample size is huge (n = 5661, obs \> predictors), non-normality unlikely impact the results.

**Effect of imputation on estimates (riv, lambda, fmi)**

```{r}
pooled_results
```

All variables have riv (relative increase in variance due to nonresponse), lambda (the proportion of the variation attributable to the missing data), fmi (the fraction of information missing due to nonresponse) less than 0.2 which can be interpreted as modest.

###### Pooled Radj and BIC for linear model:

```{r}
pool.r.squared(lm_models, adjusted = T)

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(lm_models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(lm_models$analyses[[i]])$BIC,
         Radj = glance(lm_models$analyses[[i]])$adj.r.squared)
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = fisher.trans(Radj)) %>%
  summarise(pooled_Radj = fisher.backtrans(mean(Radj_trans)),  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

Pooled Radj is 0.26, pooled BIC is 41400.

###### Visualizing the adjusted relationships

```{r}
car::avPlots(lm_model, terms = c66_46_avrdibp ~ y1_app_temp_avg_46 +
                    c66_46_q1_ht + c66_46_c_bmi + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, 
             col = "darkgray", 
             col.lines = "blue",
             pch = 16,
             id = F, ask = F)
```

Negative linear relationship.

#### 2.2.2 Apparent standard deviation of average temperature

##### 2.2.2.1 Unadjusted model (univariate)

Pool estimates over all datasets with 1 exposure variable:

```{r}
source("Functions_rmph.R")
imputed_dataset = bp_df_plot

# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrdibp ~ y1_app_temp_sd_46))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)
```

Pooled b1 for app_sd is 0.21 (p=0.21) \[-0.12, 0.54\]. The effect size is stat.insignificant and small. Also SE is quite high relatively to b1 (0.17).

###### Model diagnostics.

**Constant variance assumption**

```{r}
lm_model = lm(c66_46_avrdibp ~ y1_app_temp_sd_46, data = imputed_dataset)

#residual plot of predictors vs resid and fitted vs resid = constant variance
car::residualPlots(lm_model,
                   pch=20, col="gray",
                   fitted = T,
                   ask = F, layout = c(2,2),
                   tests = F, quadratic = F)

ols_test_breusch_pagan(lm_model)
```

Assumption satisfied.

**Linearity assumption**

```{r}
car::crPlots(lm_model, terms = ~ y1_app_temp_sd_46,
             pch=20, col="gray",
             smooth = list(smoother=car::gamLine), ask = F)
```

The relationship is linear.

**Normality assumption**

```{r}
source("Functions_rmph.R")

check_normality(lm_model)
```

In the **left panel** (histogram of residuals) we see that observed residuals almost perfectly normally distributed.

In the **right panel** (a QQ plot) there is a lack of normality in the tails of the distribution. Since the sample size is huge (n = 5661, obs \> predictors), non-normality unlikely impact the results.

**Effect of imputation on estimates (riv, lambda, fmi)**

```{r}
pooled_results
```

riv = 0, lambda = 0, fmi \~ 0 because of 1 variable only that does not have NA values.

###### Pooled Radj and BIC for linear model:

```{r}
pool.r.squared(lm_models, adjusted = F)

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(lm_models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(lm_models$analyses[[i]])$BIC,
         Radj = glance(lm_models$analyses[[i]])$adj.r.squared)
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = fisher.trans(Radj)) %>%
  summarise(pooled_Radj = fisher.backtrans(mean(Radj_trans)),  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

Pooled R=0.0003 and Radj is-0.00010 which can be considered as almost zero and BIC 42934.

###### Visualizing the adjusted relationships

```{r}
car::avPlots(lm_model, terms = c66_46_avrdibp ~ y1_app_temp_sd_46, 
             col = "darkgray", 
             col.lines = "blue",
             pch = 16,
             id = F, ask = F)
```

Horizontal line = no effect.

##### 2.2.2.2 Minimally adjusted model

Pool estimates over all datasets with minimal set of variables:

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrdibp ~ y1_app_temp_sd_46 + c66_46_edu +
                c66_46_occup + sex + 
                c66_46_urban_rural + c66_46_season))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2) 
```

Pooled b1 for app.temp is 0.13 (p=0.44) \[-0.19, 0.44\].

**Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
mi.anova(pmm_proj_multiimp,
         "c66_46_avrdibp ~ y1_app_temp_sd_46 + c66_46_edu + 
         c66_46_occup + sex + c66_46_ht_med", type = 3)
```

R2=0.07. All categorical variables are stat. significant except occup status (p=0.09) and app_sd

###### Model diagnostics.

Number of imputations

```{r}
howManyImputations::how_many_imputations(lm_models)
```

**Constant variance assumption**

```{r}
lm_model = lm(c66_46_avrdibp ~ y1_app_temp_sd_46 + c66_46_edu +
                c66_46_occup + sex + 
                c66_46_ht_med, data = imputed_dataset)

#residual plot of predictors vs resid and fitted vs resid = constant variance
car::residualPlots(lm_model,
                   pch=20, col="gray",
                   fitted = T,
                   ask = F, layout = c(2,2),
                   tests = F, quadratic = F)

ols_test_breusch_pagan(lm_model)
```

Predictions clustered, variance is more or less constant.

**Linearity assumption**

```{r}
car::crPlots(lm_model, terms = ~ y1_app_temp_sd_46 + c66_46_edu +
                c66_46_occup + sex + 
                c66_46_ht_med,
             pch=20, col="gray",
             smooth = list(smoother=car::gamLine), , ask = F)
```

Linear relationship.

**Normality assumption**

```{r}
source("Functions_rmph.R")

check_normality(lm_model)
```

In the **left panel** (histogram of residuals) we see that observed residuals almost perfectly normally distributed.

In the **right panel** (a QQ plot) there is a lack of normality in the tails of the distribution. Since the sample size is huge (n = 5661, obs \> predictors), non-normality unlikely impact the results.

**Effect of imputation on estimates (riv, lambda, fmi)**

```{r}
pooled_results
```

All variables have riv (relative increase in variance due to nonresponse), lambda (the proportion of the variation attributable to the missing data), fmi (the fraction of information missing due to nonresponse) less than 0.2 which can be interpreted as modest.

###### Pooled Radj and BIC for linear model:

```{r}
pool.r.squared(lm_models, adjusted = T)

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(lm_models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(lm_models$analyses[[i]])$BIC,
         Radj = glance(lm_models$analyses[[i]])$adj.r.squared)
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = fisher.trans(Radj)) %>%
  summarise(pooled_Radj = fisher.backtrans(mean(Radj_trans)),  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

Pooled Radj is 0.08, pooled BIC is 42535.

###### Visualizing the adjusted relationships

```{r}
car::avPlots(lm_model, terms = c66_46_avrdibp ~ y1_app_temp_sd_46 + 
               c66_46_edu + c66_46_occup + sex + 
                c66_46_ht_med, 
             col = "darkgray", 
             col.lines = "blue",
             pch = 16,
             id = F, ask = F)
```

Horizontal line.

##### 2.2.2.3 Optimized model

Pool estimates over all datasets with minimal set of variables:

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrdibp ~ y1_app_temp_sd_46  +  
                       c66_46_alc + c66_46_c_bmi +
                       c66_46_ht_med + 
                       c66_46_q1_ht +
                       c66_46_urban_rural +
                       sex ))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2) 
```

Pooled b1 for app_sd is 0.32 (p=0.03) \[0.03 : 0.61\]. The effect size became positive.

**Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
mi.anova(pmm_proj_multiimp,
         "c66_46_avrdibp ~ y1_app_temp_sd_46  +  
                       c66_46_alc + c66_46_c_bmi +
                       c66_46_ht_med + c66_46_edu +
                       c66_46_q1_ht +
                       c66_46_urban_rural +
                       day_mean + sex", type = 3)
```

R2=0.19. All categorical variables are stat. significant.

###### Model diagnostics.

Number of imputations

```{r}
howManyImputations::how_many_imputations(lm_models)
```

6 imputations needed. **Constant variance assumption**

```{r}
imputed_dataset = bp_df_plot
lm_model = lm(c66_46_avrdibp ~ y1_app_temp_sd_46  +  
                       c66_46_alc + c66_46_c_bmi +
                       c66_46_ht_med + c66_46_edu +
                       c66_46_q1_ht +
                       c66_46_urban_rural +
                       day_mean + sex, data = imputed_dataset)

#residual plot of predictors vs resid and fitted vs resid = constant variance
car::residualPlots(lm_model,
                   pch=20, col="gray",
                   fitted = T,
                   ask = F, layout = c(2,2),
                   tests = F, quadratic = F)

ols_test_breusch_pagan(lm_model)
```

We see decrease in variance from 95. The violation might affect the results. A violation of the constant variance assumption results in inaccurate confidence intervals and p-values, even in large samples, although regression coefficient estimates will still be unbiased.

**Linearity assumption**

```{r}
car::crPlots(lm_model, terms = ~ y1_app_temp_sd_46  +  
                       c66_46_alc + c66_46_c_bmi +
                       c66_46_ht_med + c66_46_edu +
                       c66_46_q1_ht +
                       c66_46_urban_rural +
                       day_mean + sex,
             pch=20, col="gray",
             smooth = list(smoother=car::gamLine), ask = F)
```

Linear

**Normality assumption**

```{r}
source("Functions_rmph.R")

check_normality(lm_model)
```

In the **left panel** (histogram of residuals) we see that observed residuals almost perfectly normally distributed.

In the **right panel** (a QQ plot) there is a lack of normality in the tails of the distribution. Since the sample size is huge (n = 5661, obs \> predictors), non-normality unlikely impact the results.

**Effect of imputation on estimates (riv, lambda, fmi)**

```{r}
pooled_results
```

All variables have riv (relative increase in variance due to nonresponse), lambda (the proportion of the variation attributable to the missing data), fmi (the fraction of information missing due to nonresponse) less than 0.2 which can be interpreted as modest.

###### Pooled Radj and BIC for linear model:

```{r}
pool.r.squared(lm_models, adjusted = T)

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(lm_models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(lm_models$analyses[[i]])$BIC,
         Radj = glance(lm_models$analyses[[i]])$adj.r.squared)
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = fisher.trans(Radj)) %>%
  summarise(pooled_Radj = fisher.backtrans(mean(Radj_trans)),  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

Pooled Radj is 0.26, pooled BIC is 41441.

###### Visualizing the adjusted relationships

```{r}
car::avPlots(lm_model, terms = c66_46_avrdibp ~ y1_app_temp_sd_46  +  
                       c66_46_alc + c66_46_c_bmi +
                       c66_46_ht_med + c66_46_edu +
                       c66_46_q1_ht +
                       c66_46_urban_rural +
                       day_mean + sex, 
             col = "darkgray", 
             col.lines = "blue",
             pch = 16,
             id = F, ask = F)
```

Horizontal line.

##### 2.2.2.4 Fully adjusted(-)

Pool estimates over all datasets with minimal set of variables:

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrdibp ~ y1_app_temp_sd_46 +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2) 	
```

Pooled b1 for app.sd is 0.33 (p=0.03).

**Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
mi.anova(pmm_proj_multiimp,
         "c66_46_avrdibp ~ y1_app_temp_sd_46 +
                    c66_46_q1_ht + c66_46_c_bmi + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean", type = 3)
```

R2=0.18. All categorical variables are stat. significant except phys, occup, diab, season, day_mean.

###### Model diagnostics.

Number of imputations

```{r}
howManyImputations::how_many_imputations(lm_models)
```

6 imputations needed.

**Constant variance assumption**

```{r}
imputed_dataset = bp_df_plot
lm_model = lm(c66_46_avrdibp ~ y1_app_temp_sd_46 +
                    c66_46_q1_ht + c66_46_c_bmi + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = imputed_dataset)

#residual plot of predictors vs resid and fitted vs resid = constant variance
car::residualPlots(lm_model,
                   pch=20, col="gray",
                   fitted = T,
                   ask = F, layout = c(2,2),
                   tests = F, quadratic = F)

ols_test_breusch_pagan(lm_model)
```

We see decrease in variance from 140. However, the violation is not serious? in this case, so we can say that the assumption is satisfied.

**Linearity assumption**

```{r}
car::crPlots(lm_model, terms = ~ y1_app_temp_sd_46 +
                    c66_46_q1_ht + c66_46_c_bmi + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean,
             pch=20, col="gray",
             smooth = list(smoother=car::gamLine), ask = F)
```

Linear relationship.

**Normality assumption**

```{r}
source("Functions_rmph.R")

check_normality(lm_model)
```

In the **left panel** (histogram of residuals) we see that observed residuals almost perfectly normally distributed.

In the **right panel** (a QQ plot) there is a lack of normality in the tails of the distribution. Since the sample size is huge (n = 5661, obs \> predictors), non-normality unlikely impact the results.

**Effect of imputation on estimates (riv, lambda, fmi)**

```{r}
pooled_results
```

All variables have riv (relative increase in variance due to nonresponse), lambda (the proportion of the variation attributable to the missing data), fmi (the fraction of information missing due to nonresponse) less than 0.2 which can be interpreted as modest.

###### Pooled Radj and BIC for linear model:

```{r}
pool.r.squared(lm_models, adjusted = T)

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(lm_models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(lm_models$analyses[[i]])$BIC,
         Radj = glance(lm_models$analyses[[i]])$adj.r.squared)
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = fisher.trans(Radj)) %>%
  summarise(pooled_Radj = fisher.backtrans(mean(Radj_trans)),  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

Pooled Radj is 0.26, pooled BIC is 41441.

###### Visualizing the adjusted relationships

```{r}
car::avPlots(lm_model, terms = c66_46_avrdibp ~ y1_app_temp_sd_46 +
                    c66_46_q1_ht + c66_46_c_bmi + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, 
             col = "darkgray", 
             col.lines = "blue",
             pch = 16,
             id = F, ask = F)
```

Positive linear relationship.

#### 2.2.3 Average temperature

##### 2.2.3.1 Unadjusted model (univariate)

Pool estimates over all datasets with 1 exposure variable:

```{r}
source("Functions_rmph.R")
imputed_dataset = bp_df_plot

# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrdibp ~ y1_avg_temp))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2) 
```

Pooled b1 for app_sd is -0.7 (p\<0.001) \[-0.88, -0.51\]. The effect size is stat.insignificant and small. SE = 0.09.

###### Model diagnostics.

**Constant variance assumption**

```{r}
lm_model = lm(c66_46_avrdibp ~ y1_avg_temp, data = imputed_dataset)

#residual plot of predictors vs resid and fitted vs resid = constant variance
car::residualPlots(lm_model,
                   pch=20, col="gray",
                   fitted = T,
                   ask = F, layout = c(2,2),
                   tests = F, quadratic = F)

ols_test_breusch_pagan(lm_model)
```

Constant variance.

**Linearity assumption**

```{r}
car::crPlots(lm_model, terms = ~ y1_avg_temp,
             pch=20, col="gray",
             smooth = list(smoother=car::gamLine), ask = F)
```

The relationship is definitely linear.

**Normality assumption**

```{r}
source("Functions_rmph.R")

check_normality(lm_model)
```

In the **left panel** (histogram of residuals) we see that observed residuals almost perfectly normally distributed.

In the **right panel** (a QQ plot) there is a lack of normality in the tails of the distribution. Since the sample size is huge (n = 5661, obs \> predictors), non-normality unlikely impact the results.

**Effect of imputation on estimates (riv, lambda, fmi)**

```{r}
pooled_results
```

riv = 0, lambda = 0, fmi \~ 0 because of 1 variable only that does not have NA values.

###### Pooled Radj and BIC for linear model:

```{r}
pool.r.squared(lm_models, adjusted = F)

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(lm_models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(lm_models$analyses[[i]])$BIC,
         Radj = glance(lm_models$analyses[[i]])$adj.r.squared)
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = fisher.trans(Radj)) %>%
  summarise(pooled_Radj = fisher.backtrans(mean(Radj_trans)),  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

Pooled Radj is 0.009 which can be considered as almost zero and BIC 42881.

###### Visualizing the adjusted relationships

```{r}
car::avPlots(lm_model, terms = c66_46_avrdibp ~ y1_avg_temp, 
             col = "darkgray", 
             col.lines = "blue",
             pch = 16,
             id = F, ask = F)
```

Negative linear relationship.

##### 2.2.3.2 Minimally adjusted model

Pool estimates over all datasets with minimal set of variables:

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrdibp ~ y1_avg_temp + c66_46_edu +
                c66_46_occup + sex + 
                c66_46_urban_rural + c66_46_season))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)  
```

Pooled b1 for app.temp is -0.55 (p\<0.01) \[-0.73, -0.37\]. The effect size decreased by 0.25 and p-value decreased.

**Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
mi.anova(pmm_proj_multiimp,
         "c66_46_avrdibp ~ y1_avg_temp + c66_46_edu + 
         c66_46_occup + sex + c66_46_ht_med", type = 3)
```

R2=0.12. All categorical variables are stat. significant except occup status (p=0.12)

###### Model diagnostics.

Number of imputations

```{r}
howManyImputations::how_many_imputations(lm_models)
```

**Constant variance assumption**

```{r}
lm_model = lm(c66_46_avrdibp ~ y1_avg_temp + c66_46_edu +
                c66_46_occup + sex + 
                c66_46_ht_med, data = imputed_dataset)

#residual plot of predictors vs resid and fitted vs resid = constant variance
car::residualPlots(lm_model,
                   pch=20, col="gray",
                   fitted = T,
                   ask = F, layout = c(2,2),
                   tests = F, quadratic = F)

ols_test_breusch_pagan(lm_model)
```

Non constant variance.

**Linearity assumption**

```{r}
car::crPlots(lm_model, terms = ~ y1_avg_temp + c66_46_edu +
                c66_46_occup + sex + 
                c66_46_ht_med,
             pch=20, col="gray",
             smooth = list(smoother=car::gamLine), , ask = F)
```

Linear relationship.

**Normality assumption**

```{r}
source("Functions_rmph.R")

check_normality(lm_model)
```

In the **left panel** (histogram of residuals) we see that observed residuals almost perfectly normally distributed.

In the **right panel** (a QQ plot) there is a lack of normality in the tails of the distribution. Since the sample size is huge (n = 5661, obs \> predictors), non-normality unlikely impact the results.

**Effect of imputation on estimates (riv, lambda, fmi)**

```{r}
pooled_results
```

All variables have riv (relative increase in variance due to nonresponse), lambda (the proportion of the variation attributable to the missing data), fmi (the fraction of information missing due to nonresponse) less than 0.2 which can be interpreted as modest.

###### Pooled Radj and BIC for linear model:

```{r}
pool.r.squared(lm_models, adjusted = T)

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(lm_models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(lm_models$analyses[[i]])$BIC,
         Radj = glance(lm_models$analyses[[i]])$adj.r.squared)
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = fisher.trans(Radj)) %>%
  summarise(pooled_Radj = fisher.backtrans(mean(Radj_trans)),  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

Pooled Radj is 0.08, pooled BIC is 42500.

###### Visualizing the adjusted relationships

```{r}
car::avPlots(lm_model, terms = c66_46_avrdibp ~ y1_avg_temp + 
               c66_46_edu + c66_46_occup + sex + 
                c66_46_ht_med, 
             col = "darkgray", 
             col.lines = "blue",
             pch = 16,
             id = F, ask = F)
```

Negative slope..

##### 2.2.3.3 Optimized model

Pool estimates over all datasets with minimal set of variables:

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrdibp  ~ y1_avg_temp +  
                       c66_46_alc + c66_46_c_bmi +
                       c66_46_ht_med +
                       c66_46_q1_ht + 
                       c66_46_urban_rural + 
                       sex))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)
```

Pooled b1 for avg is -0.80 (p\<0.01) \[-0.97 -0.63\].

**Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
mi.anova(pmm_proj_multiimp,
         "c66_46_avrdibp ~ y1_avg_temp + c66_46_phys + 
                       c66_46_alc +  +
                       c66_46_c_bmi +
                       c66_46_ht_med + c66_46_q1_ht +
                       sex ", type = 3)
```

R2=0.19. All categorical variables are stat. significant.

###### Model diagnostics.

Number of imputations

```{r}
howManyImputations::how_many_imputations(lm_models)
```

6 imputations needed.

**Constant variance assumption**

```{r}
imputed_dataset = bp_df_plot
lm_model = lm(c66_46_avrdibp ~ y1_avg_temp + c66_46_phys + 
                       c66_46_alc +  +
                       c66_46_c_bmi +
                       c66_46_ht_med + c66_46_q1_ht +
                       sex, data = imputed_dataset)

#residual plot of predictors vs resid and fitted vs resid = constant variance
car::residualPlots(lm_model,
                   pch=20, col="gray",
                   fitted = T,
                   ask = F, layout = c(2,2),
                   tests = F, quadratic = F)

ols_test_breusch_pagan(lm_model)
```

We see decrease in variance from 140. The violation might affect the results. A violation of the constant variance assumption results in inaccurate confidence intervals and p-values, even in large samples, although regression coefficient estimates will still be unbiased.

**Linearity assumption**

```{r}
car::crPlots(lm_model, terms = ~ y1_avg_temp + c66_46_phys + 
                       c66_46_alc +  +
                       c66_46_c_bmi +
                       c66_46_ht_med + c66_46_q1_ht +
                       sex,
             pch=20, col="gray",
             smooth = list(smoother=car::gamLine), ask = F)
```

Linear

**Normality assumption**

```{r}
source("Functions_rmph.R")

check_normality(lm_model)
```

In the **left panel** (histogram of residuals) we see that observed residuals almost perfectly normally distributed.

In the **right panel** (a QQ plot) there is a lack of normality in the tails of the distribution. Since the sample size is huge (n = 5661, obs \> predictors), non-normality unlikely impact the results.

**Effect of imputation on estimates (riv, lambda, fmi)**

```{r}
pooled_results
```

All variables have riv (relative increase in variance due to nonresponse), lambda (the proportion of the variation attributable to the missing data), fmi (the fraction of information missing due to nonresponse) less than 0.2 which can be interpreted as modest.

###### Pooled Radj and BIC for linear model:

```{r}
pool.r.squared(lm_models, adjusted = T)

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(lm_models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(lm_models$analyses[[i]])$BIC,
         Radj = glance(lm_models$analyses[[i]])$adj.r.squared)
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = fisher.trans(Radj)) %>%
  summarise(pooled_Radj = fisher.backtrans(mean(Radj_trans)),  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

Pooled Radj is 0.26, pooled BIC is 41339..

###### Visualizing the adjusted relationships

```{r}
car::avPlots(lm_model, terms = c66_46_avrdibp ~ y1_avg_temp + c66_46_phys + 
                       c66_46_alc +  +
                       c66_46_c_bmi +
                       c66_46_ht_med + c66_46_q1_ht +
                       sex, 
             col = "darkgray", 
             col.lines = "blue",
             pch = 16,
             id = F, ask = F)
```

Negative slope.

##### 2.2.3.4 Fully adjusted

Pool estimates over all datasets with minimal set of variables:

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrdibp ~ y1_avg_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2) 
```

Pooled b1 for avg.temp is -0.67 (p\<0.001) \[-0.85 -0.48\]. The SE is the same.

**Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
mi.anova(pmm_proj_multiimp,
         "c66_46_avrdibp ~ y1_avg_temp +
                    c66_46_q1_ht + c66_46_c_bmi + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean", type = 3)
```

R2=0.18. All categorical variables are stat. significant except occup, diab, season,, day_mean. Both day_mean and seasin are not included in the optimized model.

###### Model diagnostics.

Number of imputations

```{r}
howManyImputations::how_many_imputations(lm_models)
```

6 imputations needed.

**Constant variance assumption**

```{r}
imputed_dataset = bp_df_plot
lm_model = lm(c66_46_avrdibp ~ y1_avg_temp +
                    c66_46_q1_ht + c66_46_c_bmi + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = imputed_dataset)

#residual plot of predictors vs resid and fitted vs resid = constant variance
car::residualPlots(lm_model,
                   pch=20, col="gray",
                   fitted = T,
                   ask = F, layout = c(2,2),
                   tests = F, quadratic = F)

ols_test_breusch_pagan(lm_model)
```

We see decrease in variance from 140. However, the violation is not serious? in this case, so we can say that the assumption is satisfied.

**Linearity assumption**

```{r}
car::crPlots(lm_model, terms = ~ y1_avg_temp +
                    c66_46_q1_ht + c66_46_c_bmi + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean,
             pch=20, col="gray",
             smooth = list(smoother=car::gamLine), ask = F)
```

Linear relationship.

**Normality assumption**

```{r}
source("Functions_rmph.R")

check_normality(lm_model)
```

In the **left panel** (histogram of residuals) we see that observed residuals almost perfectly normally distributed.

In the **right panel** (a QQ plot) there is a lack of normality in the tails of the distribution. Since the sample size is huge (n = 5661, obs \> predictors), non-normality unlikely impact the results.

**Effect of imputation on estimates (riv, lambda, fmi)**

```{r}
pooled_results
```

All variables have riv (relative increase in variance due to nonresponse), lambda (the proportion of the variation attributable to the missing data), fmi (the fraction of information missing due to nonresponse) less than 0.2 which can be interpreted as modest.

###### Pooled Radj and BIC for linear model:

```{r}
pool.r.squared(lm_models, adjusted = T)

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(lm_models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(lm_models$analyses[[i]])$BIC,
         Radj = glance(lm_models$analyses[[i]])$adj.r.squared)
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = fisher.trans(Radj)) %>%
  summarise(pooled_Radj = fisher.backtrans(mean(Radj_trans)),  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

Pooled Radj is 0.26, pooled BIC is 41403.

###### Visualizing the adjusted relationships

```{r}
car::avPlots(lm_model, terms = c66_46_avrdibp ~ y1_avg_temp +
                    c66_46_q1_ht + c66_46_c_bmi + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                     + sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, 
             col = "darkgray", 
             col.lines = "blue",
             pch = 16,
             id = F, ask = F)
```

Negative slope.

#### 2.2.4 Standard Deviation of average temperature

##### 2.2.4.1 Unadjusted model (univariate)

Pool estimates over all datasets with 1 exposure variable:

```{r}
source("Functions_rmph.R")
imputed_dataset = bp_df_plot

# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrdibp ~ y1_sd_temp))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2) 
```

Pooled b1 for avd_sd is 0.26 (p=0.12) \[-0.07, 0.58\]. SE = 0.17 .

###### Model diagnostics.

**Constant variance assumption**

```{r}
lm_model = lm(c66_46_avrdibp ~ y1_sd_temp, data = imputed_dataset)

#residual plot of predictors vs resid and fitted vs resid = constant variance
car::residualPlots(lm_model,
                   pch=20, col="gray",
                   fitted = T,
                   ask = F, layout = c(2,2),
                   tests = F, quadratic = F)

ols_test_breusch_pagan(lm_model)
```

Variance started decreasing from 125.4. The violation might affect the results. A violation of the constant variance assumption results in inaccurate confidence intervals and p-values, even in large samples, although regression coefficient estimates will still be unbiased.

**Linearity assumption**

```{r}
car::crPlots(lm_model, terms = ~ y1_sd_temp,
             pch=20, col="gray",
             smooth = list(smoother=car::gamLine), ask = F)
```

The relationship is definitely linear.

**Normality assumption**

```{r}
source("Functions_rmph.R")

check_normality(lm_model)
```

In the **left panel** (histogram of residuals) we see that observed residuals almost perfectly normally distributed.

In the **right panel** (a QQ plot) there is a lack of normality in the tails of the distribution. Since the sample size is huge (n = 5661, obs \> predictors), non-normality unlikely impact the results.

**Effect of imputation on estimates (riv, lambda, fmi)**

```{r}
pooled_results
```

riv = 0, lambda = 0, fmi \~ 0 because of 1 variable only that does not have NA values.

###### Pooled Radj and BIC for linear model:

```{r}
pool.r.squared(lm_models, adjusted = F)

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(lm_models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(lm_models$analyses[[i]])$BIC,
         Radj = glance(lm_models$analyses[[i]])$adj.r.squared)
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = fisher.trans(Radj)) %>%
  summarise(pooled_Radj = fisher.backtrans(mean(Radj_trans)),  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

Pooled Radj is-0.0002 which can be considered as almost zero and BIC 42933.

###### Visualizing the adjusted relationships

```{r}
car::avPlots(lm_model, terms = c66_46_avrdibp ~ y1_sd_temp, 
             col = "darkgray", 
             col.lines = "blue",
             pch = 16,
             id = F, ask = F)
```

Horizontal line.

##### 2.2.4.2 Minimally adjusted model

Pool estimates over all datasets with minimal set of variables:

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrdibp ~ y1_sd_temp + c66_46_edu +
                c66_46_occup + sex + 
                c66_46_urban_rural + c66_46_season))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2) 
```

Pooled b1 for app.temp is 0.16 (p=0.32) \[-0.15 0.48\].

**Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
mi.anova(pmm_proj_multiimp,
         "c66_46_avrdibp ~ y1_sd_temp + c66_46_edu + 
         c66_46_occup + sex + c66_46_ht_med", type = 3)
```

R2=0.11. All categorical variables are stat. significant except occup status (p=0.12) and avg_sd

###### Model diagnostics.

Number of imputations

```{r}
howManyImputations::how_many_imputations(lm_models)
```

**Constant variance assumption**

```{r}
lm_model = lm(c66_46_avrdibp ~ y1_sd_temp + c66_46_edu +
                c66_46_occup + sex + 
                c66_46_ht_med, data = imputed_dataset)

#residual plot of predictors vs resid and fitted vs resid = constant variance
car::residualPlots(lm_model,
                   pch=20, col="gray",
                   fitted = T,
                   ask = F, layout = c(2,2),
                   tests = F, quadratic = F)

ols_test_breusch_pagan(lm_model)
```

This clustering migh be due to the fact that there is only 1 contin.variable and 4 categorical

**Linearity assumption**

```{r}
car::crPlots(lm_model, terms = ~ y1_sd_temp + c66_46_edu +
                c66_46_occup + sex + 
                c66_46_ht_med,
             pch=20, col="gray",
             smooth = list(smoother=car::gamLine), , ask = F)
```

Linear relationship.

**Normality assumption**

```{r}
source("Functions_rmph.R")

check_normality(lm_model)
```

In the **left panel** (histogram of residuals) we see that observed residuals almost perfectly normally distributed.

In the **right panel** (a QQ plot) there is a lack of normality in the tails of the distribution. Since the sample size is huge (n = 5661, obs \> predictors), non-normality unlikely impact the results.

**Effect of imputation on estimates (riv, lambda, fmi)**

```{r}
pooled_results
```

All variables have riv (relative increase in variance due to nonresponse), lambda (the proportion of the variation attributable to the missing data), fmi (the fraction of information missing due to nonresponse) less than 0.2 which can be interpreted as modest.

###### Pooled Radj and BIC for linear model:

```{r}
pool.r.squared(lm_models, adjusted = T)

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(lm_models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(lm_models$analyses[[i]])$BIC,
         Radj = glance(lm_models$analyses[[i]])$adj.r.squared)
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = fisher.trans(Radj)) %>%
  summarise(pooled_Radj = fisher.backtrans(mean(Radj_trans)),  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

Pooled Radj is 0.08, pooled BIC is 42535.

###### Visualizing the adjusted relationships

```{r}
car::avPlots(lm_model, terms = c66_46_avrdibp ~ y1_sd_temp + 
               c66_46_edu + c66_46_occup + sex + 
                c66_46_ht_med, 
             col = "darkgray", 
             col.lines = "blue",
             pch = 16,
             id = F, ask = F)
```

Horizontal line.

##### 2.2.4.3 Optimized model

Pool estimates over all datasets with minimal set of variables:

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrdibp  ~ y1_sd_temp +  
                       c66_46_alc + c66_46_c_bmi +
                       c66_46_ht_med +
                       c66_46_q1_ht +
                       c66_46_urban_rural + 
                       sex))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2) 
```

Pooled b1 for avg is 0.34 (p=0.02) \[0.05, 0.63\].

**Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
mi.anova(pmm_proj_multiimp,
         "c66_46_avrdibp ~ y1_sd_temp +  
                       c66_46_alc + c66_46_c_bmi +
                       c66_46_edu + c66_46_ht_med +
                       c66_46_q1_ht +
                       c66_46_urban_rural + day_mean +
                       sex", type = 3)
```

R2=0.19. All categorical variables are stat. significant.

###### Model diagnostics.

Number of imputations

```{r}
howManyImputations::how_many_imputations(lm_models)
```

6 imputations needed. **Constant variance assumption**

```{r}
imputed_dataset = bp_df_plot
lm_model = lm(c66_46_avrdibp ~y1_sd_temp +  
                       c66_46_alc + c66_46_c_bmi +
                       c66_46_edu + c66_46_ht_med +
                       c66_46_q1_ht +
                       c66_46_urban_rural + day_mean +
                       sex, data = imputed_dataset)

#residual plot of predictors vs resid and fitted vs resid = constant variance
car::residualPlots(lm_model,
                   pch=20, col="gray",
                   fitted = T,
                   ask = F, layout = c(2,2),
                   tests = F, quadratic = F)

ols_test_breusch_pagan(lm_model)
```

We see decrease in variance from 140. The violation might affect the results. A violation of the constant variance assumption results in inaccurate confidence intervals and p-values, even in large samples, although regression coefficient estimates will still be unbiased.

**Linearity assumption**

```{r}
car::crPlots(lm_model, terms = ~ y1_sd_temp +  
                       c66_46_alc + c66_46_c_bmi +
                       c66_46_edu + c66_46_ht_med +
                       c66_46_q1_ht +
                       c66_46_urban_rural + day_mean +
                       sex ,
             pch=20, col="gray",
             smooth = list(smoother=car::gamLine), ask = F)
```

Linear

**Normality assumption**

```{r}
source("Functions_rmph.R")

check_normality(lm_model)
```

In the **left panel** (histogram of residuals) we see that observed residuals almost perfectly normally distributed.

In the **right panel** (a QQ plot) there is a lack of normality in the tails of the distribution. Since the sample size is huge (n = 5661, obs \> predictors), non-normality unlikely impact the results.

**Effect of imputation on estimates (riv, lambda, fmi)**

```{r}
pooled_results
```

All variables have riv (relative increase in variance due to nonresponse), lambda (the proportion of the variation attributable to the missing data), fmi (the fraction of information missing due to nonresponse) less than 0.2 which can be interpreted as modest.

###### Pooled Radj and BIC for linear model:

```{r}
pool.r.squared(lm_models, adjusted = T)

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(lm_models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(lm_models$analyses[[i]])$BIC,
         Radj = glance(lm_models$analyses[[i]])$adj.r.squared)
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = fisher.trans(Radj)) %>%
  summarise(pooled_Radj = fisher.backtrans(mean(Radj_trans)),  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

Pooled Radj is 0.25, pooled BIC is 41386..

###### Visualizing the adjusted relationships

```{r}
car::avPlots(lm_model, terms = c66_46_avrdibp ~ y1_sd_temp +  
                       c66_46_alc + c66_46_c_bmi +
                       c66_46_edu + c66_46_ht_med +
                       c66_46_q1_ht +
                       c66_46_urban_rural + day_mean +
                       sex, 
             col = "darkgray", 
             col.lines = "blue",
             pch = 16,
             id = F, ask = F)
```

Horizontal line.

##### 2.2.4.4 Fully adjusted(-)

Pool estimates over all datasets with minimal set of variables:

```{r}
source("Functions_rmph.R")
# Apply the linear function to each imputed dataset
lm_models <- with(pmm_proj_multiimp, 
               lm(c66_46_avrdibp ~ y1_sd_temp +
                    c66_46_q1_ht + c66_46_c_bmi_cat + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean))

# Use `pool()` to combine results across imputations
pooled_results <- pool(lm_models)

# Summarize pooled results
summary(pooled_results, conf.int = TRUE, conf.level = 0.95)

round.summary(lm_models, digits = 2)
```

Pooled b1 for avg.temp is 0.8 (p\<0.01) \[0.38, 1.22\]. The SE increaed to 0.22.

```{r}
library(texreg)

# Suppose 'lm_models' is a mids object containing your imputed datasets
# and you've done:
# pooled_fit <- pool(with(lm_models, lm(...)))

# Just feed pooled_fit to texreg
texreg(pooled_results)
```

**Combined ANOVA to assess the significance of categorical variables and computes the pooled R2 as whole across all imputed datasets:**

```{r}
mi.anova(pmm_proj_multiimp,
         "c66_46_avrdibp ~ y1_sd_temp +
                    c66_46_q1_ht + c66_46_c_bmi + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean", type = 3)
```

R2=0.18. All categorical variables are stat. significant except phys, occup, diab, season, day_mean.

###### Model diagnostics.

Number of imputations

```{r}
howManyImputations::how_many_imputations(lm_models)
```

6 imputations needed.

**Constant variance assumption**

```{r}
imputed_dataset = bp_df_plot
lm_model = lm(c66_46_avrdibp ~ y1_sd_temp +
                    c66_46_q1_ht + c66_46_c_bmi + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                   sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, data = imputed_dataset)

#residual plot of predictors vs resid and fitted vs resid = constant variance
car::residualPlots(lm_model,
                   pch=20, col="gray",
                   fitted = T,
                   ask = F, layout = c(2,2),
                   tests = F, quadratic = F)

ols_test_breusch_pagan(lm_model)
```

We see decrease in variance from 140. However, the violation is not serious? in this case, so we can say that the assumption is satisfied.

**Linearity assumption**

```{r}
car::crPlots(lm_model, terms = ~ y1_sd_temp +
                    c66_46_q1_ht + c66_46_c_bmi + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                   sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean,
             pch=20, col="gray",
             smooth = list(smoother=car::gamLine), ask = F)
```

Linear relationship.

**Normality assumption**

```{r}
source("Functions_rmph.R")

check_normality(lm_model)
```

In the **left panel** (histogram of residuals) we see that observed residuals almost perfectly normally distributed.

In the **right panel** (a QQ plot) there is a lack of normality in the tails of the distribution. Since the sample size is huge (n = 5661, obs \> predictors), non-normality unlikely impact the results.

**Effect of imputation on estimates (riv, lambda, fmi)**

```{r}
pooled_results
```

All variables have riv (relative increase in variance due to nonresponse), lambda (the proportion of the variation attributable to the missing data), fmi (the fraction of information missing due to nonresponse) less than 0.2 which can be interpreted as modest.

###### Pooled Radj and BIC for linear model:

```{r}
pool.r.squared(lm_models, adjusted = T)

# Fisher transformation for Radj
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)

num_imp <- length(lm_models$analyses)

# Extract BIC and Adjusted R² efficiently
model_perf <- map_dfr(seq_len(num_imp), function(i) {
  tibble(imputation = i,
         BIC = glance(lm_models$analyses[[i]])$BIC,
         Radj = glance(lm_models$analyses[[i]])$adj.r.squared)
})

# Pool Adjusted R² (with Fisher transformation) and BIC
pooled_bic_radj <- model_perf %>%
  mutate(Radj_trans = fisher.trans(Radj)) %>%
  summarise(pooled_Radj = fisher.backtrans(mean(Radj_trans)),  
            pooled_BIC = mean(BIC))

pooled_bic_radj
```

Pooled Radj is 0.26, pooled BIC is 41440.

###### Visualizing the adjusted relationships

```{r}
car::avPlots(lm_model, terms = c66_46_avrdibp ~ y1_sd_temp +
                    c66_46_q1_ht + c66_46_c_bmi + 
                    c66_46_ht_med + c66_46_edu +  
                    c66_46_smok + c66_46_alc +
                   c66_46_phys + c66_46_occup + 
                    sex + 
                    c66_46_diab + c66_46_hpcl + 
                    c66_46_season + c66_46_urban_rural+day_mean, 
             col = "darkgray", 
             col.lines = "blue",
             pch = 16,
             id = F, ask = F)
```

Positive slope.